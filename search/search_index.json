{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>When we interact with people \ud83d\udde3\ufe0f\ud83d\udc42, we naturally remember details from past interactions \ud83d\udcad, feelings \ud83d\ude1c\ud83d\ude22, and shared experiences \ud83e\udd1d. That's what makes us human. We're bringing this same ability to AI, helping it recall just like us.</p> <p>Give the github repo a starhug \u2b50\ufe0f\u2014it\u2019s feeling a lil\u2019 lonely \ud83e\udd7a</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p>Temporal Memory Recall: Enables AI to remember timestamped memories from past interactions, tracking updates to show how information evolves over time.</p> </li> <li> <p>Multi-Tenancy: Designed to accommodate multiple organizations, agents, and users along their interactions and memories.</p> </li> <li> <p>Flexible Name Handling: Uses placeholders for user and agent names in memories, allowing easy updates if names change later.</p> </li> <li> <p>Scalability: Designed to scale to millions of users, interactions, memories etc. with strategic indexes and constraints for very fast search.</p> </li> <li> <p>Developer-Friendly Design: Provides a modular architecture that allows you to adapt it to fit your specific needs with integration of new features etc.</p> </li> </ul>"},{"location":"#our-vision","title":"Our Vision \ud83d\udd2d","text":"<p>Currently, Memora manages text-based memories, assisting AI in maintaining context and evolving over time. However, our dream is way bigger towards the full spectrum of human memory, enabling AI to interact as naturally as a close friend. This will need:</p> <ul> <li> <p>Emotion-Tagged Memories: Allowing AI to recall memories along with the emotions experienced at that time, such as joy \ud83d\ude04, sadness \ud83d\ude22, surprise \ud83d\ude32...</p> </li> <li> <p>Multi-modal Memories: As we move into an era where AI is ever-present (24/7), it should be capable of recalling video \ud83c\udfa5 and audio \ud83d\udd09 segments from interactions, akin to how humans replay past events as a mental film in our heads.</p> </li> </ul>"},{"location":"#join-us","title":"Join Us!","text":"<p>We're building Memora in the open, and we'd love your \ud83e\udef5 help. No contribution is too small, even fixing typos \u270f\ufe0f. Check out our CONTRIBUTING.md.</p> <p>Let's give AI a human touch, together! \ud83d\ude01</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>memora/\n\u251c\u2500\u2500 agent/          # The Orchestrator\n\u251c\u2500\u2500 graph_db/       # Graph database implementation\n\u251c\u2500\u2500 llm_backends/   # Backend LLM implementations for Memora\n\u251c\u2500\u2500 prompts/        # Prompts for memory operations\n\u251c\u2500\u2500 schema/         # Data models and schemas\n\u2514\u2500\u2500 vector_db/      # Vector database implementation\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Set up and use Memora.</li> <li>Advanced Usage - Explore advanced use cases and custom implementations</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"advanced_usage/","title":"Advanced Usage","text":""},{"location":"advanced_usage/#the-graph-model","title":"The Graph Model","text":"<p>Multi-Tenant Design</p> <ul> <li>Memora accommodates multiple organizations, agents, and users in one system \ud83c\udfe2\ud83d\udc65\ud83e\udd16</li> <li>Strategic indexes for fast queries regardless of <code>MemorySearchScope</code> \u26a1</li> </ul>"},{"location":"advanced_usage/#indexes-and-constraints","title":"Indexes and Constraints","text":"<ul> <li>Organization Node Key (<code>unique_org_id</code>): Unique identifier for each organization (<code>org_id</code>) within the graph.</li> <li>User Node Key (<code>unique_user_id</code>): Unique combination of <code>org_id</code> and <code>user_id</code> for each user within an organization.</li> <li>Agent Node Key (<code>unique_agent_id</code>): Unique combination of <code>org_id</code> and <code>agent_id</code> for agents.</li> <li>Memory Node Key (<code>unique_memory_id</code>): Unique combination of <code>org_id</code>, <code>user_id</code>, and <code>memory_id</code> for memories associated with users.</li> <li>Interaction Node Key (<code>unique_interaction_id</code>): Unique combination of <code>org_id</code>, <code>user_id</code>, and <code>interaction_id</code> for interactions.</li> <li>Interaction Updated Timestamp Index (<code>index_updated_at</code>): Index on the <code>updated_at</code> attribute to facilitate sorting interactions by their most recent update.</li> <li>Date Node Key (<code>unique_date_id</code>): Unique combination of <code>org_id</code>, <code>user_id</code>, and <code>date</code> for date records associated with users.</li> </ul>"},{"location":"advanced_usage/#organizations-and-users","title":"Organizations and Users","text":"<ul> <li>Multiple organizations can exist within the system, each with its own set of users and agents.</li> <li>Each user has a collection of memories and interactions associated with them.</li> </ul>"},{"location":"advanced_usage/#agents","title":"Agents","text":"<ul> <li>Agents can be associated with an organization or with both an organization and a specific user (allowing for scenarios such as a personalized AI for each user within the organization).</li> </ul>"},{"location":"advanced_usage/#interactions-and-memories","title":"Interactions and Memories","text":"<ul> <li>Users have interactions, which are conversations with agents.</li> <li>Each interaction is timestamped and linked to a specific date of occurance.</li> <li>Memories are extracted from these interactions and stored in a user's memory collection.</li> <li>Memories are also linked to both their source messages and source interactions.</li> <li>All contrary updates to a memory are connected, so there is a historical view of memories, enabling tracking of how information evolves over time.</li> </ul>"},{"location":"advanced_usage/#message-structure","title":"Message Structure","text":"<ul> <li>Interactions consist of message blocks, representing individual messages in a conversation.</li> <li>Message blocks are ordered and contain information about the role (user or agent) and content.</li> </ul>"},{"location":"advanced_usage/#integration-of-vector-and-graph-databases","title":"Integration of Vector and Graph Databases","text":"<p>Memora integrates vector and graph databases to efficiently manage and search memory data. The vector database, such as Qdrant (with more options coming soon), utilizes both dense and sparse embeddings (such as SPLADE) to perform hybrid search. Memories are first searched in the vector database, where the vector IDs correspond to the <code>memory_ids</code> of memory nodes in the graph database. This design allows for seamless retrieval and resolution of memories from Neo4j, ensuring that the most up-to-date version of a memory is utilized.</p> <p>Name Placeholders \ud83d\udcdd</p> <p>Memories are stored using placeholders for user names and agent labels, like <code>user_{short_uuid} loves peanuts</code> or <code>agent_{short_uuid} wishes to go on a trip to Kyoto</code>. This approach allows for future changes to the user's name or agent label, ensuring that we always use the latest name or label.</p> <p>Additionally, indexes have been created in Qdrant to support multi-tenancy and enable search scopes, such as specific user searches within an organization or searches across organizations.</p>"},{"location":"advanced_usage/#reducing-memory-search-latency","title":"Reducing Memory Search Latency","text":"<p>Instead of using <code>await memora.recall_memories_for_message(...)</code>, which internally first calls a model to generate search queries, you can achieve better latency by:</p> <ol> <li>Having your chat model generate the memory search queries directly</li> <li>Passing these queries to <code>await memora.search_memories_as_one(...)</code></li> </ol> <p>This approach bypasses an extra model call to generate memory queries, reducing both token usage and latency. Here's a simple example:</p> <pre><code>from openai import AsyncOpenAI\nimport json\n\n... # Preceding code where you have initialized memora, and stored org_id and user_id in variables.\n\n# Define the memory search tool\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search_memories\",\n            \"description\": \"Search through user's memories with specific queries.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"queries\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of search queries to find relevant memories.\"\n                    }\n                },\n                \"required\": [\"queries\"]\n            }\n        },\n    }\n]\n\n# Async client initialization\nclient = AsyncOpenAI(api_key=\"YourOpenAIAPIKey\")\n\nmessages=[\n    {\n        \"role\": \"system\", \n        \"content\": \"You are an assistant. When responding to the user, if you need memory to provide the best response, call the 'search_memories' tool with the required search queries.\"\n    },\n    {\n        \"role\": \"user\", \n        \"content\": \"Hey, what\u2019s the name of the restaurant we went to last weekend? The one with the amazing tacos?\"\n    }\n]\n\n# Step 1: Prompt the model.\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    tools=tools  # Include the memory search tool in the request\n)\n\n# Step 2: Extract the response and any tool call responses\nresponse_message = response.choices[0].message\ntool_calls = response_message.tool_calls\n\nmessages.append(response_message) # Add the LLM's response to the conversation\n\nif tool_calls: # The memory search tool was called.\n\n    # There is only one tool (memory search, so we just extract the arguments).\n    search_args = json.loads(tool_calls[0].function.arguments)  # Example: {\"queries\": [\"restaurant last weekend\", \"amazing tacos\"]}\n    queries = search_args[\"queries\"] # [\"restaurant last weekend\", \"amazing tacos\"]\n\n    # Step 3: Perform memory search with queries as a single batch\n    recalled_memories = await memora.search_memories_as_one(\n        org_id=org_id,\n        user_id=user_id,\n        search_queries=queries,\n        search_across_agents=True\n    )\n\n    # recalled_memories: [\n    # Memory(..., memory_id='uuid string', memory=\"Jake confirmed Chezy has the best tacos, saying his mouth literally watered.\", obtained_at=datetime(...), message_sources=[...]),\n    # Memory(..., memory_id='uuid string', memory=\"Jake is planing to go to Chezy this weekend.\", obtained_at=datetime(...), message_sources=[...]),\n    #...]\n\n    # Add the tool response to the conversation\n    messages.append(\n        {\n            \"tool_call_id\": tool_calls[0].id, \n            \"role\": \"tool\", # Indicates this message is from tool use\n            \"name\": \"search_memories\",\n            \"content\": str([memory.memory_and_timestamp_dict() for memory in recalled_memories]),\n        }\n    )\n\n    # Make a final API call with the updated conversation that has the memories of the tool call.\n    final_response = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages\n    )\n\n    # Print the final response\n    print(f\"&gt;&gt;&gt; Assistant Reply: {final_response.choices[0].message.content}\")\n\nelse: # The memory search tool wasn't called\n    print(f\"&gt;&gt;&gt; Assistant Reply: {response_message.content}\")\n</code></pre> <p>Note</p> <p>If you deploy the graph and vector database very close to your application, it reduces latency caused by network trips a lot.</p>"},{"location":"advanced_usage/#extending-base-classes","title":"Extending Base Classes","text":"<p>Memora provides several base classes that can be extended to implement custom functionality:</p>"},{"location":"advanced_usage/#custom-llm-backend","title":"Custom LLM Backend","text":"<p>To create a custom LLM backend for your llm provider that Memora can use, extend the <code>BaseBackendLLM</code> class:</p> <pre><code>from memora.llm_backends.base import BaseBackendLLM\nfrom typing_extensions import override\nfrom pydantic import BaseModel\nfrom typing import Any, Dict, List, Type, Union\n\nclass CustomBackendLLM(BaseBackendLLM):\n\n    def __init__(\n        self,\n        model: str, \n        temperature: float = 1,\n        top_p: float = 1,\n        max_tokens: int = 1024,\n        # Any more arguments you want to add.\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the CustomBackendLLM class with specific parameters.\n\n        Example (May Differ for your case):\n            self.custom_client = AsyncCustomClient(...)\n\n            self.model = model\n            self.temperature = temperature\n            self.top_p = top_p\n            self.max_tokens = max_tokens\n        \"\"\"\n        pass\n\n    @override\n    async def close(self) -&gt; None: \n        \"\"\"\n        Closes the LLM connection.\n\n        Example (May Differ for your case):\n            await self.custom_client.close()\n            self.custom_client = None\n\n            OR JUST\n            self.custom_client = None\n        \"\"\"\n        pass\n\n    @property\n    @override\n    def get_model_kwargs(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns dictionary of model configuration parameters\n\n        Example (May have more parameters for your case):\n            return {\n                \"model\": self.model, # model_name: gpt-4o\n                \"temperature\": self.temperature, # 1\n                \"top_p\": self.top_p, # 1\n                \"max_tokens\": self.max_tokens, # 1024\n                \"stream\": False,\n            }\n        \"\"\"\n        pass\n\n    @abstractmethod\n    @override\n    async def __call__(self, messages: List[Dict[str, str]], output_schema_model: Type[BaseModel] | None = None) -&gt; Union[str, BaseModel]:\n        \"\"\"\n        Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n        Args:\n            messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n            output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output (\ud83d\udccc Ensure your model provider supports this for the chosen model)\n\n        Returns:\n            Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n\n        Example (May Differ for your case):\n            if output_schema_model:\n                response = await self.custom_client.chat.completions.create(\n                    messages=messages,\n                    **self.get_model_kwargs,\n                    response_format={\"type\": \"json_object\"},\n                )\n                content = response.choices[0].message.content\n                return output_schema_model.model_validate_json(content)\n\n            else:\n                response = await self.custom_client.chat.completions.create(\n                    messages=messages,\n                    **self.get_model_kwargs,\n                )\n                return response.choices[0].message.content\n        \"\"\"\n        pass\n</code></pre> <p>Note</p> <p>For more information on the <code>BaseBackendLLM</code> class and methods, refer to its API Reference Page.</p>"},{"location":"advanced_usage/#custom-graph-database","title":"Custom Graph Database","text":"<p>To implement a custom graph database that Memora can use, extend the <code>BaseGraphDB</code> class:</p> <pre><code>from memora.graph_db.base import BaseGraphDB\nfrom memora.schema import models\nfrom typing_extensions import override\n\nclass CustomGraphDB(BaseGraphDB):\n    @override\n    async def setup(self):\n        # Initialize your graph database\n        pass\n\n    @override\n    async def create_organization(self, org_name: str) -&gt; models.Organization:\n        # Implement organization creation logic\n        pass\n\n    # Implement other required methods\n</code></pre> <p>Note</p> <p>For more information on the <code>BaseGraphDB</code> class and methods, refer to its API Reference Page.</p>"},{"location":"advanced_usage/#custom-vector-database","title":"Custom Vector Database","text":"<p>To implement a custom vector database that Memora can use, extend the <code>BaseVectorDB</code> class:</p> <pre><code>from memora.vector_db.base import BaseVectorDB\nfrom typing_extensions import override\n\nclass CustomVectorDB(BaseVectorDB):\n\n    @override\n    async def close(self) -&gt; None:\n        pass\n\n    @override\n    async def setup(self):\n        # Initialize your vector database\n        pass\n\n    # Implement other required methods\n</code></pre> <p>Note</p> <p>For more information on the <code>BaseVectorDB</code> class and methods, refer to its API Reference Page.</p>"},{"location":"advanced_usage/#extending-existing-implementations","title":"Extending Existing Implementations","text":"<p>If you need to extend an existing implementation, such as the Neo4j graph interface or the Qdrant vector database, you can create a subclass:</p> <pre><code>from memora.graph_db import Neo4jGraphInterface\n\nclass ExtendedNeo4jGraph(Neo4jGraphInterface):\n    # Intialize parent class and add your methods.\n    pass\n\nfrom memora.vector_db import QdrantDB\n\nclass ExtendedQdrantDB(QdrantDB):\n    # Intialize parent class and add your methods.\n    pass\n\n# Note: This can also be done for any of the `BaseBackendLLM` implementations like GroqBackendLLM, TogetherBackendLLM, etc. So you can enable streaming, tools, etc.\n</code></pre> <p>Note</p> <ul> <li>For more information on the <code>Neo4jGraphInterface</code> class and methods, refer to its API Reference Page.</li> <li>For more information on the <code>QdrantDB</code> class and methods, refer to its API Reference Page.</li> </ul> <p>By extending these classes, you can tailor Memora to your specific use case while maintaining compatibility with the core functionality.</p>"},{"location":"getting_started/","title":"Prerequisites","text":"<p>Before using Memora, you'll need to set up the following:</p> <ol> <li> <p>Neo4j Database </p> <ul> <li>Option A: Install Neo4j locally (Free) </li> <li>Option B: Use Neo4j AuraDB Cloud (Free Option available)</li> </ul> </li> <li> <p>Qdrant Vector Database </p> <ul> <li>Option A: Install Qdrant locally (Free) </li> <li>Option B: Use Qdrant Cloud (Free Option available)</li> </ul> </li> <li> <p>LLM Providers    Choose one of the following providers and obtain an API key:  </p> <ul> <li>OpenAI </li> <li>Azure OpenAI </li> <li>Together AI </li> <li>Groq</li> <li>Or Integrate Your Own LLM Provider. (See Docs: Custom LLM Backend)</li> </ul> </li> <li> <p>Optional: Rust/Cargo Setup    The <code>neo4j-rust-ext</code> package may require Rust/Cargo for building from source if pre-built wheels are unavailable or fail to install on your system.  </p> <ul> <li>For Unix-like systems:     Run the following command in your terminal: <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre></li> <li>For other platforms or installation methods:     Refer to the Rust installation guide.  </li> </ul> <p>Note: This setup is necessary if you encounter build errors related to the <code>neo4j-rust-ext</code> or <code>py-rust-stemmers</code> package.</p> </li> </ol>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Install Memora using pip:</p> <pre><code>pip install memora-core\n</code></pre>"},{"location":"getting_started/#basic-setup","title":"Basic Setup","text":"<p>Here's how to initialize Memora with the databases and an LLM provider:</p> <pre><code>from memora import Memora\nfrom qdrant_client import AsyncQdrantClient\nfrom memora.vector_db import QdrantDB\nfrom memora.graph_db import Neo4jGraphInterface\nfrom memora.llm_backends import GroqBackendLLM\n\n# Initialize databases\nvector_db = QdrantDB(async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\"))\n\ngraph_db = Neo4jGraphInterface(uri=\"Neo4jURI\", username=\"Username\", password=\"Password\", database=\"DBName\")\n\n# Only call setup on the very first run - (creates necessary indexes, contraints, etc.)\nawait vector_db.setup()\nawait graph_db.setup()\n\n\n# Initialize Memora with Groq as the LLM provider\nmemora = Memora(\n    vector_db=vector_db,\n    graph_db=graph_db,\n    # Fast model for memory search queries / filtering.\n    memory_search_model=GroqBackendLLM(api_key=\"GROQ_API_KEY\", model=\"mixtral-8x7b-32768\"),\n    # Powerful model for memory extraction\n    extraction_model=GroqBackendLLM(api_key=\"GROQ_API_KEY\", model=\"llama-3.3-70b-versatile\", max_tokens=8000)\n)\n</code></pre>"},{"location":"getting_started/#creating-an-organization-user-and-agent","title":"Creating An Organization, User, and Agent","text":"<pre><code># Create an organization\norg = await memora.graph.create_organization(\"My Organization\")\norg_id = org.org_id # Short UUID e.g 'gmDr4sUiWMCqbGAiV8jjbU'\n\n# Create a user in the organization\nuser = await memora.graph.create_user(org_id, \"Alice\")\nuser_id = user.user_id # Short UUID e.g '89sSRXoz53gdWPJoTEEass'\n\n# Create an agent belonging to the organization and user (optionally).\nagent = await memora.graph.create_agent(org_id, \"Jenny, Personal AI\", user_id=user_id)\nagent_id = agent.agent_id # Short UUID e.g 'CcyKXxhi2skEcDpRzNZim7'\n</code></pre> <p>Important Note</p> <p>For organization, user, agent, and interaction IDs, we use <code>shortuuid.uuid()</code> to generate compact short UUIDs, a base57-encoded version of standard UUIDs. They are shorter but retain the same uniqueness.</p> <p>For memory IDs, we use the standard UUIDs <code>uuid.uuid4()</code>, as these are supported as vector IDs for the vast majority of vector databases.</p>"},{"location":"getting_started/#core-operations","title":"Core Operations","text":""},{"location":"getting_started/#1-saving-an-interaction-and-its-memories","title":"1. Saving an Interaction and its Memories","text":"<pre><code># Example interaction\ninteraction = [\n    {\"role\": \"user\", \"content\": \"I had another peanut incident today; it confirms I am allergic.\"},\n    {\"role\": \"assistant\", \"content\": \"Oh Jake, I'm sorry to hear that. Are you okay? I'll make sure to keep peanuts \ud83e\udd5c away from any future food-related situations.\"},\n    {\"role\": \"user\", \"content\": \"Yes, I'm okay, though it stressed out Sarah, my wife, which isn't cool because she is due in December.\"},\n    {\"role\": \"assistant\", \"content\": \"I'm glad you're fine now, and that's wonderful news about Sarah's pregnancy! Congratulations to you both. When in December is she due?\"},\n    {\"role\": \"user\", \"content\": \"December 15th.\"},\n    {\"role\": \"assistant\", \"content\": \"Congrats again, and feel free to bombard me with every baby question you have \ud83d\udc76\ud83c\udf7c, we're raising this baby together LOL \ud83d\ude09\"}\n]\n\n# Save the interaction and its memories\ninteraction_id, created_at_datetime = await memora.save_or_update_interaction_and_memories(\n    org_id=org_id,\n    user_id=user_id,\n    agent_id=agent_id,\n    interaction=interaction,\n    current_datetime=datetime.now()\n)\n</code></pre>"},{"location":"getting_started/#2-updating-an-existing-interaction-and-its-memories","title":"2. Updating An Existing Interaction and its Memories","text":"<p>The updated interaction will be compared with the existing one:</p> <ul> <li>If differences are found, truncates existing record from that point and     replaces with updated version. Old memories from truncated message(s)     remain but are not linked to the truncated message(s).</li> <li>If no differences, appends new messages from the update.</li> </ul> <pre><code>updated_interaction = interaction + [\n    {\"role\": \"user\", \"content\": \"Thanks! We're pretty sure it's a girl, but we'll know for certain at the next ultrasound.\"},\n    {\"role\": \"assistant\", \"content\": \"The anticipation must be building. Do you have any name ideas?.\"}\n]\n\n# Update the existing interaction (In this case it simply appends the new messages)\ninteraction_id, updated_at_datetime = await memora.save_or_update_interaction_and_memories(\n    org_id=org_id,\n    user_id=user_id,\n    agent_id=agent_id,\n    interaction=updated_interaction,\n    interaction_id=interaction_id,  # Pass the existing interaction_id to update\n    current_datetime=datetime.now()\n)\n</code></pre>"},{"location":"getting_started/#3-searching-memories","title":"3. Searching Memories","text":"<pre><code>from memora.vector_db.base import MemorySearchScope\n\n# Returns a consolidated list of results for all queries (sorted by relevance), instead of individual lists per query.\nmemories = await memora.search_memories_as_one(\n    org_id=org_id,\n    user_id=user_id,\n    search_queries=[\"Who is my wife?\", \"When is my wife due?\"],\n    search_across_agents=True\n)\n\n# memories: [\n# Memory(..., memory_id='uuid string', memory=\"Jake married Sarah on August 12th, 2023\", obtained_at=datetime(...), message_sources=[...]), \n# Memory(..., memory_id='uuid string', memory=\"Jake's wife Sarah is due on December 15th\", obtained_at=datetime(...), message_sources=[...])\n# ...]\n\n\n\n# Perform a batch search for memories, obtaining a list of results for each query, and can specify the search scope\nbatch_memories = await memora.search_memories_as_batch(\n    org_id=org_id,\n    search_queries=[\"user's allergies\", \"user's family details\"],\n    user_id=user_id,\n    memory_search_scope=MemorySearchScope.USER,  # Can be \"user\" or \"organization\"\n    search_across_agents=True\n)\n# batch_memories: [\n# [Memory(..., memory_id='uuid string', memory=\"Jake has confirmed he is allergic to peanuts\", obtained_at=datetime(...), message_sources=[...]), ...], \n# [Memory(..., memory_id='uuid string', memory=\"Jake's wife Sarah is due on December 15th\", obtained_at=datetime(...), message_sources=[...]), ...]\n#]\n</code></pre>"},{"location":"getting_started/#4-recall-memories-for-a-users-message-in-interaction","title":"4. Recall Memories for a User's Message in Interaction","text":"<pre><code>recalled_memories, just_memory_ids = await memora.recall_memories_for_message(\n    org_id,\n    user_id,\n    latest_msg=\"Sarah is really in pain more nowdays, so both of us can't sleep.\",\n    # Optional: Add previous messages in the interaction for context.\n    preceding_msg_for_context=[],\n    # Optional: Exclude previously recalled memories (e.g They are already in the conversation). See sample personal assistant below.\n    filter_out_memory_ids_set={'4b9df118-fa11-4e29-abfd-3b02587aa251'}  \n)\n\n# recalled_memories: [\n# Memory(..., memory_id='uuid string', memory=\"Jake's wife Sarah is due on December 15th\", obtained_at=datetime(...), message_sources=[...]),\n# Memory(..., memory_id='uuid string', memory=\"Jake and Sarah are pretty confident the baby\u2019s a girl but will confirm at the next ultrasound.\", obtained_at=datetime(...), message_sources=[...]),  \n# ...]\n\n# just_memory_ids: [\"uuid string\", \"uuid string\", ...]\n</code></pre>"},{"location":"getting_started/#5-managing-memories","title":"5. Managing Memories","text":"<pre><code># Get all memories for a user\nall_memories = await memora.graph.get_all_user_memories(org_id, user_id)\n\n# Get memories from a specific interaction\ninteraction_memories = await memora.graph.get_interaction(org_id, user_id, interaction_id, with_memories=True, with_messages=False)\n\n# Get the history of a specific memory, this contains all updates of a memory in descending order (starting with the latest version to the oldest)\nhistory = await memora.graph.get_user_memory_history(org_id, user_id, \"memory_uuid\")\n\n# Delete a specific memory\nawait memora.graph.delete_user_memory(org_id, user_id, \"memory_uuid\")\n</code></pre> <p>Note</p> <p>For more methods, see the <code>API Reference</code> page on your desired GraphDB implementation. (Just Neo4j for now.)</p>"},{"location":"getting_started/#a-simple-example","title":"A Simple Example","text":"<pre><code>from openai import AsyncOpenAI\n\n... # Preceding code where you have initialized memora, and stored org_id and user_id in variables.\n\n# Async client initialization\nclient = AsyncOpenAI(api_key=\"YourOpenAIAPIKey\")\n\nmessages=[{ \"role\": \"system\", \"content\": \"You are jake's assistant, given memories in 'memory recall: ...'\"}]\n\nuser_message = \"Hello, what is my wife's name ?\"\nrecalled_memories, _ = await memora.recall_memories_for_message(org_id, user_id, latest_msg=user_message)\n\ninclude_memory_in_message = \"\"\"\n    memory recall: {memories}\\n---\\nmessage: {message}\n\"\"\".format(memories=str([memory.memory_and_timestamp_dict() for memory in recalled_memories]), message=user_message)\n\nmessages.append({'role': 'user', 'content': include_memory_in_message})\nresponse = await client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\nprint(f\"&gt;&gt;&gt; Assistant Reply: {response.choices[0].message.content}\")\n</code></pre>"},{"location":"getting_started/#a-tad-bit-complex-personal-assistant-with-memora","title":"A Tad Bit Complex Personal Assistant with Memora","text":"<p>Here's a sample example of a personal assistant that using Memora:</p> <pre><code>from typing import *\nfrom groq import AsyncGroq\nfrom qdrant_client import AsyncQdrantClient\nfrom memora.vector_db import QdrantDB\nfrom memora.graph_db import Neo4jGraphInterface\nfrom memora.llm_backends import GroqBackendLLM\n\nclass PersonalAssistant:\n\n    def __init__(self, org_id: str, user_id: str, system_prompt: str):\n\n        self.org_id = org_id\n        self.user_id = user_id\n\n        # Initialize databases\n        vector_db = QdrantDB(async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\"))\n        graph_db = Neo4jGraphInterface(\n            uri=\"NEO4J_URI\",\n            username=\"NEO4J_USERNAME\", password=\"NEO4J_PASSWORD\",\n            database=\"NEO4J_DATABASE\"\n        )\n\n        self.memora = Memora(\n            vector_db=vector_db, graph_db=graph_db,\n            memory_search_model=GroqBackendLLM(api_key=\"GROQ_API_KEY\", model=\"mixtral-8x7b-32768\"),\n            extraction_model=GroqBackendLLM(api_key=\"GROQ_API_KEY\", model=\"llama-3.3-70b-versatile\", max_tokens=8000),\n        )\n\n        # We recommend using your LLM provider implementation (openai, groq client etc.) instead of BaseBackendLLM for the chat model to utilize features like streaming and tools.\n        self.chat_client = AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\n\n        # Track history: clean version without memory recalls. See \"Why Track Two histories?\" below.\n        self.base_history = [{\"role\": \"system\", \"content\": system_prompt}]\n\n        # Version with memory recalls for prompting\n        self.prompt_history = self.base_history.copy()\n        self.already_recalled_memory_ids: Set[str] = set()\n\n    async def chat(self, user_message: str) -&gt; str:\n\n        recalled_memories, recalled_memory_ids = await self.memora.recall_memories_for_message(\n            self.org_id, self.user_id,\n            user_message, preceding_msg_for_context=self.base_history[1:], # Exclude system prompt.\n            filter_out_memory_ids_set=self.already_recalled_memory_ids\n        )\n\n        include_memory_in_message = \"\"\"\n            memory recall: {memories}\\n---\\nmessage: {message}\n        \"\"\".format(memories=str([memory.memory_and_timestamp_dict() for memory in recalled_memories]), message=user_message)\n\n        # Get model response\n        response = await self.chat_client.chat.completions.create(\n            messages=self.prompt_history + [{\"role\": \"user\", \"content\": include_memory_in_message}],\n            model=\"llama-3.3-70b-versatile\",\n            stream=False\n        )\n        assistant_reply = response.choices[0].message.content\n\n        # Update conversation histories\n        self.base_history.extend([{\"role\": \"user\", \"content\": user_message}, {\"role\": \"assistant\", \"content\": assistant_reply}])\n\n        # Note: This version uses the message with memory recalled.\n        self.prompt_history.extend([{\"role\": \"user\", \"content\": include_memory_in_message}, {\"role\": \"assistant\", \"content\": assistant_reply}])\n\n        self.already_recalled_memory_ids.update(recalled_memory_ids or [])\n\n        return assistant_reply\n\n    async def save_interaction(self) -&gt; Tuple[str, datetime]:\n\n        interaction_id, created_at = await self.memora.save_or_update_interaction_and_memories(\n            self.org_id, self.user_id, \n            interaction=self.base_history[1:] # Always use the base_history with system prompt for saving / updating.\n        )\n        return interaction_id, created_at\n\n\nasync def main():\n\n    assistant = PersonalAssistant(\n        org_id, user_id, \n        \"You are jake's assistant, given memories in 'memory recall: ...'\"\n    )\n\n    while True:\n        msg = input(\"&gt;&gt;&gt; Jake: \")\n        if msg == \"quit()\":\n            break\n        print(f\"&gt;&gt;&gt; Assistant: {await assistant.chat(msg)}\")\n\n    interaction_id, created_at = await assistant.save_interaction()\n    print(f\"Interaction saved with ID: {interaction_id} and created at: {str(created_at)}\")\n</code></pre> Why Track Two histories? <p>We need two versions:</p> <ol> <li>Base History: Contains only the user message and the assistant's reply.</li> <li>Prompted History: Includes the recalled memories along with the user message and the assistant's reply. This version is used for prompting.</li> </ol> <p>The recalled memories are kept in the prompted history for many reasons one being so later in the same interaction if the user asks why the AI provided a particular reply, the AI can reference the memories listed under <code>memory recall</code> above that user message.</p> <p>When saving the interaction or using preceding messages for context in a memory search, we use the base history without the memory recalls. This is because for saving, we only need new memories from that interaction and when referencing preceding messages, we focus on what was previously said, so their memory recalls are not necessary.</p>"},{"location":"getting_started/#advanced-usage","title":"Advanced Usage","text":"<p>For more advanced usage, please refer to our Advanced Usage Guide.</p>"},{"location":"api/","title":"API Reference","text":"<p> This is the API Reference. For most users, the Getting Started or Advanced Usage guides are the go-to resources for effectively using the library. However, if you need a deeper understanding of Memora's components, you're in the right place.</p>"},{"location":"api/#components","title":"Components","text":""},{"location":"api/#agent","title":"Agent","text":"<p>The <code>Memora</code> class orchestrates all necessary memory operations and offers a unified interface for utilizing the Memory Agent.</p>"},{"location":"api/#graph-database","title":"Graph Database","text":"<p>Currently, our graph database implementation focuses on storing organizations, agents, users, their interactions with the agent (messages), and the memories derived from these interactions, all with date and time etc.</p>"},{"location":"api/#vector-database","title":"Vector Database","text":"<p>Enables semantic search across memories using dense vector embeddings for semantic similarity, combined with sparse text embeddings (like SPLADE) for hybrid search.</p>"},{"location":"api/#llm-backends","title":"LLM Backends","text":"<p>An interface for LLM providers, along with some implementations, that Memora can use in the backend for memory operations, such as understanding context, determining what to store and recall, linking memories to source messages and more.</p>"},{"location":"api/#schemas","title":"Schemas","text":"<p>Pydantic data models that are used for structured memory extraction, storage, and retrieval to ensure consistency and model structured output.</p>"},{"location":"api/agent/memora/","title":"<code>Memora</code> Class","text":"<p>Here's the reference information for the <code>Memora</code> class, with all its parameters, attributes and methods.</p>"},{"location":"api/agent/memora/#memora.Memora","title":"memora.Memora","text":"<pre><code>Memora(\n    vector_db: BaseVectorDB,\n    graph_db: BaseGraphDB,\n    memory_search_model: BaseBackendLLM,\n    extraction_model: BaseBackendLLM,\n    enable_logging: bool = False,\n)\n</code></pre> <p>This class orchestrates all necessary memory operations and offers a unified interface for utilizing the Memory Agent.</p> PARAMETER DESCRIPTION <code>vector_db</code> <p>Vector database.</p> <p> TYPE: <code>BaseVectorDB</code> </p> <code>graph_db</code> <p>Graph database.</p> <p> TYPE: <code>BaseGraphDB</code> </p> <code>memory_search_model</code> <p>Model for memory search queries and Optional final filtering.</p> <p> TYPE: <code>BaseBackendLLM</code> </p> <code>extraction_model</code> <p>Model for memory extraction operations.</p> <p> TYPE: <code>BaseBackendLLM</code> </p> <code>enable_logging</code> <p>Whether to enable console logging.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Note <p>The graph database will be associated with the vector database.</p> Source code in <code>memora/agent/main.py</code> <pre><code>def __init__(\n    self,\n    vector_db: BaseVectorDB,\n    graph_db: BaseGraphDB,\n    memory_search_model: BaseBackendLLM,\n    extraction_model: BaseBackendLLM,\n    enable_logging: bool = False,\n):\n    \"\"\"\n    Initialize the Memora instance.\n\n    Args:\n        vector_db (BaseVectorDB): Vector database.\n        graph_db (BaseGraphDB): Graph database.\n        memory_search_model (BaseBackendLLM): Model for memory search queries and Optional final filtering.\n        extraction_model (BaseBackendLLM): Model for memory extraction operations.\n        enable_logging (bool): Whether to enable console logging.\n\n    Note:\n        The graph database will be associated with the vector database.\n    \"\"\"\n\n    self.memory_search_model = memory_search_model\n    self.extraction_model = extraction_model\n    self.vector_db = vector_db\n    self.graph = graph_db\n\n    # Associate the vector database with the graph database.\n    self.graph.associated_vector_db = self.vector_db\n\n    self.logger = logging.getLogger(__name__)\n    if enable_logging:\n        logging.basicConfig(level=logging.INFO)\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora-attributes","title":"Attributes","text":""},{"location":"api/agent/memora/#memora.Memora.extraction_model","title":"extraction_model  <code>instance-attribute</code>","text":"<pre><code>extraction_model = extraction_model\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.graph","title":"graph  <code>instance-attribute</code>","text":"<pre><code>graph = graph_db\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.memory_search_model","title":"memory_search_model  <code>instance-attribute</code>","text":"<pre><code>memory_search_model = memory_search_model\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.vector_db","title":"vector_db  <code>instance-attribute</code>","text":"<pre><code>vector_db = vector_db\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora-functions","title":"Functions","text":""},{"location":"api/agent/memora/#memora.Memora.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close and clean up resources used by Memora.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close and clean up resources used by Memora.\"\"\"\n\n    await self.vector_db.close()\n    await self.graph.close()\n    await self.memory_search_model.close()\n    await self.extraction_model.close()\n    self.logger.info(\"Memora resources cleaned.\")\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.filter_retrieved_memories_with_model","title":"filter_retrieved_memories_with_model  <code>async</code>","text":"<pre><code>filter_retrieved_memories_with_model(\n    message: str,\n    search_queries_used: List[str],\n    retrieved_memories: List[models.Memory],\n    current_datetime: datetime = datetime.now(),\n) -&gt; Set[str] | None\n</code></pre> <p>Filter retrieved memories using the memory search model.</p> PARAMETER DESCRIPTION <code>message</code> <p>The message that triggered the search queries and retrieved memories.</p> <p> TYPE: <code>str</code> </p> <code>search_queries_used</code> <p>List of search queries used.</p> <p> TYPE: <code>List[str]</code> </p> <code>retrieved_memories</code> <p>List of retrieved memories.</p> <p> TYPE: <code>List[Memory]</code> </p> <code>current_datetime</code> <p>Current datetime.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>now()</code> </p> RETURNS DESCRIPTION <code>Set[str] | None</code> <p>Set[str] | None: Distinct Set of selected memory IDs (that passed the filter), or None if LLM was unable to analyze and select.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def filter_retrieved_memories_with_model(\n    self,\n    message: str,\n    search_queries_used: List[str],\n    retrieved_memories: List[models.Memory],\n    current_datetime: datetime = datetime.now(),\n) -&gt; Set[str] | None:\n    \"\"\"\n    Filter retrieved memories using the memory search model.\n\n    Args:\n        message (str): The message that triggered the search queries and retrieved memories.\n        search_queries_used (List[str]): List of search queries used.\n        retrieved_memories (List[Memory]): List of retrieved memories.\n        current_datetime (datetime): Current datetime.\n\n    Returns:\n        Set[str] | None: Distinct Set of selected memory IDs (that passed the filter), or None if LLM was unable to analyze and select.\n    \"\"\"\n\n    self.logger.info(f\"Starting memory filtering for message: {message[:100]}...\")\n    self.logger.debug(f\"Number of search queries used: {len(search_queries_used)}\")\n    self.logger.debug(\n        f\"Number of retrieved memories to filter: {len(retrieved_memories)}\"\n    )\n\n    current_day_of_week = current_datetime.strftime(\"%A\")\n    self.logger.debug(\n        f\"Current day of week: {current_day_of_week}, datetime: {current_datetime.isoformat()}\"\n    )\n\n    self.logger.info(\"Calling memory search model for filtering...\")\n\n    # Just use only needed information from retrieved memories.\n    retrieved_memories: List[Dict[str, str]] = [\n        memoryObj.id_memory_and_timestamp_dict() for memoryObj in retrieved_memories\n    ]\n\n    response = await self.memory_search_model(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": FILTER_RETRIEVED_MEMORIES_SYSTEM_PROMPT.format(\n                    day_of_week=current_day_of_week,\n                    current_datetime_str=current_datetime.isoformat(),\n                    latest_room_message=str(message),\n                    memory_search_queries=\"\\n- \".join(search_queries_used),\n                ),\n            },\n            {\"role\": \"user\", \"content\": str(retrieved_memories)},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"REASONS AND JUST memory_id enclosed in (&lt;&lt; &gt;&gt;):\\n- Reason: \",\n            },  # For Guided Response.\n        ]\n    )\n\n    selected_id_pattern = re.compile(r\"&lt;&lt;(.*?)&gt;&gt;\", re.DOTALL)\n    selected_memories_ids = selected_id_pattern.findall(response)\n\n    if (\n        not selected_memories_ids\n    ):  # The LLM misbehaved not extracting any memory_ids or &lt;&lt; NONE &gt;&gt;.\n        self.logger.warning(\n            \"No memory IDs were extracted from the model response, due to LLM misbehavior.\"\n        )\n        return None\n\n    # The LLM is undeterministic and can select the same memory_ids multiple times.\n    filtered_ids = set(\n        [\n            str(selection).strip()\n            for selection in selected_memories_ids\n            if str(selection).strip().lower() != \"none\"\n        ]\n    )\n    self.logger.info(\n        f\"Memory filtering complete. Selected {len(filtered_ids)} unique memories\"\n    )\n    self.logger.debug(f\"Selected memory IDs: {filtered_ids}\")\n\n    return filtered_ids\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.generate_memory_search_queries","title":"generate_memory_search_queries  <code>async</code>","text":"<pre><code>generate_memory_search_queries(\n    message: str,\n    preceding_messages_for_context: List[\n        Dict[str, str]\n    ] = [],\n    current_datetime: datetime = datetime.now(),\n) -&gt; List[str]\n</code></pre> <p>Generate memory search queries based on the given message and context.</p> PARAMETER DESCRIPTION <code>message</code> <p>The message to recall memories for.</p> <p> TYPE: <code>str</code> </p> <code>preceding_messages_for_context</code> <p>Preceding messages for context.</p> <p> TYPE: <code>List[Dict[str, str]]</code> DEFAULT: <code>[]</code> </p> <code>current_datetime</code> <p>Current datetime.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>now()</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List[str]: List of generated memory search queries.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def generate_memory_search_queries(\n    self,\n    message: str,\n    preceding_messages_for_context: List[Dict[str, str]] = [],\n    current_datetime: datetime = datetime.now(),\n) -&gt; List[str]:\n    \"\"\"\n    Generate memory search queries based on the given message and context.\n\n    Args:\n        message (str): The message to recall memories for.\n        preceding_messages_for_context (List[Dict[str, str]]): Preceding messages for context.\n        current_datetime (datetime): Current datetime.\n\n    Returns:\n        List[str]: List of generated memory search queries.\n    \"\"\"\n\n    current_day_of_week = current_datetime.strftime(\"%A\")\n    response = await self.memory_search_model(\n        messages=[\n            {\"role\": \"system\", \"content\": MSG_MEMORY_SEARCH_PROMPT},\n            {\n                \"role\": \"user\",\n                \"content\": MSG_MEMORY_SEARCH_TEMPLATE.format(\n                    day_of_week=current_day_of_week,\n                    current_datetime_str=current_datetime.isoformat(),\n                    message_of_user=str(message),\n                    preceding_messages=str(preceding_messages_for_context),\n                ),\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"&amp;&amp; MEMORY_SEARCH &amp;&amp;\",\n            },  # For Guided Response.\n        ]\n    )\n\n    arguments_pattern = re.compile(r\"&lt;&lt;(.*?)&gt;&gt;\", re.DOTALL)\n    arguments_passed = arguments_pattern.findall(response)\n    memory_search_queries = (\n        [arg.strip() for arg in arguments_passed] if arguments_passed else []\n    )\n\n    self.logger.info(f\"Generated memory search queries: {memory_search_queries}\")\n\n    return memory_search_queries\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.recall_memories_for_message","title":"recall_memories_for_message  <code>async</code>","text":"<pre><code>recall_memories_for_message(\n    org_id: str,\n    user_id: str,\n    latest_msg: str,\n    agent_id: Optional[str] = None,\n    preceding_msg_for_context: List[Dict[str, str]] = [],\n    current_datetime: datetime = datetime.now(),\n    filter_out_memory_ids_set: Set[str] = set(),\n    search_memories_across_agents: bool = True,\n    enable_final_model_based_memory_filter: bool = False,\n) -&gt; Tuple[List[models.Memory] | None, List[str] | None]\n</code></pre> <p>Recall memories for the given message.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>str</code> </p> <code>latest_msg</code> <p>The latest message from user to find memories for.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Agent ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>preceding_msg_for_context</code> <p>Preceding messages for context.</p> <p> TYPE: <code>List[Dict[str, str]]</code> DEFAULT: <code>[]</code> </p> <code>current_datetime</code> <p>Current datetime.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>now()</code> </p> <code>filter_out_memory_ids_set</code> <p>Set of memory IDs to filter out.</p> <p> TYPE: <code>Set[str]</code> DEFAULT: <code>set()</code> </p> <code>search_memories_across_agents</code> <p>Whether to search memories across all agents.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>enable_final_model_based_memory_filter</code> <p>\ud83d\udcdd Experimental feature; enables filtering of retrieved memories using a model. Note that a small model (~ 8B or lower) might not select some memories that are indirectly needed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[List[Memory] | None, List[str] | None]</code> <p>Tuple[List[Memory] | None, List[str] | None]:</p> <ul> <li> <p>List[Memory]: Containing Memories to be recalled (None if no relevant memories found):</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> </li> <li> <p>List[str]: Just the memory IDs (empty list [] if no memory was recalled).</p> </li> </ul> Source code in <code>memora/agent/main.py</code> <pre><code>async def recall_memories_for_message(\n    self,\n    org_id: str,\n    user_id: str,\n    latest_msg: str,\n    agent_id: Optional[str] = None,\n    preceding_msg_for_context: List[Dict[str, str]] = [],\n    current_datetime: datetime = datetime.now(),\n    filter_out_memory_ids_set: Set[str] = set(),\n    search_memories_across_agents: bool = True,\n    enable_final_model_based_memory_filter: bool = False,\n) -&gt; Tuple[List[models.Memory] | None, List[str] | None]:\n    \"\"\"\n    Recall memories for the given message.\n\n    Args:\n        org_id (str): Organization ID.\n        user_id (str): User ID.\n        latest_msg (str): The latest message from user to find memories for.\n        agent_id (Optional[str]): Agent ID.\n        preceding_msg_for_context (List[Dict[str, str]]): Preceding messages for context.\n        current_datetime (datetime): Current datetime.\n        filter_out_memory_ids_set (Set[str]): Set of memory IDs to filter out.\n        search_memories_across_agents (bool): Whether to search memories across all agents.\n        enable_final_model_based_memory_filter (bool): \ud83d\udcdd Experimental feature; enables filtering of retrieved memories using a model. Note that a small model (~ 8B or lower) might not select some memories that are indirectly needed.\n\n    Returns:\n        Tuple[List[Memory] | None, List[str] | None]:\n\n            + List[Memory]: Containing Memories to be recalled (None if no relevant memories found):\n\n                + org_id: Short UUID string identifying the organization\n                + agent_id: Short UUID string identifying the agent\n                + user_id: Short UUID string identifying the user\n                + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n                + memory_id: Full UUID string identifying the memory\n                + memory: The resolved memory\n                + obtained_at: DateTime object of when the memory was obtained\n                + message_sources: List of messages in the interaction that triggered the memory\n\n            + List[str]: Just the memory IDs (empty list [] if no memory was recalled).\n    \"\"\"\n\n    self.logger.info(\n        f\"Getting memories for message from user {user_id} in org {org_id}\"\n    )\n    self.logger.debug(\n        f\"Message context - agent_id: {agent_id}, context messages: {len(preceding_msg_for_context)}\"\n    )\n    self.logger.debug(\n        f\"Model-based filtering enabled: {enable_final_model_based_memory_filter}\"\n    )\n\n    self.logger.info(\"Generating memory search queries\")\n    search_queries = await self.generate_memory_search_queries(\n        message=latest_msg,\n        preceding_messages_for_context=preceding_msg_for_context,\n        current_datetime=current_datetime,\n    )\n    self.logger.debug(f\"Generated {len(search_queries)} search queries\")\n\n    if not search_queries:\n        self.logger.warning(\"No search queries generated\")\n        search_queries = [latest_msg]  # Use the latest message as a fallback.\n\n    self.logger.info(\"Searching memories based on generated queries\")\n\n    retrieved_memories = await self.search_memories_as_one(\n        org_id=org_id,\n        user_id=user_id,\n        search_queries=search_queries,\n        filter_out_memory_ids_set=filter_out_memory_ids_set,\n        agent_id=agent_id,\n        search_across_agents=search_memories_across_agents,\n    )\n\n    if not retrieved_memories:\n        self.logger.info(\"No memories found for the message\")\n        return None, None\n\n    self.logger.info(f\"Retrieved {len(retrieved_memories)} memories\")\n\n    if not enable_final_model_based_memory_filter:\n        return (\n            retrieved_memories,  # memories.\n            [memory.memory_id for memory in retrieved_memories],  # memory ids.\n        )\n\n    self.logger.info(\"Applying model-based memory filtering\")\n    filtered_memory_ids = await self.filter_retrieved_memories_with_model(\n        latest_msg, search_queries, retrieved_memories, current_datetime\n    )\n\n    if (\n        filtered_memory_ids is None\n    ):  # The LLM was unable to filter just needed memories.\n        self.logger.info(\"Model-based filtering failed\")\n        return (\n            retrieved_memories,  # memories.\n            [memory.memory_id for memory in retrieved_memories],  # memory ids.\n        )\n\n    if (\n        len(filtered_memory_ids) == 0\n    ):  # The LLM filtered out all memories (deemed none are needed to be recalled).\n        self.logger.info(\"Model-based filtering returned no memories\")\n        return None, None\n\n    memory_dict = {memory.memory_id: memory for memory in retrieved_memories}\n    selected_memories = [\n        memoryObj\n        for memory_id in filtered_memory_ids\n        if (memoryObj := memory_dict.get(memory_id)) is not None\n    ]\n\n    self.logger.info(\n        f\"Selected {len(selected_memories)} memories after model-based filtering\"\n    )\n    return selected_memories, list(filtered_memory_ids)\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.save_or_update_interaction_and_memories","title":"save_or_update_interaction_and_memories  <code>async</code>","text":"<pre><code>save_or_update_interaction_and_memories(\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    interaction: List[Dict[str, str]],\n    interaction_id: Optional[str] = None,\n    current_datetime: datetime = datetime.now(),\n    extract_agent_memories: bool = False,\n    update_across_agents: bool = True,\n    max_retries: int = 3,\n) -&gt; Tuple[str, datetime]\n</code></pre> <p>Save a new interaction or update an existing one, and the extracted memories.</p> <p>FOR UPDATES -&gt; Will Compare updated interaction with existing one:     - If differences are found, truncates existing record from that point and     replaces with updated version. Old memories from truncated message(s)     remain but become standalone (no longer linked to truncated messages).     - If no differences, appends new messages from the update.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Agent ID.</p> <p> TYPE: <code>str</code> </p> <code>interaction</code> <p>List of interaction messages.</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>interaction_id</code> <p>Interaction ID for updates.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>current_datetime</code> <p>Current datetime.</p> <p> TYPE: <code>datetime</code> DEFAULT: <code>now()</code> </p> <code>extract_agent_memories</code> <p>Whether to extract agent memories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>update_across_agents</code> <p>Whether to update memories across all agents.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_retries</code> <p>Maximum number of retries.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>Tuple[str, datetime]</code> <p>Tuple[str, datetime] containing:</p> <ul> <li>interaction_id: Short UUID string</li> <li>created_at or updated_at: Datetime object</li> </ul> RAISES DESCRIPTION <code>Exception</code> <p>If saving the interaction and its memories fails after max retries.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def save_or_update_interaction_and_memories(\n    self,\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    interaction: List[Dict[str, str]],\n    interaction_id: Optional[str] = None,\n    current_datetime: datetime = datetime.now(),\n    extract_agent_memories: bool = False,\n    update_across_agents: bool = True,\n    max_retries: int = 3,\n) -&gt; Tuple[str, datetime]:\n    \"\"\"\n    Save a new interaction or update an existing one, and the extracted memories.\n\n    FOR UPDATES -&gt; Will Compare updated interaction with existing one:\n        - If differences are found, truncates existing record from that point and\n        replaces with updated version. Old memories from truncated message(s)\n        remain but become standalone (no longer linked to truncated messages).\n        - If no differences, appends new messages from the update.\n\n    Args:\n        org_id (str): Organization ID.\n        user_id (str): User ID.\n        agent_id (str): Agent ID.\n        interaction (List[Dict[str, str]]): List of interaction messages.\n        interaction_id (Optional[str]): Interaction ID for updates.\n        current_datetime (datetime): Current datetime.\n        extract_agent_memories (bool): Whether to extract agent memories.\n        update_across_agents (bool): Whether to update memories across all agents.\n        max_retries (int): Maximum number of retries.\n\n    Returns:\n        Tuple[str, datetime] containing:\n\n            + interaction_id: Short UUID string\n            + created_at or updated_at: Datetime object\n\n    Raises:\n        Exception: If saving the interaction and its memories fails after max retries.\n    \"\"\"\n    operation = \"Updating\" if interaction_id else \"Saving\"\n    self.logger.info(f\"{operation} interaction for user {user_id} in org {org_id}\")\n    self.logger.debug(\n        f\"Interaction context - agent_id: {agent_id}, extract_agent_memories: {extract_agent_memories}\"\n    )\n    self.logger.debug(f\"Interaction size: {len(interaction)} messages\")\n\n    for retry in range(max_retries + 1):\n        try:\n            self.logger.debug(f\"Attempt {retry + 1}/{max_retries + 1}\")\n            user, agent = await self._get_user_and_agent(org_id, user_id, agent_id)\n\n            if interaction_id:\n                self.logger.debug(\n                    f\"Fetching previously extracted memories for interaction {interaction_id}\"\n                )\n                previously_extracted_memories: List[Dict[str, str]] = [\n                    memoryObj.memory_and_timestamp_dict()\n                    for memoryObj in (\n                        (\n                            await self.graph.get_interaction(\n                                org_id,\n                                user_id,\n                                interaction_id,\n                                with_messages=False,\n                                with_memories=True,\n                            )\n                        ).memories\n                        or []\n                    )\n                ]\n\n                self.logger.debug(\n                    f\"Found {len(previously_extracted_memories)} previously extracted memories\"\n                )\n\n                system_content = MEMORY_EXTRACTION_UPDATE_SYSTEM_PROMPT.format(\n                    day_of_week=current_datetime.strftime(\"%A\"),\n                    current_datetime_str=current_datetime.isoformat(),\n                    agent_label=agent.agent_label,\n                    user_name=user.user_name,\n                    extract_for_agent=(\n                        f\"and {agent.agent_label}\" if extract_agent_memories else \"\"\n                    ),\n                    previous_memories=str(previously_extracted_memories),\n                    schema=MemoryExtractionResponse.model_json_schema(),\n                )\n            else:\n\n                system_content = MEMORY_EXTRACTION_SYSTEM_PROMPT.format(\n                    day_of_week=current_datetime.strftime(\"%A\"),\n                    current_datetime_str=current_datetime.isoformat(),\n                    agent_label=agent.agent_label,\n                    user_name=user.user_name,\n                    extract_for_agent=(\n                        f\"and {agent.agent_label}\" if extract_agent_memories else \"\"\n                    ),\n                    schema=MemoryExtractionResponse.model_json_schema(),\n                )\n\n            self.logger.debug(\"Preparing messages for memory extraction\")\n            messages = [{\"role\": \"system\", \"content\": system_content}]\n            messages += [\n                {\n                    \"role\": msg[\"role\"],\n                    \"content\": EXTRACTION_MSG_BLOCK_FORMAT.format(\n                        message_id=i, content=msg[\"content\"]\n                    ),\n                }\n                for i, msg in enumerate(interaction)\n            ]\n\n            self.logger.info(\"Extracting memories from interaction\")\n            response: MemoryExtractionResponse = await self.extraction_model(\n                messages=messages, output_schema_model=MemoryExtractionResponse\n            )\n\n            candidate_memories, candidate_memories_msg_sources = (\n                self._process_extracted_memories(response, user, agent)\n            )\n            self.logger.debug(\n                f\"Extracted {len(candidate_memories)} candidate memories\"\n            )\n\n            if not candidate_memories:\n                self.logger.info(\"No useful information extracted for memories\")\n                if interaction_id:\n                    return await self.graph.update_interaction_and_memories(\n                        org_id,\n                        agent_id,\n                        user_id,\n                        interaction_id,\n                        updated_memories_and_interaction=MemoriesAndInteraction(\n                            interaction=interaction,\n                            interaction_date=current_datetime,\n                            memories=[],\n                            contrary_memories=[],\n                        ),\n                    )\n                else:\n                    return await self.graph.save_interaction_with_memories(\n                        org_id,\n                        agent_id,\n                        user_id,\n                        memories_and_interaction=MemoriesAndInteraction(\n                            interaction=interaction,\n                            interaction_date=current_datetime,\n                            memories=[],\n                            contrary_memories=[],\n                        ),\n                    )\n\n            self.logger.info(\"Searching for existing related memories\")\n            existing_memories = await self.search_memories_as_one(\n                org_id=org_id,\n                user_id=user_id,\n                search_queries=candidate_memories,\n                agent_id=agent_id,\n                search_across_agents=update_across_agents,\n            )\n\n            if not existing_memories:\n                self.logger.info(\"No related existing memories found\")\n                if interaction_id:\n                    return await self.graph.update_interaction_and_memories(\n                        org_id,\n                        agent_id,\n                        user_id,\n                        interaction_id,\n                        updated_memories_and_interaction=MemoriesAndInteraction(\n                            interaction=interaction,\n                            interaction_date=current_datetime,\n                            memories=[\n                                MemoryToStore(\n                                    memory=memory, source_msg_block_pos=source_msgs\n                                )\n                                for memory, source_msgs in zip(\n                                    candidate_memories,\n                                    candidate_memories_msg_sources,\n                                )\n                            ],\n                            contrary_memories=[],\n                        ),\n                    )\n                else:\n                    return await self.graph.save_interaction_with_memories(\n                        org_id,\n                        agent_id,\n                        user_id,\n                        memories_and_interaction=MemoriesAndInteraction(\n                            interaction=interaction,\n                            interaction_date=current_datetime,\n                            memories=[\n                                MemoryToStore(\n                                    memory=memory, source_msg_block_pos=source_msgs\n                                )\n                                for memory, source_msgs in zip(\n                                    candidate_memories,\n                                    candidate_memories_msg_sources,\n                                )\n                            ],\n                            contrary_memories=[],\n                        ),\n                    )\n\n            candidate_memories = [\n                {\"memory\": memory, \"POS_ID\": i}\n                for i, memory in enumerate(candidate_memories)\n            ]\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": COMPARE_EXISTING_AND_NEW_MEMORIES_SYSTEM_PROMPT.format(\n                        day_of_week=current_datetime.strftime(\"%A\"),\n                        current_datetime_str=current_datetime.isoformat(),\n                        agent_placeholder=f\"agent_{agent.agent_id}\",\n                        user_placeholder=f\"user_{user.user_id}\",\n                        schema=MemoryComparisonResponse.model_json_schema(),\n                    ),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": COMPARE_EXISTING_AND_NEW_MEMORIES_INPUT_TEMPLATE.format(\n                        existing_memories_string=str(existing_memories),\n                        new_memories_string=str(candidate_memories),\n                    ),\n                },\n            ]\n\n            response: MemoryComparisonResponse = await self.extraction_model(\n                messages=messages, output_schema_model=MemoryComparisonResponse\n            )\n\n            new_memories = []\n            for memory in response.new_memories:\n                try:\n                    new_memories.append(\n                        (\n                            memory.memory,\n                            candidate_memories_msg_sources[\n                                memory.source_candidate_pos_id\n                            ],\n                        )\n                    )\n                except Exception:\n                    continue\n\n            new_contrary_memories = []\n            for memory in response.contrary_memories:\n                try:\n                    new_contrary_memories.append(\n                        (\n                            memory.memory,\n                            candidate_memories_msg_sources[\n                                memory.source_candidate_pos_id\n                            ],\n                            memory.contradicted_memory_id,\n                        )\n                    )\n                except Exception:\n                    continue\n\n            if interaction_id:\n                return await self.graph.update_interaction_and_memories(\n                    org_id,\n                    agent_id,\n                    user_id,\n                    interaction_id,\n                    updated_memories_and_interaction=MemoriesAndInteraction(\n                        interaction=interaction,\n                        interaction_date=current_datetime,\n                        memories=[\n                            MemoryToStore(\n                                memory=memory_tuple[0],\n                                source_msg_block_pos=memory_tuple[1],\n                            )\n                            for memory_tuple in new_memories\n                        ],\n                        contrary_memories=[\n                            ContraryMemoryToStore(\n                                memory=memory_tuple[0],\n                                source_msg_block_pos=memory_tuple[1],\n                                existing_contrary_memory_id=memory_tuple[2],\n                            )\n                            for memory_tuple in new_contrary_memories\n                        ],\n                    ),\n                )\n            else:\n                return await self.graph.save_interaction_with_memories(\n                    org_id,\n                    agent_id,\n                    user_id,\n                    memories_and_interaction=MemoriesAndInteraction(\n                        interaction=interaction,\n                        interaction_date=current_datetime,\n                        memories=[\n                            MemoryToStore(\n                                memory=memory_tuple[0],\n                                source_msg_block_pos=memory_tuple[1],\n                            )\n                            for memory_tuple in new_memories\n                        ],\n                        contrary_memories=[\n                            ContraryMemoryToStore(\n                                memory=memory_tuple[0],\n                                source_msg_block_pos=memory_tuple[1],\n                                existing_contrary_memory_id=memory_tuple[2],\n                            )\n                            for memory_tuple in new_contrary_memories\n                        ],\n                    ),\n                )\n\n        except Exception:\n            if retry == max_retries:\n                self.logger.error(\n                    f\"Failed to save/update interaction after {max_retries} retries\",\n                    exc_info=True,\n                )\n                raise\n            else:\n                self.logger.warning(\n                    f\"Attempt {retry + 1} failed, retrying...\", exc_info=True\n                )\n                continue\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.search_memories_as_batch","title":"search_memories_as_batch  <code>async</code>","text":"<pre><code>search_memories_as_batch(\n    org_id: str,\n    search_queries: List[str],\n    filter_out_memory_ids_set: Set[str] = set(),\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n    memory_search_scope: MemorySearchScope = MemorySearchScope.USER,\n    search_across_agents: bool = True,\n) -&gt; List[List[models.Memory]]\n</code></pre> <p>Retrieve memories corresponding to a list of search queries.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID.</p> <p> TYPE: <code>str</code> </p> <code>search_queries</code> <p>List of search queries.</p> <p> TYPE: <code>List[str]</code> </p> <code>filter_out_memory_ids_set</code> <p>Set of memory IDs to filter out.</p> <p> TYPE: <code>Set[str]</code> DEFAULT: <code>set()</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>agent_id</code> <p>Agent ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>memory_search_scope</code> <p>Scope of memory search.</p> <p> TYPE: <code>MemorySearchScope</code> DEFAULT: <code>USER</code> </p> <code>search_across_agents</code> <p>Whether to search memories across all agents.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[List[Memory]]</code> <p>List[List[Memory]]: Batch results of retrieved memories.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def search_memories_as_batch(\n    self,\n    org_id: str,\n    search_queries: List[str],\n    filter_out_memory_ids_set: Set[str] = set(),\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n    memory_search_scope: MemorySearchScope = MemorySearchScope.USER,\n    search_across_agents: bool = True,\n) -&gt; List[List[models.Memory]]:\n    \"\"\"\n    Retrieve memories corresponding to a list of search queries.\n\n    Args:\n        org_id (str): Organization ID.\n        search_queries (List[str]): List of search queries.\n        filter_out_memory_ids_set (Set[str]): Set of memory IDs to filter out.\n        user_id (Optional[str]): User ID.\n        agent_id (Optional[str]): Agent ID.\n        memory_search_scope (MemorySearchScope): Scope of memory search.\n        search_across_agents (bool): Whether to search memories across all agents.\n\n    Returns:\n        List[List[Memory]]: Batch results of retrieved memories.\n    \"\"\"\n\n    self.logger.info(f\"Batch searching memories in org {org_id}\")\n    self.logger.debug(\n        f\"Search context - user_id: {user_id}, agent_id: {agent_id}, scope: {memory_search_scope}\"\n    )\n    self.logger.debug(f\"Number of search queries: {len(search_queries)}\")\n\n    batch_results = await self.vector_db.search_memories(\n        queries=search_queries,\n        memory_search_scope=memory_search_scope,\n        org_id=org_id,\n        user_id=user_id,\n        agent_id=agent_id if not search_across_agents else None,\n    )\n\n    batch_org_user_mem_ids = [\n        [\n            {\n                \"memory_id\": memory[0].memory_id,\n                \"user_id\": memory[0].user_id,\n                \"org_id\": memory[0].org_id,\n            }\n            for memory in result\n            if memory[0].memory_id not in filter_out_memory_ids_set\n        ]\n        for result in batch_results\n    ]\n\n    if not any(batch_org_user_mem_ids):\n        self.logger.info(\n            \"No memories retrieved or left after filtering out `filter_out_memory_ids_set`\"\n        )\n        return []\n\n    return await self.graph.fetch_user_memories_resolved_batch(\n        batch_org_user_mem_ids\n    )\n</code></pre>"},{"location":"api/agent/memora/#memora.Memora.search_memories_as_one","title":"search_memories_as_one  <code>async</code>","text":"<pre><code>search_memories_as_one(\n    org_id: str,\n    user_id: str,\n    search_queries: List[str],\n    filter_out_memory_ids_set: Set[str] = set(),\n    agent_id: Optional[str] = None,\n    search_across_agents: bool = True,\n) -&gt; List[models.Memory]\n</code></pre> <p>Retrieve memories corresponding to the search queries for a message.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>str</code> </p> <code>search_queries</code> <p>List of search queries.</p> <p> TYPE: <code>List[str]</code> </p> <code>filter_out_memory_ids_set</code> <p>Set of memory IDs to filter out.</p> <p> TYPE: <code>Set[str]</code> DEFAULT: <code>set()</code> </p> <code>agent_id</code> <p>Agent ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>search_across_agents</code> <p>Whether to search memories across all agents.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory]: List of retrieved memories.</p> Source code in <code>memora/agent/main.py</code> <pre><code>async def search_memories_as_one(\n    self,\n    org_id: str,\n    user_id: str,\n    search_queries: List[str],\n    filter_out_memory_ids_set: Set[str] = set(),\n    agent_id: Optional[str] = None,\n    search_across_agents: bool = True,\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Retrieve memories corresponding to the search queries for a message.\n\n    Args:\n        org_id (str): Organization ID.\n        user_id (str): User ID.\n        search_queries (List[str]): List of search queries.\n        filter_out_memory_ids_set (Set[str]): Set of memory IDs to filter out.\n        agent_id (Optional[str]): Agent ID.\n        search_across_agents (bool): Whether to search memories across all agents.\n\n    Returns:\n        List[Memory]: List of retrieved memories.\n    \"\"\"\n\n    self.logger.info(f\"Searching memories for user {user_id} in org {org_id}\")\n    self.logger.debug(f\"Search queries: {search_queries}\")\n    self.logger.debug(\n        f\"Agent context - agent_id: {agent_id}, memories_across_agents: {search_across_agents}\"\n    )\n\n    batch_results = await self.vector_db.search_memories(\n        queries=search_queries,\n        memory_search_scope=MemorySearchScope.USER,\n        org_id=org_id,\n        user_id=user_id,\n        agent_id=agent_id if not search_across_agents else None,\n    )\n\n    # Flatten and sort memories by score across the batch results\n    sorted_memories = sorted(\n        [\n            memory_and_score\n            for result in batch_results\n            for memory_and_score in result\n        ],\n        key=lambda x: x[1],\n        reverse=True,\n    )\n\n    # Extract the (org, user and memory ids), filtering out the ones to be excluded\n    org_user_mem_ids = [\n        {\n            \"memory_id\": memory[0].memory_id,\n            \"user_id\": memory[0].user_id,\n            \"org_id\": memory[0].org_id,\n        }\n        for memory in sorted_memories\n        if memory[0].memory_id not in filter_out_memory_ids_set\n    ]\n\n    if not org_user_mem_ids:\n        self.logger.info(\n            \"No memories retrieved or left after filtering out `filter_out_memory_ids_set`\"\n        )\n        return []\n\n    return await self.graph.fetch_user_memories_resolved(org_user_mem_ids)\n</code></pre>"},{"location":"api/graph_db/base/","title":"<code>BaseGraphDB</code> Interface Class","text":"<p>This interface defines the contract that all graph database implementations must follow.</p>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB","title":"memora.graph_db.base.BaseGraphDB","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining a common interface for different Graph DB implementations.</p> <p>This class provides a standardized interface for graph database operations, including creating, retrieving, and deleting memory nodes and relationships.</p>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB-functions","title":"Functions","text":""},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the database connection.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Closes the database connection.\"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.create_agent","title":"create_agent  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>create_agent(\n    org_id: str,\n    agent_label: str,\n    user_id: Optional[str] = None,\n) -&gt; models.Agent\n</code></pre> <p>Creates a new agent in the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_label</code> <p>Label/name for the agent.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional Short UUID of the user. This is used when the agent is created specifically for a user, indicating that both the organization and the user will have this agent.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def create_agent(\n    self, org_id: str, agent_label: str, user_id: Optional[str] = None\n) -&gt; models.Agent:\n    \"\"\"\n    Creates a new agent in the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_label (str): Label/name for the agent.\n        user_id (Optional[str]): Optional Short UUID of the user. This is used when the agent is created\n            specifically for a user, indicating that both the organization and the\n            user will have this agent.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.create_organization","title":"create_organization  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>create_organization(org_name: str) -&gt; models.Organization\n</code></pre> <p>Creates a new organization in the graph database.</p> PARAMETER DESCRIPTION <code>org_name</code> <p>The name of the organization to create.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def create_organization(self, org_name: str) -&gt; models.Organization:\n    \"\"\"\n    Creates a new organization in the graph database.\n\n    Args:\n        org_name (str): The name of the organization to create.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.create_user","title":"create_user  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>create_user(org_id: str, user_name: str) -&gt; models.User\n</code></pre> <p>Creates a new user in the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_name</code> <p>Name for the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def create_user(self, org_id: str, user_name: str) -&gt; models.User:\n    \"\"\"\n    Creates a new user in the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_name (str): Name for the user.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_agent","title":"delete_agent  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_agent(org_id: str, agent_id: str) -&gt; None\n</code></pre> <p>Deletes an agent from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_agent(self, org_id: str, agent_id: str) -&gt; None:\n    \"\"\"\n    Deletes an agent from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_all_user_interactions_and_their_memories","title":"delete_all_user_interactions_and_their_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_all_user_interactions_and_their_memories(\n    org_id: str, user_id: str\n) -&gt; None\n</code></pre> <p>Deletes all interactions and their associated memories for a specific user in an organization.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user whose interactions should be deleted</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_all_user_interactions_and_their_memories(\n    self,\n    org_id: str,\n    user_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes all interactions and their associated memories for a specific user in an organization.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user whose interactions should be deleted\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_all_user_memories","title":"delete_all_user_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_all_user_memories(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Deletes all memories of a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_all_user_memories(\n    self,\n    org_id: str,\n    user_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes all memories of a specific user.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_organization","title":"delete_organization  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_organization(org_id: str) -&gt; None\n</code></pre> <p>Deletes an organization from the graph database.</p> Warning <p>This operation will delete all nodes and relationships from this organization including users, agents, memories, interactions etc.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_organization(self, org_id: str) -&gt; None:\n    \"\"\"\n    Deletes an organization from the graph database.\n\n    Warning:\n        This operation will delete all nodes and relationships from this organization\n        including users, agents, memories, interactions etc.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_user","title":"delete_user  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_user(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Deletes a user from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_user(self, org_id: str, user_id: str) -&gt; None:\n    \"\"\"\n    Deletes a user from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_user_interaction_and_its_memories","title":"delete_user_interaction_and_its_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_user_interaction_and_its_memories(\n    org_id: str, user_id: str, interaction_id: str\n) -&gt; None\n</code></pre> <p>Deletes an interaction record and its associated memories.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction to delete.</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_user_interaction_and_its_memories(\n    self,\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes an interaction record and its associated memories.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction to delete.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.delete_user_memory","title":"delete_user_memory  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_user_memory(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; None\n</code></pre> <p>Deletes a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory to delete</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memory is also deleted there for data consistency.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_user_memory(\n    self,\n    org_id: str,\n    user_id: str,\n    memory_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory to delete\n\n    Note:\n        If the graph database is associated with a vector database, the memory is also deleted there for data consistency.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.fetch_user_memories_resolved","title":"fetch_user_memories_resolved  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>fetch_user_memories_resolved(\n    org_user_mem_ids: List[Dict[str, str]]\n) -&gt; List[models.Memory]\n</code></pre> <p>Fetches memories from the GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.</p> This method performs several operations <ol> <li>Retrieves memories using (org_id, user_id, memory_ids)</li> <li>If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version</li> <li>Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels</li> </ol> PARAMETER DESCRIPTION <code>org_user_mem_ids</code> <p>List of Dicts containing org, user, and memory ids of the memories to fetch and process</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Example <pre><code>&gt;&gt;&gt; org_user_mem_ids = [{'memory_id': '443ac3a8-fe87-49a4-93d2-05d3eb58ddeb', 'org_id': 'gmDr4sUiWMNqbGAiV8ijbU', 'user_id': 'CcyKXxhi2skEcDpRzNZim7'}, ...]\n&gt;&gt;&gt; memories = graphInstance.fetch_memories_resolved(org_user_mem_ids)\n&gt;&gt;&gt; print([memoryObj.memory for memoryObj in memories])\n[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"]\n</code></pre> Note <ul> <li>Org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.</li> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def fetch_user_memories_resolved(\n    self, org_user_mem_ids: List[Dict[str, str]]\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Fetches memories from the GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.\n\n    This method performs several operations:\n      1. Retrieves memories using (org_id, user_id, memory_ids)\n      2. If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version\n      3. Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels\n\n    Args:\n        org_user_mem_ids (List[Dict[str, str]]): List of Dicts containing org, user, and memory ids of the memories to fetch and process\n\n    Returns:\n        List[Memory] containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Example:\n        ```python\n        &gt;&gt;&gt; org_user_mem_ids = [{'memory_id': '443ac3a8-fe87-49a4-93d2-05d3eb58ddeb', 'org_id': 'gmDr4sUiWMNqbGAiV8ijbU', 'user_id': 'CcyKXxhi2skEcDpRzNZim7'}, ...]\n        &gt;&gt;&gt; memories = graphInstance.fetch_memories_resolved(org_user_mem_ids)\n        &gt;&gt;&gt; print([memoryObj.memory for memoryObj in memories])\n        [\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"]\n        ```\n\n    Note:\n        - Org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.fetch_user_memories_resolved_batch","title":"fetch_user_memories_resolved_batch  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>fetch_user_memories_resolved_batch(\n    batch_org_user_mem_ids: List[List[Dict[str, str]]]\n) -&gt; List[List[models.Memory]]\n</code></pre> <p>Fetches memories from the GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.</p> This method performs several operations <ol> <li>Retrieves memories using (org_id, user_id, memory_ids)</li> <li>If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version</li> <li>Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels</li> </ol> PARAMETER DESCRIPTION <code>batch_org_user_mem_ids</code> <p>List of lists containing Dicts with org, user, and memory ids of the memories to fetch and process</p> <p> TYPE: <code>List[List[Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[List[Memory]]</code> <p>List[List[Memory]] with memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Example <pre><code>&gt;&gt;&gt; batch_org_user_mem_ids = [[{\"memory_id\": \"413ac3a8-fe87-49a4-93d2-05d3eb58ddeb\", \"org_id\": \"gmDr4sUiWMNqbGAiV8ijbU\", \"user_id\": \"CcyKXxhi2skEcDpRzNZim7\"}, ...], [{...}, ...]]\n&gt;&gt;&gt; batch_memories = graphInstance.fetch_memories_resolved_batch(batch_org_user_mem_ids)\n&gt;&gt;&gt; print([[memoryObj.memory for memoryObj in memories] for memories in batch_memories])\n[[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"], [\"John is about to propose to Sarah\"]]\n</code></pre> Note <ul> <li>Batch org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.</li> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def fetch_user_memories_resolved_batch(\n    self, batch_org_user_mem_ids: List[List[Dict[str, str]]]\n) -&gt; List[List[models.Memory]]:\n    \"\"\"\n    Fetches memories from the GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.\n\n    This method performs several operations:\n      1. Retrieves memories using (org_id, user_id, memory_ids)\n      2. If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version\n      3. Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels\n\n    Args:\n        batch_org_user_mem_ids (List[List[Dict[str, str]]]): List of lists containing Dicts with org, user, and memory ids of the memories to fetch and process\n\n    Returns:\n        List[List[Memory]] with memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Example:\n        ```python\n        &gt;&gt;&gt; batch_org_user_mem_ids = [[{\"memory_id\": \"413ac3a8-fe87-49a4-93d2-05d3eb58ddeb\", \"org_id\": \"gmDr4sUiWMNqbGAiV8ijbU\", \"user_id\": \"CcyKXxhi2skEcDpRzNZim7\"}, ...], [{...}, ...]]\n        &gt;&gt;&gt; batch_memories = graphInstance.fetch_memories_resolved_batch(batch_org_user_mem_ids)\n        &gt;&gt;&gt; print([[memoryObj.memory for memoryObj in memories] for memories in batch_memories])\n        [[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"], [\"John is about to propose to Sarah\"]]\n        ```\n\n    Note:\n        - Batch org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_agent","title":"get_agent  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_agent(org_id: str, agent_id: str) -&gt; models.Agent\n</code></pre> <p>Gets a specific agent belonging to the specified organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_agent(self, org_id: str, agent_id: str) -&gt; models.Agent:\n    \"\"\"\n    Gets a specific agent belonging to the specified organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to retrieve.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_org_agents","title":"get_all_org_agents  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_org_agents(org_id: str) -&gt; List[models.Agent]\n</code></pre> <p>Gets all agents belonging to the specified organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Agent]</code> <p>A List[Agent], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_org_agents(self, org_id: str) -&gt; List[models.Agent]:\n    \"\"\"\n    Gets all agents belonging to the specified organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n\n    Returns:\n        A List[Agent], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_org_users","title":"get_all_org_users  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_org_users(org_id: str) -&gt; List[models.User]\n</code></pre> <p>Gets all users belonging to the specified organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[User]</code> <p>List[User], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_org_users(self, org_id: str) -&gt; List[models.User]:\n    \"\"\"\n    Gets all users belonging to the specified organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n\n    Returns:\n        List[User], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_organizations","title":"get_all_organizations  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_organizations() -&gt; List[models.Organization]\n</code></pre> <p>Gets all organizations from the graph database.</p> RETURNS DESCRIPTION <code>List[Organization]</code> <p>List[Organization] each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_organizations(self) -&gt; List[models.Organization]:\n    \"\"\"\n    Gets all organizations from the graph database.\n\n    Returns:\n        List[Organization] each containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_user_agents","title":"get_all_user_agents  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_user_agents(\n    org_id: str, user_id: str\n) -&gt; List[models.Agent]\n</code></pre> <p>Gets all agents for a user within an organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Agent]</code> <p>A List[Agent], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_user_agents(\n    self, org_id: str, user_id: str\n) -&gt; List[models.Agent]:\n    \"\"\"\n    Gets all agents for a user within an organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n\n    Returns:\n        A List[Agent], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_user_interactions","title":"get_all_user_interactions  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_user_interactions(\n    org_id: str,\n    user_id: str,\n    with_their_messages: bool = True,\n    with_their_memories: bool = True,\n    skip: int = 0,\n    limit: int = 100,\n) -&gt; List[models.Interaction]\n</code></pre> <p>Retrieves all interactions for a specific user in an organization.</p> Note <p>Interactions are sorted in descending order by their updated at datetime. (So most recent interactions are first).</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>with_their_messages</code> <p>Whether to also retrieve messages of an interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>with_their_memories</code> <p>Whether to also retrieve memories gotten across all occurrences of an interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip</code> <p>Number of interactions to skip. (Useful for pagination)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>limit</code> <p>Maximum number of interactions to retrieve. (Useful for pagination)</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RETURNS DESCRIPTION <code>List[Interaction]</code> <p>List[Interaction], each containing an Interaction with:</p> <ul> <li>org_id: Short UUID string identifying the organization.</li> <li>user_id: Short UUID string identifying the user.</li> <li>agent_id: Short UUID string identifying the agent.</li> <li>interaction_id: Short UUID string identifying the interaction.</li> <li>created_at: DateTime object of when the interaction was created.</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> <li>messages (if <code>with_their_messages</code> = True): List of messages in the interaction.</li> <li>memories (if <code>with_their_memories</code> = True): List of memories gotten from all occurrences of this interaction.</li> </ul> Note <p>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_user_interactions(\n    self,\n    org_id: str,\n    user_id: str,\n    with_their_messages: bool = True,\n    with_their_memories: bool = True,\n    skip: int = 0,\n    limit: int = 100,\n) -&gt; List[models.Interaction]:\n    \"\"\"\n    Retrieves all interactions for a specific user in an organization.\n\n    Note:\n        Interactions are sorted in descending order by their updated at datetime. (So most recent interactions are first).\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        with_their_messages (bool): Whether to also retrieve messages of an interaction.\n        with_their_memories (bool): Whether to also retrieve memories gotten across all occurrences of an interaction.\n        skip (int): Number of interactions to skip. (Useful for pagination)\n        limit (int): Maximum number of interactions to retrieve. (Useful for pagination)\n\n    Returns:\n        List[Interaction], each containing an Interaction with:\n\n            + org_id: Short UUID string identifying the organization.\n            + user_id: Short UUID string identifying the user.\n            + agent_id: Short UUID string identifying the agent.\n            + interaction_id: Short UUID string identifying the interaction.\n            + created_at: DateTime object of when the interaction was created.\n            + updated_at: DateTime object of when the interaction was last updated.\n            + messages (if `with_their_messages` = True): List of messages in the interaction.\n            + memories (if `with_their_memories` = True): List of memories gotten from all occurrences of this interaction.\n\n    Note:\n        A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_all_user_memories","title":"get_all_user_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_all_user_memories(\n    org_id: str,\n    user_id: str,\n    agent_id: Optional[str] = None,\n) -&gt; List[models.Memory]\n</code></pre> <p>Retrieves all memories associated with a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Optional short UUID string identifying the agent. If provided, only memories obtained from interactions with this agent are returned. Otherwise, all memories associated with the user are returned.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_all_user_memories(\n    self, org_id: str, user_id: str, agent_id: Optional[str] = None\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Retrieves all memories associated with a specific user.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        agent_id (Optional[str]): Optional short UUID string identifying the agent. If provided, only memories obtained from\n            interactions with this agent are returned.\n            Otherwise, all memories associated with the user are returned.\n\n    Returns:\n        List[Memory] containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_associated_vector_db","title":"get_associated_vector_db  <code>abstractmethod</code>","text":"<pre><code>get_associated_vector_db() -&gt; Optional[BaseVectorDB]\n</code></pre> <p>The vector database associated with the graph database, these is used inside the graph transactional blocks to ensure data consistency when handling memories across both stores (e.g., saving memories to the vector store and creating corresponding nodes in the graph db).</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\ndef get_associated_vector_db(self) -&gt; Optional[BaseVectorDB]:\n    \"\"\"\n    The vector database associated with the graph database, these is used inside the graph transactional blocks\n    to ensure data consistency when handling memories across both stores (e.g., saving memories to the vector\n    store and creating corresponding nodes in the graph db).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_interaction","title":"get_interaction  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_interaction(\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n    with_messages: bool = True,\n    with_memories: bool = True,\n) -&gt; models.Interaction\n</code></pre> <p>Retrieves all messages associated with a specific interaction.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction.</p> <p> TYPE: <code>str</code> </p> <code>with_messages</code> <p>Whether to retrieve messages along with the interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>with_memories</code> <p>Whether to also retrieve memories gotten across all occurrences of this interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Interaction</code> <p>Interaction containing:</p> <ul> <li>org_id: Short UUID string identifying the organization.</li> <li>user_id: Short UUID string identifying the user.</li> <li>agent_id: Short UUID string identifying the agent.</li> <li>interaction_id: Short UUID string identifying the interaction.</li> <li>created_at: DateTime object of when the interaction was created.</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> <li>messages (if <code>with_messages</code> = True): List of messages in the interaction.</li> <li>memories (if <code>with_memories</code> = True): List of memories gotten from all occurrences of this interaction.</li> </ul> Note <p>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_interaction(\n    self,\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n    with_messages: bool = True,\n    with_memories: bool = True,\n) -&gt; models.Interaction:\n    \"\"\"\n    Retrieves all messages associated with a specific interaction.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction.\n        with_messages (bool): Whether to retrieve messages along with the interaction.\n        with_memories (bool): Whether to also retrieve memories gotten across all occurrences of this interaction.\n\n    Returns:\n        Interaction containing:\n\n            + org_id: Short UUID string identifying the organization.\n            + user_id: Short UUID string identifying the user.\n            + agent_id: Short UUID string identifying the agent.\n            + interaction_id: Short UUID string identifying the interaction.\n            + created_at: DateTime object of when the interaction was created.\n            + updated_at: DateTime object of when the interaction was last updated.\n            + messages (if `with_messages` = True): List of messages in the interaction.\n            + memories (if `with_memories` = True): List of memories gotten from all occurrences of this interaction.\n\n    Note:\n        A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_organization","title":"get_organization  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_organization(org_id: str) -&gt; models.Organization\n</code></pre> <p>Gets a specific organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_organization(self, org_id: str) -&gt; models.Organization:\n    \"\"\"\n    Gets a specific organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization to retrieve.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_user","title":"get_user  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_user(org_id: str, user_id: str) -&gt; models.User\n</code></pre> <p>Gets a specific user belonging to the specified organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_user(self, org_id: str, user_id: str) -&gt; models.User:\n    \"\"\"\n    Gets a specific user belonging to the specified organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to retrieve.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_user_memory","title":"get_user_memory  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_user_memory(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; models.Memory\n</code></pre> <p>Retrieves a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Memory</code> <p>Memory containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>The memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_user_memory(\n    self, org_id: str, user_id: str, memory_id: str\n) -&gt; models.Memory:\n    \"\"\"\n    Retrieves a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory\n\n    Returns:\n        Memory containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - The memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.get_user_memory_history","title":"get_user_memory_history  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>get_user_memory_history(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; List[models.Memory]\n</code></pre> <p>Retrieves the history of a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing the history of memory details in descending order (starting with the current version, to the oldest version):</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def get_user_memory_history(\n    self, org_id: str, user_id: str, memory_id: str\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Retrieves the history of a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory\n\n    Returns:\n        List[Memory] containing the history of memory details in descending order (starting with the current version, to the oldest version):\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.save_interaction_with_memories","title":"save_interaction_with_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>save_interaction_with_memories(\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]\n</code></pre> <p>Creates a new interaction record with associated memories.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>memories_and_interaction</code> <p>Contains both the interaction and the associated memories.</p> <p> TYPE: <code>MemoriesAndInteraction</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also stored there for data consistency.</p> RETURNS DESCRIPTION <code>Tuple[str, datetime]</code> <p>Tuple[str, datetime] containing:</p> <ul> <li>interaction_id: Short UUID string identifying the created interaction</li> <li>created_at: DateTime object of when the interaction was created.</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def save_interaction_with_memories(\n    self,\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]:\n    \"\"\"\n    Creates a new interaction record with associated memories.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent.\n        user_id (str): Short UUID string identifying the user.\n        memories_and_interaction (MemoriesAndInteraction): Contains both the interaction and the associated memories.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also stored there for data consistency.\n\n    Returns:\n        Tuple[str, datetime] containing:\n\n            + interaction_id: Short UUID string identifying the created interaction\n            + created_at: DateTime object of when the interaction was created.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.setup","title":"setup  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>setup(*args, **kwargs) -&gt; None\n</code></pre> <p>Sets up the database, e.g., creates indexes, constraints, etc.</p> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def setup(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Sets up the database, e.g., creates indexes, constraints, etc.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.update_agent","title":"update_agent  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>update_agent(\n    org_id: str, agent_id: str, new_agent_label: str\n) -&gt; models.Agent\n</code></pre> <p>Updates an existing agent in the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to update.</p> <p> TYPE: <code>str</code> </p> <code>new_agent_label</code> <p>New label/name for the agent.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def update_agent(\n    self, org_id: str, agent_id: str, new_agent_label: str\n) -&gt; models.Agent:\n    \"\"\"\n    Updates an existing agent in the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to update.\n        new_agent_label (str): New label/name for the agent.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.update_interaction_and_memories","title":"update_interaction_and_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>update_interaction_and_memories(\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    interaction_id: str,\n    updated_memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]\n</code></pre> <p>Update an existing interaction record and add new memories.</p> Compares updated interaction with existing one <ul> <li>If differences are found, truncates existing record from that point and replaces with updated version. Old memories from truncated message(s) remain but become standalone (no longer linked to truncated messages).</li> <li>If no differences, appends new messages from the update.</li> </ul> <p>New memories are always added, regardless of interaction changes.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent in the updated interaction.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction to update.</p> <p> TYPE: <code>str</code> </p> <code>updated_memories_and_interaction</code> <p>Contains both the updated interaction and the associated new memories.</p> <p> TYPE: <code>MemoriesAndInteraction</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also stored there for data consistency.</p> RETURNS DESCRIPTION <code>Tuple[str, datetime]</code> <p>Tuple[str, datetime] containing:</p> <ul> <li>interaction_id: Short UUID string identifying the updated interaction</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def update_interaction_and_memories(\n    self,\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    interaction_id: str,\n    updated_memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]:\n    \"\"\"\n    Update an existing interaction record and add new memories.\n\n    Compares updated interaction with existing one:\n        - If differences are found, truncates existing record from that point and\n        replaces with updated version. Old memories from truncated message(s)\n        remain but become standalone (no longer linked to truncated messages).\n        - If no differences, appends new messages from the update.\n\n    New memories are always added, regardless of interaction changes.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent in the updated interaction.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction to update.\n        updated_memories_and_interaction (MemoriesAndInteraction): Contains both the updated interaction and the associated new memories.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also stored there for data consistency.\n\n    Returns:\n        Tuple[str, datetime] containing:\n\n            + interaction_id: Short UUID string identifying the updated interaction\n            + updated_at: DateTime object of when the interaction was last updated.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.update_organization","title":"update_organization  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>update_organization(\n    org_id: str, new_org_name: str\n) -&gt; models.Organization\n</code></pre> <p>Updates an existing organization in the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>The Short UUID of the organization to update.</p> <p> TYPE: <code>str</code> </p> <code>new_org_name</code> <p>The new name for the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def update_organization(\n    self, org_id: str, new_org_name: str\n) -&gt; models.Organization:\n    \"\"\"\n    Updates an existing organization in the graph database.\n\n    Args:\n        org_id (str): The Short UUID of the organization to update.\n        new_org_name (str): The new name for the organization.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/base/#memora.graph_db.base.BaseGraphDB.update_user","title":"update_user  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>update_user(\n    org_id: str, user_id: str, new_user_name: str\n) -&gt; models.User\n</code></pre> <p>Updates an existing user in the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to update.</p> <p> TYPE: <code>str</code> </p> <code>new_user_name</code> <p>The new name for the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/base.py</code> <pre><code>@abstractmethod\nasync def update_user(\n    self, org_id: str, user_id: str, new_user_name: str\n) -&gt; models.User:\n    \"\"\"\n    Updates an existing user in the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to update.\n        new_user_name (str): The new name for the user.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/graph_db/neo4j/","title":"<code>Neo4j</code> GraphDB Implementation","text":"<p>This section details the Neo4j implementation of the <code>BaseGraphDB</code> interface. The implementation is structured into several subclasses, each handling a specific group of methods from the <code>BaseGraphDB</code> interface. These subclasses are then inherited by the <code>Neo4jGraphInterface</code>, which serves as a unified interface for interacting with the Neo4j implementation.</p>"},{"location":"api/graph_db/neo4j/#graph-interface","title":"Graph Interface","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface","title":"memora.graph_db.neo4j.Neo4jGraphInterface","text":"<pre><code>Neo4jGraphInterface(\n    uri: str,\n    username: str,\n    password: str,\n    database: str,\n    associated_vector_db: Optional[BaseVectorDB] = None,\n    enable_logging: bool = False,\n)\n</code></pre> <p>               Bases: <code>Neo4jOrganization</code>, <code>Neo4jAgent</code>, <code>Neo4jUser</code>, <code>Neo4jInteraction</code>, <code>Neo4jMemory</code></p> PARAMETER DESCRIPTION <code>uri</code> <p>The URI of the Neo4j database.</p> <p> TYPE: <code>str</code> </p> <code>username</code> <p>The username for authentication.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>The password for authentication.</p> <p> TYPE: <code>str</code> </p> <code>database</code> <p>The name of the Neo4j database.</p> <p> TYPE: <code>str</code> </p> <code>associated_vector_db</code> <p>The vector database to be associated with the graph for data consistency (e.g adding / deleting memories across both.)</p> <p> TYPE: <code>Optional[BaseVectorDB]</code> DEFAULT: <code>None</code> </p> <code>enable_logging</code> <p>Whether to enable console logging</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>from memora.graph_db.neo4j import Neo4jGraphInterface\nfrom qdrant_client import AsyncQdrantClient\nfrom memora.vector_db.qdrant import QdrantDB\n\nneo4j_interface = Neo4jGraphInterface(\n    uri=\"Neo4jURI\",\n    username=\"Neo4jUsername\",\n    password=\"Neo4jPassword\",\n    database=\"Neo4jDatabaseName\",\n    # Optional Association\n    associated_vector_db=QdrantDB(async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\"))\n)\n</code></pre> Source code in <code>memora/graph_db/neo4j/interface.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    username: str,\n    password: str,\n    database: str,\n    associated_vector_db: Optional[BaseVectorDB] = None,\n    enable_logging: bool = False,\n):\n    \"\"\"\n    A unified interface for interacting with the Neo4j graph database.\n\n    Args:\n        uri (str): The URI of the Neo4j database.\n        username (str): The username for authentication.\n        password (str): The password for authentication.\n        database (str): The name of the Neo4j database.\n        associated_vector_db (Optional[BaseVectorDB]): The vector database to be associated with the graph for data consistency (e.g adding / deleting memories across both.)\n        enable_logging (bool): Whether to enable console logging\n\n    Example:\n        ```python\n        from memora.graph_db.neo4j import Neo4jGraphInterface\n        from qdrant_client import AsyncQdrantClient\n        from memora.vector_db.qdrant import QdrantDB\n\n        neo4j_interface = Neo4jGraphInterface(\n            uri=\"Neo4jURI\",\n            username=\"Neo4jUsername\",\n            password=\"Neo4jPassword\",\n            database=\"Neo4jDatabaseName\",\n            # Optional Association\n            associated_vector_db=QdrantDB(async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\"))\n        )\n        ```\n    \"\"\"\n\n    self.driver = AsyncGraphDatabase.driver(uri=uri, auth=(username, password))\n    self.database = database\n    self.associated_vector_db = associated_vector_db\n\n    # Configure logging\n    self.logger = logging.getLogger(__name__)\n    if enable_logging:\n        logging.basicConfig(level=logging.INFO)\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface-attributes","title":"Attributes","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.associated_vector_db","title":"associated_vector_db  <code>instance-attribute</code>","text":"<pre><code>associated_vector_db = associated_vector_db\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.database","title":"database  <code>instance-attribute</code>","text":"<pre><code>database = database\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.driver","title":"driver  <code>instance-attribute</code>","text":"<pre><code>driver = driver(uri=uri, auth=(username, password))\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> Source code in <code>memora/graph_db/neo4j/interface.py</code> <pre><code>@override\nasync def close(self):\n    self.logger.info(\"Closing Neo4j driver\")\n    await self.driver.close()\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.get_associated_vector_db","title":"get_associated_vector_db","text":"<pre><code>get_associated_vector_db() -&gt; Optional[BaseVectorDB]\n</code></pre> <p>The vector database associated with the graph database, these is used inside the graph transactional blocks to ensure data consistency when handling memories across both stores (e.g., saving memories to the vector store and creating corresponding nodes in the graph db).</p> Source code in <code>memora/graph_db/neo4j/interface.py</code> <pre><code>@override\ndef get_associated_vector_db(self) -&gt; Optional[BaseVectorDB]:\n    \"\"\"\n    The vector database associated with the graph database, these is used inside the graph transactional blocks\n    to ensure data consistency when handling memories across both stores (e.g., saving memories to the vector\n    store and creating corresponding nodes in the graph db).\n    \"\"\"\n    return self.associated_vector_db\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jGraphInterface.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(*args, **kwargs) -&gt; None\n</code></pre> <p>Sets up Neo4j database constraints and indices for the graph schema.</p> Source code in <code>memora/graph_db/neo4j/interface.py</code> <pre><code>@override\nasync def setup(self, *args, **kwargs) -&gt; None:\n    \"\"\"Sets up Neo4j database constraints and indices for the graph schema.\"\"\"\n\n    async def create_constraints_and_indexes(tx):\n        self.logger.info(\"Creating constraints and indexes\")\n        # Organization node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_org_id IF NOT EXISTS \n            FOR (o:Org) REQUIRE o.org_id IS NODE KEY\n        \"\"\"\n        )\n\n        # User node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_org_user IF NOT EXISTS\n            FOR (u:User) REQUIRE (u.org_id, u.user_id) IS NODE KEY\n        \"\"\"\n        )\n\n        # Agent node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_org_agent IF NOT EXISTS \n            FOR (a:Agent) REQUIRE (a.org_id, a.agent_id) IS NODE KEY\n        \"\"\"\n        )\n\n        # Memory node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_user_memory IF NOT EXISTS\n            FOR (m:Memory) REQUIRE (m.org_id, m.user_id, m.memory_id) IS NODE KEY\n        \"\"\"\n        )\n\n        # Interaction node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_user_interaction IF NOT EXISTS\n            FOR (i:Interaction) REQUIRE (i.org_id, i.user_id, i.interaction_id) IS NODE KEY\n        \"\"\"\n        )\n\n        # Index on interaction updated_at, useful for sorting interactions by most recent when retrieving.\n        await tx.run(\n            \"\"\"\n            CREATE INDEX interaction_updated_timestamp_index IF NOT EXISTS\n            FOR (i:Interaction) ON (i.updated_at);\n        \"\"\"\n        )\n\n        # Date node key\n        await tx.run(\n            \"\"\"\n            CREATE CONSTRAINT unique_user_date IF NOT EXISTS\n            FOR (d:Date) REQUIRE (d.org_id, d.user_id, d.date) IS NODE KEY\n        \"\"\"\n        )\n\n    self.logger.info(\"Setting up Neo4j database constraints and indices\")\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(create_constraints_and_indexes)\n    self.logger.info(\"Setup complete\")\n</code></pre>"},{"location":"api/graph_db/neo4j/#component-classes","title":"Component Classes","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization","title":"memora.graph_db.neo4j.Neo4jOrganization","text":"<p>               Bases: <code>BaseGraphDB</code></p>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization.create_organization","title":"create_organization  <code>async</code>","text":"<pre><code>create_organization(org_name: str) -&gt; models.Organization\n</code></pre> <p>Creates a new organization in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_name</code> <p>The name of the organization to create.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/neo4j/organization.py</code> <pre><code>@override\nasync def create_organization(self, org_name: str) -&gt; models.Organization:\n    \"\"\"\n    Creates a new organization in the Neo4j graph database.\n\n    Args:\n        org_name (str): The name of the organization to create.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n\n    if not isinstance(org_name, str) or not org_name:\n        raise TypeError(\"`org_name` must be a string and have a value.\")\n\n    org_id = shortuuid.uuid()\n    self.logger.info(f\"Creating organization with ID {org_id}\")\n\n    async def create_org_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            CREATE (o:Org {\n                org_id: $org_id,\n                org_name: $org_name,\n                created_at: datetime()\n            })\n            RETURN o{.org_id, .org_name, .created_at} as org\n        \"\"\",\n            org_id=org_id,\n            org_name=org_name,\n        )\n\n        record = await result.single()\n        return record[\"org\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n\n        org_data = await session.execute_write(create_org_tx)\n\n        if org_data is None:\n            self.logger.info(f\"Failed to create organization {org_id}\")\n            raise neo4j.exceptions.Neo4jError(\"Failed to create organization.\")\n\n        self.logger.info(f\"Successfully created organization {org_id}\")\n        return models.Organization(\n            org_id=org_data[\"org_id\"],\n            org_name=org_data[\"org_name\"],\n            created_at=(org_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization.delete_organization","title":"delete_organization  <code>async</code>","text":"<pre><code>delete_organization(org_id: str) -&gt; None\n</code></pre> <p>Deletes an organization from the Neo4j graph database.</p> Warning <p>This operation will delete all nodes and relationships from this organization including users, agents, memories, interactions etc.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/neo4j/organization.py</code> <pre><code>@override\nasync def delete_organization(self, org_id: str) -&gt; None:\n    \"\"\"\n    Deletes an organization from the Neo4j graph database.\n\n    Warning:\n        This operation will delete all nodes and relationships from this organization\n        including users, agents, memories, interactions etc.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization to delete.\n    \"\"\"\n\n    if not isinstance(org_id, str) or not org_id:\n        raise TypeError(\"`org_id` must be a string and have a value.\")\n\n    self.logger.info(f\"Deleting organization {org_id} and all associated data\")\n\n    async def delete_org_tx(tx):\n        # Delete all nodes and relationships associated with the org\n        await tx.run(\n            \"\"\"\n            CALL apoc.periodic.iterate(\"\n            MATCH (o:Org {org_id: $org_id})\n            CALL apoc.path.subgraphNodes(o, {}) YIELD node\n            RETURN node\",\n            \"DETACH DELETE node\",\n            {batchSize: 1000, parallel: true, params: {org_id: $org_id}})\n        \"\"\",\n            org_id=org_id,\n        )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_org_tx)\n        self.logger.info(f\"Successfully deleted organization {org_id}\")\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization.get_all_organizations","title":"get_all_organizations  <code>async</code>","text":"<pre><code>get_all_organizations() -&gt; List[models.Organization]\n</code></pre> <p>Gets all organizations from the graph database.</p> RETURNS DESCRIPTION <code>List[Organization]</code> <p>List[Organization] each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/neo4j/organization.py</code> <pre><code>@override\nasync def get_all_organizations(self) -&gt; List[models.Organization]:\n    \"\"\"\n    Gets all organizations from the graph database.\n\n    Returns:\n        List[Organization] each containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n\n    self.logger.info(\"Getting all organizations\")\n\n    async def get_all_org_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org)\n            RETURN o{.org_id, .org_name, .created_at} as org\n        \"\"\"\n        )\n        records = await result.value(\"org\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n\n        all_org_data = await session.execute_read(get_all_org_tx)\n\n        return [\n            models.Organization(\n                org_id=org_data[\"org_id\"],\n                org_name=org_data[\"org_name\"],\n                created_at=(org_data[\"created_at\"]).to_native(),\n            )\n            for org_data in all_org_data\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization.get_organization","title":"get_organization  <code>async</code>","text":"<pre><code>get_organization(org_id: str) -&gt; models.Organization\n</code></pre> <p>Gets a specific organization from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/neo4j/organization.py</code> <pre><code>@override\nasync def get_organization(self, org_id: str) -&gt; models.Organization:\n    \"\"\"\n    Gets a specific organization from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization to retrieve.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n\n    if not isinstance(org_id, str) or not org_id:\n        raise TypeError(\"`org_id` must be a string and have a value.\")\n\n    async def get_org_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org {org_id: $org_id})\n            RETURN o{.org_id, .org_name, .created_at} as org\n        \"\"\",\n            org_id=org_id,\n        )\n        record = await result.single()\n        return record[\"org\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        org_data = await session.execute_read(get_org_tx)\n\n        if org_data is None:\n            self.logger.info(f\"Organization {org_id} not found\")\n            raise neo4j.exceptions.Neo4jError(\n                \"Organization (`org_id`) does not exist.\"\n            )\n\n        return models.Organization(\n            org_id=org_data[\"org_id\"],\n            org_name=org_data[\"org_name\"],\n            created_at=(org_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jOrganization.update_organization","title":"update_organization  <code>async</code>","text":"<pre><code>update_organization(\n    org_id: str, new_org_name: str\n) -&gt; models.Organization\n</code></pre> <p>Updates an existing organization in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>The Short UUID of the organization to update.</p> <p> TYPE: <code>str</code> </p> <code>new_org_name</code> <p>The new name for the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Organization</code> <p>Organization object containing:</p> <ul> <li>org_id: Short UUID string</li> <li>org_name: Organization name</li> <li>created_at: DateTime object of when the organization was created</li> </ul> Source code in <code>memora/graph_db/neo4j/organization.py</code> <pre><code>@override\nasync def update_organization(\n    self, org_id: str, new_org_name: str\n) -&gt; models.Organization:\n    \"\"\"\n    Updates an existing organization in the Neo4j graph database.\n\n    Args:\n        org_id (str): The Short UUID of the organization to update.\n        new_org_name (str): The new name for the organization.\n\n    Returns:\n        Organization object containing:\n\n            + org_id: Short UUID string\n            + org_name: Organization name\n            + created_at: DateTime object of when the organization was created\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, new_org_name)\n    ):\n        raise ValueError(\n            \"Both `org_id` and `new_org_name` must be a string and have a value.\"\n        )\n\n    self.logger.info(f\"Updating organization {org_id}\")\n\n    async def update_org_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org {org_id: $org_id})\n            SET o.org_name = $new_org_name\n            RETURN o{.org_id, .org_name, .created_at} as org\n        \"\"\",\n            org_id=org_id,\n            new_org_name=new_org_name,\n        )\n\n        record = await result.single()\n        return record[\"org\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n\n        org_data = await session.execute_write(update_org_tx)\n\n        if org_data is None:\n            self.logger.info(f\"Organization {org_id} not found\")\n            raise neo4j.exceptions.Neo4jError(\n                \"Organization (`org_id`) does not exist.\"\n            )\n\n        self.logger.info(f\"Successfully updated organization {org_id}\")\n        return models.Organization(\n            org_id=org_data[\"org_id\"],\n            org_name=org_data[\"org_name\"],\n            created_at=(org_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent","title":"memora.graph_db.neo4j.Neo4jAgent","text":"<p>               Bases: <code>BaseGraphDB</code></p>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.create_agent","title":"create_agent  <code>async</code>","text":"<pre><code>create_agent(\n    org_id: str,\n    agent_label: str,\n    user_id: Optional[str] = None,\n) -&gt; models.Agent\n</code></pre> <p>Creates a new agent in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_label</code> <p>Label/name for the agent.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional Short UUID of the user. This is used when the agent is created specifically for a user, indicating that both the organization and the user will have this agent.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def create_agent(\n    self, org_id: str, agent_label: str, user_id: Optional[str] = None\n) -&gt; models.Agent:\n    \"\"\"\n    Creates a new agent in the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_label (str): Label/name for the agent.\n        user_id (Optional[str]): Optional Short UUID of the user. This is used when the agent is created\n            specifically for a user, indicating that both the organization and the\n            user will have this agent.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, agent_label)):\n        raise ValueError(\n            \"Both `org_id` and `agent_label` must be a string and have a value.\"\n        )\n\n    if user_id:\n        if not isinstance(user_id, str):\n            raise ValueError(\"`user_id` must be a string.\")\n\n    agent_id = shortuuid.uuid()\n    self.logger.info(f\"Creating new agent with ID {agent_id}\")\n\n    async def create_agent_tx(tx):\n        if user_id:\n            result = await tx.run(\n                \"\"\"\n                MATCH (o:Org {org_id: $org_id}), (u:User {org_id: $org_id, user_id: $user_id})\n                CREATE (a:Agent {\n                    org_id: $org_id,\n                    user_id: $user_id,\n                    agent_id: $agent_id,\n                    agent_label: $agent_label,\n                    created_at: datetime()\n                })\n                CREATE (o)-[:HAS_AGENT]-&gt;(a)\n                CREATE (u)-[:HAS_AGENT]-&gt;(a)\n                RETURN a{.org_id, .user_id, .agent_id, .agent_label, .created_at} as agent\n            \"\"\",\n                org_id=org_id,\n                user_id=user_id,\n                agent_id=agent_id,\n                agent_label=agent_label,\n            )\n        else:\n            result = await tx.run(\n                \"\"\"\n                MATCH (o:Org {org_id: $org_id})\n                CREATE (a:Agent {\n                    org_id: $org_id,\n                    agent_id: $agent_id,\n                    agent_label: $agent_label,\n                    created_at: datetime()\n                })\n                CREATE (o)-[:HAS_AGENT]-&gt;(a)\n                RETURN a{.org_id, .agent_id, .agent_label, .created_at} as agent\n            \"\"\",\n                org_id=org_id,\n                agent_id=agent_id,\n                agent_label=agent_label,\n            )\n\n        record = await result.single()\n        return record[\"agent\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        agent_data = await session.execute_write(create_agent_tx)\n\n        if agent_data is None:\n            self.logger.info(f\"Failed to create agent {agent_id}\")\n            raise neo4j.exceptions.Neo4jError(\"Failed to create agent.\")\n\n        self.logger.info(f\"Successfully created agent {agent_id}\")\n        return models.Agent(\n            org_id=agent_data[\"org_id\"],\n            agent_id=agent_data[\"agent_id\"],\n            user_id=agent_data.get(\"user_id\"),\n            agent_label=agent_data[\"agent_label\"],\n            created_at=(agent_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.delete_agent","title":"delete_agent  <code>async</code>","text":"<pre><code>delete_agent(org_id: str, agent_id: str) -&gt; None\n</code></pre> <p>Deletes an agent from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def delete_agent(self, org_id: str, agent_id: str) -&gt; None:\n    \"\"\"\n    Deletes an agent from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to delete.\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, agent_id)):\n        raise ValueError(\n            \"`org_id` and `agent_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Deleting agent {agent_id}\")\n\n    async def delete_agent_tx(tx):\n        # Using node key (org_id, agent_id) for faster lookup\n        await tx.run(\n            \"\"\"\n            MATCH (a:Agent {org_id: $org_id, agent_id: $agent_id})\n            DETACH DELETE a\n        \"\"\",\n            org_id=org_id,\n            agent_id=agent_id,\n        )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_agent_tx)\n        self.logger.info(f\"Successfully deleted agent {agent_id}\")\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.get_agent","title":"get_agent  <code>async</code>","text":"<pre><code>get_agent(org_id: str, agent_id: str) -&gt; models.Agent\n</code></pre> <p>Gets a specific agent belonging to the specified organization from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def get_agent(self, org_id: str, agent_id: str) -&gt; models.Agent:\n    \"\"\"\n    Gets a specific agent belonging to the specified organization from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to retrieve.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n\n    async def get_agent_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (a:Agent {org_id: $org_id, agent_id: $agent_id})\n            RETURN a{.org_id, .user_id, .agent_id, .agent_label, .created_at} as agent\n        \"\"\",\n            org_id=org_id,\n            agent_id=agent_id,\n        )\n        record = await result.single()\n        return record[\"agent\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        agent_data = await session.execute_read(get_agent_tx)\n\n        if agent_data is None:\n            self.logger.info(\n                f\"Failed to get agent {agent_id}: Agent does not exist\"\n            )\n            raise neo4j.exceptions.Neo4jError(\n                \"Agent (`org_id`, `agent_id`) does not exist.\"\n            )\n\n        return models.Agent(\n            org_id=agent_data[\"org_id\"],\n            agent_id=agent_data[\"agent_id\"],\n            user_id=agent_data.get(\"user_id\"),\n            agent_label=agent_data[\"agent_label\"],\n            created_at=(agent_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.get_all_org_agents","title":"get_all_org_agents  <code>async</code>","text":"<pre><code>get_all_org_agents(org_id: str) -&gt; List[models.Agent]\n</code></pre> <p>Gets all agents belonging to the specified organization from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Agent]</code> <p>A List[Agent], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def get_all_org_agents(self, org_id: str) -&gt; List[models.Agent]:\n    \"\"\"\n    Gets all agents belonging to the specified organization from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n\n    Returns:\n        A List[Agent], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n\n    if not isinstance(org_id, str) or not org_id:\n        raise ValueError(\"`org_id` must be a string and have a value.\")\n\n    self.logger.info(f\"Getting all agents for organization {org_id}\")\n\n    async def get_org_agents_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org {org_id: $org_id})-[:HAS_AGENT]-&gt;(a:Agent)\n            RETURN a{.org_id, .user_id, .agent_id, .agent_label, .created_at} as agent\n        \"\"\",\n            org_id=org_id,\n        )\n        records = await result.value(\"agent\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        all_agents_data = await session.execute_read(get_org_agents_tx)\n\n        return [\n            models.Agent(\n                org_id=agent_data[\"org_id\"],\n                agent_id=agent_data[\"agent_id\"],\n                user_id=agent_data.get(\"user_id\"),\n                agent_label=agent_data[\"agent_label\"],\n                created_at=(agent_data[\"created_at\"]).to_native(),\n            )\n            for agent_data in all_agents_data\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.get_all_user_agents","title":"get_all_user_agents  <code>async</code>","text":"<pre><code>get_all_user_agents(\n    org_id: str, user_id: str\n) -&gt; List[models.Agent]\n</code></pre> <p>Gets all agents for a user within an organization from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Agent]</code> <p>A List[Agent], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def get_all_user_agents(\n    self, org_id: str, user_id: str\n) -&gt; List[models.Agent]:\n    \"\"\"\n    Gets all agents for a user within an organization from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n\n    Returns:\n        A List[Agent], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    self.logger.info(\n        f\"Getting all agents for user {user_id} in organization {org_id}\"\n    )\n\n    async def get_user_agents_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})-[:HAS_AGENT]-&gt;(a:Agent)\n            RETURN a{.org_id, .user_id, .agent_id, .agent_label, .created_at} as agent\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n        )\n        records = await result.value(\"agent\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        all_agents_data = await session.execute_read(get_user_agents_tx)\n\n        return [\n            models.Agent(\n                org_id=agent_data[\"org_id\"],\n                agent_id=agent_data[\"agent_id\"],\n                user_id=agent_data.get(\"user_id\"),\n                agent_label=agent_data[\"agent_label\"],\n                created_at=(agent_data[\"created_at\"]).to_native(),\n            )\n            for agent_data in all_agents_data\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jAgent.update_agent","title":"update_agent  <code>async</code>","text":"<pre><code>update_agent(\n    org_id: str, agent_id: str, new_agent_label: str\n) -&gt; models.Agent\n</code></pre> <p>Updates an existing agent in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent to update.</p> <p> TYPE: <code>str</code> </p> <code>new_agent_label</code> <p>New label/name for the agent.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Agent</code> <p>Agent containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Optional Short UUID string</li> <li>agent_id: Short UUID string</li> <li>agent_label: Agent label/name</li> <li>created_at: DateTime object of when the agent was created</li> </ul> Source code in <code>memora/graph_db/neo4j/agent.py</code> <pre><code>@override\nasync def update_agent(\n    self, org_id: str, agent_id: str, new_agent_label: str\n) -&gt; models.Agent:\n    \"\"\"\n    Updates an existing agent in the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent to update.\n        new_agent_label (str): New label/name for the agent.\n\n    Returns:\n        Agent containing:\n\n            + org_id: Short UUID string\n            + user_id: Optional Short UUID string\n            + agent_id: Short UUID string\n            + agent_label: Agent label/name\n            + created_at: DateTime object of when the agent was created\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str)\n        for param in (org_id, agent_id, new_agent_label)\n    ):\n        raise ValueError(\n            \"`org_id`, `agent_id` and `new_agent_name` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Updating agent {agent_id}\")\n\n    async def update_agent_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (a:Agent {org_id: $org_id, agent_id: $agent_id})\n            SET a.agent_label = $new_agent_label\n            RETURN a{.org_id, .user_id, .agent_id, .agent_label, .created_at} as agent\n        \"\"\",\n            org_id=org_id,\n            agent_id=agent_id,\n            new_agent_label=new_agent_label,\n        )\n\n        record = await result.single()\n        return record[\"agent\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        agent_data = await session.execute_write(update_agent_tx)\n\n        if agent_data is None:\n            self.logger.info(\n                f\"Failed to update agent {agent_id}: Agent does not exist\"\n            )\n            raise neo4j.exceptions.Neo4jError(\n                \"Agent (`org_id`, `agent_id`) does not exist.\"\n            )\n\n        self.logger.info(f\"Successfully updated agent {agent_id}\")\n        return models.Agent(\n            org_id=agent_data[\"org_id\"],\n            agent_id=agent_data[\"agent_id\"],\n            user_id=agent_data.get(\"user_id\"),\n            agent_label=agent_data[\"agent_label\"],\n            created_at=(agent_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser","title":"memora.graph_db.neo4j.Neo4jUser","text":"<p>               Bases: <code>BaseGraphDB</code></p>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser.create_user","title":"create_user  <code>async</code>","text":"<pre><code>create_user(org_id: str, user_name: str) -&gt; models.User\n</code></pre> <p>Creates a new user in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_name</code> <p>Name for the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created</li> </ul> Source code in <code>memora/graph_db/neo4j/user.py</code> <pre><code>@override\nasync def create_user(self, org_id: str, user_name: str) -&gt; models.User:\n    \"\"\"\n    Creates a new user in the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_name (str): Name for the user.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_name)):\n        raise ValueError(\n            \"Both `org_id` and `user_name` must be a string and have a value.\"\n        )\n\n    user_id = shortuuid.uuid()\n    self.logger.info(f\"Creating new user with ID {user_id}\")\n\n    async def create_user_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org {org_id: $org_id})\n            CREATE (u:User {\n                org_id: $org_id,\n                user_id: $user_id,\n                user_name: $user_name,\n                created_at: datetime()\n            })\n            CREATE (u)-[:BELONGS_TO]-&gt;(o)\n            CREATE (ic:InteractionCollection {\n                org_id: $org_id,\n                user_id: $user_id\n            })\n            CREATE (mc:MemoryCollection {\n                org_id: $org_id,\n                user_id: $user_id\n            })\n            CREATE (u)-[:INTERACTIONS_IN]-&gt;(ic)\n            CREATE (u)-[:HAS_MEMORIES]-&gt;(mc)\n            RETURN u{.org_id, .user_id, .user_name, .created_at} as user\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            user_name=user_name,\n        )\n        record = await result.single()\n        return record[\"user\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        user_data = await session.execute_write(create_user_tx)\n\n        if user_data is None:\n            self.logger.info(f\"Failed to create user {user_id}\")\n            raise neo4j.exceptions.Neo4jError(\"Failed to create user.\")\n\n        return models.User(\n            org_id=user_data[\"org_id\"],\n            user_id=user_data[\"user_id\"],\n            user_name=user_data[\"user_name\"],\n            created_at=(user_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser.delete_user","title":"delete_user  <code>async</code>","text":"<pre><code>delete_user(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Deletes a user from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/graph_db/neo4j/user.py</code> <pre><code>@override\nasync def delete_user(self, org_id: str, user_id: str) -&gt; None:\n    \"\"\"\n    Deletes a user from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to delete.\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    self.logger.info(f\"Deleting user {user_id}\")\n\n    async def delete_user_tx(tx):\n        await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})\n            OPTIONAL MATCH (u)-[:INTERACTIONS_IN]-&gt;(interactioncollection)\n            OPTIONAL MATCH (interactioncollection)-[:HAD_INTERACTION]-&gt;(interaction)\n            OPTIONAL MATCH (interaction)-[:FIRST_MESSAGE|IS_NEXT*]-&gt;(message)\n            OPTIONAL MATCH (u)-[:HAS_MEMORIES]-&gt;(memcollection)\n            OPTIONAL MATCH (memcollection)-[:INCLUDES]-&gt;(memory)\n            OPTIONAL MATCH (interaction)-[:HAS_OCCURRENCE_ON]-&gt;(date)\n            DETACH DELETE u, interactioncollection, interaction, message, memcollection, memory, date\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n        )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_user_tx)\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser.get_all_org_users","title":"get_all_org_users  <code>async</code>","text":"<pre><code>get_all_org_users(org_id: str) -&gt; List[models.User]\n</code></pre> <p>Gets all users belonging to the specified organization from the graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[User]</code> <p>List[User], each containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/neo4j/user.py</code> <pre><code>@override\nasync def get_all_org_users(self, org_id: str) -&gt; List[models.User]:\n    \"\"\"\n    Gets all users belonging to the specified organization from the graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n\n    Returns:\n        List[User], each containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n\n    if not isinstance(org_id, str) or not org_id:\n        raise ValueError(\"`org_id` must be a string and have a value.\")\n\n    self.logger.info(f\"Getting all users for organization {org_id}\")\n\n    async def get_users_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (o:Org {org_id: $org_id})&lt;-[:BELONGS_TO]-(u:User)\n            RETURN u{.org_id, .user_id, .user_name, .created_at} as user\n        \"\"\",\n            org_id=org_id,\n        )\n        records = await result.value(\"user\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n\n        all_users_data = await session.execute_read(get_users_tx)\n\n        return [\n            models.User(\n                org_id=user_data[\"org_id\"],\n                user_id=user_data[\"user_id\"],\n                user_name=user_data[\"user_name\"],\n                created_at=(user_data[\"created_at\"]).to_native(),\n            )\n            for user_data in all_users_data\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser.get_user","title":"get_user  <code>async</code>","text":"<pre><code>get_user(org_id: str, user_id: str) -&gt; models.User\n</code></pre> <p>Gets a specific user belonging to the specified organization from the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/neo4j/user.py</code> <pre><code>@override\nasync def get_user(self, org_id: str, user_id: str) -&gt; models.User:\n    \"\"\"\n    Gets a specific user belonging to the specified organization from the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to retrieve.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    async def get_user_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})\n            RETURN u{.org_id, .user_id, .user_name, .created_at} as user\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n        )\n        record = await result.single()\n        return record[\"user\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        user_data = await session.execute_read(get_user_tx)\n\n        if user_data is None:\n            self.logger.info(f\"Failed to get user {user_id}: User does not exist\")\n            raise neo4j.exceptions.Neo4jError(\n                \"User (`org_id`, `user_id`) does not exist.\"\n            )\n\n        return models.User(\n            org_id=user_data[\"org_id\"],\n            user_id=user_data[\"user_id\"],\n            user_name=user_data[\"user_name\"],\n            created_at=(user_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jUser.update_user","title":"update_user  <code>async</code>","text":"<pre><code>update_user(\n    org_id: str, user_id: str, new_user_name: str\n) -&gt; models.User\n</code></pre> <p>Updates an existing user in the Neo4j graph database.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user to update.</p> <p> TYPE: <code>str</code> </p> <code>new_user_name</code> <p>The new name for the user.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>User</code> <p>User containing:</p> <ul> <li>org_id: Short UUID string</li> <li>user_id: Short UUID string</li> <li>user_name: User's name</li> <li>created_at: DateTime object of when the user was created.</li> </ul> Source code in <code>memora/graph_db/neo4j/user.py</code> <pre><code>@override\nasync def update_user(\n    self, org_id: str, user_id: str, new_user_name: str\n) -&gt; models.User:\n    \"\"\"\n    Updates an existing user in the Neo4j graph database.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user to update.\n        new_user_name (str): The new name for the user.\n\n    Returns:\n        User containing:\n\n            + org_id: Short UUID string\n            + user_id: Short UUID string\n            + user_name: User's name\n            + created_at: DateTime object of when the user was created.\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str)\n        for param in (org_id, user_id, new_user_name)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `new_user_name` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Updating user {user_id}\")\n\n    async def update_user_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})\n            SET u.user_name = $new_user_name\n            RETURN u{.org_id, .user_id, .user_name, .created_at} as user\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            new_user_name=new_user_name,\n        )\n\n        record = await result.single()\n        return record[\"user\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        user_data = await session.execute_write(update_user_tx)\n\n        if user_data is None:\n            self.logger.info(\n                f\"Failed to update user {user_id}: User does not exist\"\n            )\n            raise neo4j.exceptions.Neo4jError(\n                \"User (`org_id`, `user_id`) does not exist.\"\n            )\n\n        return models.User(\n            org_id=user_data[\"org_id\"],\n            user_id=user_data[\"user_id\"],\n            user_name=user_data[\"user_name\"],\n            created_at=(user_data[\"created_at\"]).to_native(),\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction","title":"memora.graph_db.neo4j.Neo4jInteraction","text":"<p>               Bases: <code>BaseGraphDB</code></p>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.delete_all_user_interactions_and_their_memories","title":"delete_all_user_interactions_and_their_memories  <code>async</code>","text":"<pre><code>delete_all_user_interactions_and_their_memories(\n    org_id: str, user_id: str\n) -&gt; None\n</code></pre> <p>Deletes all interactions and their associated memories for a specific user in an organization.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user whose interactions should be deleted</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def delete_all_user_interactions_and_their_memories(\n    self,\n    org_id: str,\n    user_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes all interactions and their associated memories for a specific user in an organization.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user whose interactions should be deleted\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    self.logger.info(f\"Deleting all interactions and memories for user {user_id}\")\n\n    async def delete_all_tx(tx):\n        await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})-[:INTERACTIONS_IN]-&gt;(ic)-[:HAD_INTERACTION]-&gt;(interaction:Interaction)\n\n            OPTIONAL MATCH (interaction)&lt;-[:INTERACTION_SOURCE]-(memory:Memory)\n            OPTIONAL MATCH (interaction)-[:HAS_OCCURRENCE_ON]-&gt;(date:Date)\n            OPTIONAL MATCH (interaction)-[:FIRST_MESSAGE|IS_NEXT*]-&gt;(messages:MessageBlock)\n\n            DETACH DELETE interaction, memory, date, messages\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n        )\n\n        if (\n            self.associated_vector_db\n        ):  # If the graph database is associated with a vector database\n            self.logger.info(\n                f\"Deleting all memories from vector database for user {user_id}\"\n            )\n            await self.associated_vector_db.delete_all_user_memories(\n                org_id, user_id\n            )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_all_tx)\n        self.logger.info(\n            f\"Successfully deleted all interactions and memories for user {user_id}\"\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.delete_user_interaction_and_its_memories","title":"delete_user_interaction_and_its_memories  <code>async</code>","text":"<pre><code>delete_user_interaction_and_its_memories(\n    org_id: str, user_id: str, interaction_id: str\n) -&gt; None\n</code></pre> <p>Deletes an interaction record and its associated memories.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction to delete.</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def delete_user_interaction_and_its_memories(\n    self,\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes an interaction record and its associated memories.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction to delete.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str)\n        for param in (org_id, user_id, interaction_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `interaction_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(\n        f\"Deleting interaction {interaction_id} and its memories for user {user_id}\"\n    )\n\n    interaction_memories = (\n        await self.get_interaction(\n            org_id, user_id, interaction_id, with_messages=False, with_memories=True\n        )\n    ).memories\n\n    interaction_memories_ids = [memory.memory_id for memory in interaction_memories]\n\n    async def delete_tx(tx):\n        # Delete the interaction, its messages and memories.\n        await tx.run(\n            \"\"\"\n            MATCH (interaction: Interaction {\n                org_id: $org_id, \n                user_id: $user_id, \n                interaction_id: $interaction_id\n                })-[r:FIRST_MESSAGE|IS_NEXT*]-&gt;(message:MessageBlock)\n\n            OPTIONAL MATCH (interaction)&lt;-[:INTERACTION_SOURCE]-(memory)\n            OPTIONAL MATCH (interaction)-[:HAS_OCCURRENCE_ON]-&gt;(date:Date) WHERE NOT (date)&lt;-[:HAS_OCCURRENCE_ON]-()\n\n            DETACH DELETE interaction, message, memory, date\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            interaction_id=interaction_id,\n        )\n\n        if (\n            self.associated_vector_db and interaction_memories_ids\n        ):  # If the graph database is associated with a vector database\n            # Delete memories from vector DB.\n            await self.associated_vector_db.delete_memories(\n                interaction_memories_ids\n            )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_tx)\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.get_all_user_interactions","title":"get_all_user_interactions  <code>async</code>","text":"<pre><code>get_all_user_interactions(\n    org_id: str,\n    user_id: str,\n    with_their_messages: bool = True,\n    with_their_memories: bool = True,\n    skip: int = 0,\n    limit: int = 100,\n) -&gt; List[models.Interaction]\n</code></pre> <p>Retrieves all interactions for a specific user in an organization.</p> Note <p>Interactions are sorted in descending order by their updated at datetime. (So most recent interactions are first).</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>with_their_messages</code> <p>Whether to also retrieve messages of an interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>with_their_memories</code> <p>Whether to also retrieve memories gotten across all occurrences of an interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip</code> <p>Number of interactions to skip. (Useful for pagination)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>limit</code> <p>Maximum number of interactions to retrieve. (Useful for pagination)</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RETURNS DESCRIPTION <code>List[Interaction]</code> <p>List[Interaction], each containing an Interaction with:</p> <ul> <li>org_id: Short UUID string identifying the organization.</li> <li>user_id: Short UUID string identifying the user.</li> <li>agent_id: Short UUID string identifying the agent.</li> <li>interaction_id: Short UUID string identifying the interaction.</li> <li>created_at: DateTime object of when the interaction was created.</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> <li>messages (if <code>with_their_messages</code> = True): List of messages in the interaction.</li> <li>memories (if <code>with_their_memories</code> = True): List of memories gotten from all occurrences of this interaction.</li> </ul> Note <p>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></p> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def get_all_user_interactions(\n    self,\n    org_id: str,\n    user_id: str,\n    with_their_messages: bool = True,\n    with_their_memories: bool = True,\n    skip: int = 0,\n    limit: int = 100,\n) -&gt; List[models.Interaction]:\n    \"\"\"\n    Retrieves all interactions for a specific user in an organization.\n\n    Note:\n        Interactions are sorted in descending order by their updated at datetime. (So most recent interactions are first).\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        with_their_messages (bool): Whether to also retrieve messages of an interaction.\n        with_their_memories (bool): Whether to also retrieve memories gotten across all occurrences of an interaction.\n        skip (int): Number of interactions to skip. (Useful for pagination)\n        limit (int): Maximum number of interactions to retrieve. (Useful for pagination)\n\n    Returns:\n        List[Interaction], each containing an Interaction with:\n\n            + org_id: Short UUID string identifying the organization.\n            + user_id: Short UUID string identifying the user.\n            + agent_id: Short UUID string identifying the agent.\n            + interaction_id: Short UUID string identifying the interaction.\n            + created_at: DateTime object of when the interaction was created.\n            + updated_at: DateTime object of when the interaction was last updated.\n            + messages (if `with_their_messages` = True): List of messages in the interaction.\n            + memories (if `with_their_memories` = True): List of memories gotten from all occurrences of this interaction.\n\n    Note:\n        A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    if not all(isinstance(param, int) for param in (skip, limit)):\n        raise ValueError(\"`skip` and `limit` must be integers.\")\n\n    self.logger.info(\n        f\"Retrieving all interactions for user {user_id} with messages={with_their_messages} and memories={with_their_memories}\"\n    )\n\n    async def get_interactions_tx(tx):\n\n        query = \"\"\"\n            MATCH (user:User {org_id: $org_id, user_id: $user_id})-[:INTERACTIONS_IN]-&gt;(ic)-[:HAD_INTERACTION]-&gt;(interaction:Interaction)\n            ORDER BY interaction.updated_at DESC\n            SKIP $skip\n            LIMIT $limit\n\n            // Initialize messages/memories upfront\n            WITH interaction, [] AS messages, [] AS memories\n        \"\"\"\n\n        if with_their_messages:\n            query += \"\"\"\n            OPTIONAL MATCH (interaction)-[:FIRST_MESSAGE|IS_NEXT*]-&gt;(m:MessageBlock)\n            WITH interaction, collect(m{.*}) as messages, memories\n            \"\"\"\n\n        if with_their_memories:\n            query += \"\"\"\n            OPTIONAL MATCH (interaction)&lt;-[:INTERACTION_SOURCE]-(mem:Memory)\n            OPTIONAL MATCH (mem)-[:MESSAGE_SOURCE]-&gt;(msg)\n\n            WITH interaction, messages, mem, collect(msg{.*}) AS msg_sources\n\n            OPTIONAL MATCH (user:User {org_id: mem.org_id, user_id: mem.user_id})              \n            OPTIONAL MATCH (agent:Agent {org_id: mem.org_id, agent_id: mem.agent_id})\n\n            WITH interaction, messages, collect(mem{\n                                                    .*, \n                                                    memory: apoc.text.replace(\n                                                        apoc.text.replace(mem.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                                                        '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                                                    ),\n                                                    message_sources: msg_sources\n                                                    }) as memories\n            \"\"\"\n\n        query += \"\"\"\n            RETURN interaction{\n                .org_id,\n                .user_id,\n                .agent_id,\n                .interaction_id,\n                .created_at,\n                .updated_at,\n                messages: messages,\n                memories: memories\n            } as interaction\n        \"\"\"\n\n        result = await tx.run(\n            query, org_id=org_id, user_id=user_id, skip=skip, limit=limit\n        )\n\n        records = await result.value(\"interaction\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        all_interactions_data = await session.execute_read(get_interactions_tx)\n\n        return [\n            models.Interaction(\n                org_id=interaction_data[\"org_id\"],\n                user_id=interaction_data[\"user_id\"],\n                agent_id=interaction_data[\"agent_id\"],\n                interaction_id=interaction_data[\"interaction_id\"],\n                created_at=(interaction_data[\"created_at\"]).to_native(),\n                updated_at=(interaction_data[\"updated_at\"]).to_native(),\n                messages=[\n                    models.MessageBlock(\n                        role=message.get(\"role\"),\n                        content=message.get(\"content\"),\n                        msg_position=message[\"msg_position\"],\n                    )\n                    for message in (interaction_data.get(\"messages\") or [])\n                ],\n                memories=[\n                    models.Memory(\n                        org_id=memory[\"org_id\"],\n                        agent_id=memory[\"agent_id\"],\n                        user_id=memory[\"user_id\"],\n                        interaction_id=memory[\"interaction_id\"],\n                        memory_id=memory[\"memory_id\"],\n                        memory=memory[\"memory\"],\n                        obtained_at=(memory[\"obtained_at\"]).to_native(),\n                        message_sources=[\n                            models.MessageBlock(\n                                role=msg.get(\"role\"),\n                                content=msg.get(\"content\"),\n                                msg_position=msg[\"msg_position\"],\n                            )\n                            for msg in (memory.get(\"message_sources\") or [])\n                        ],\n                    )\n                    for memory in (interaction_data.get(\"memories\") or [])\n                ],\n            )\n            for interaction_data in all_interactions_data\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.get_interaction","title":"get_interaction  <code>async</code>","text":"<pre><code>get_interaction(\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n    with_messages: bool = True,\n    with_memories: bool = True,\n) -&gt; models.Interaction\n</code></pre> <p>Retrieves all messages associated with a specific interaction.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction.</p> <p> TYPE: <code>str</code> </p> <code>with_messages</code> <p>Whether to retrieve messages along with the interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>with_memories</code> <p>Whether to also retrieve memories gotten across all occurrences of this interaction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Interaction</code> <p>Interaction containing:</p> <ul> <li>org_id: Short UUID string identifying the organization.</li> <li>user_id: Short UUID string identifying the user.</li> <li>agent_id: Short UUID string identifying the agent.</li> <li>interaction_id: Short UUID string identifying the interaction.</li> <li>created_at: DateTime object of when the interaction was created.</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> <li>messages (if <code>with_messages</code> = True): List of messages in the interaction.</li> <li>memories (if <code>with_memories</code> = True): List of memories gotten from all occurrences of this interaction.</li> </ul> Note <p>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></p> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def get_interaction(\n    self,\n    org_id: str,\n    user_id: str,\n    interaction_id: str,\n    with_messages: bool = True,\n    with_memories: bool = True,\n) -&gt; models.Interaction:\n    \"\"\"\n    Retrieves all messages associated with a specific interaction.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction.\n        with_messages (bool): Whether to retrieve messages along with the interaction.\n        with_memories (bool): Whether to also retrieve memories gotten across all occurrences of this interaction.\n\n    Returns:\n        Interaction containing:\n\n            + org_id: Short UUID string identifying the organization.\n            + user_id: Short UUID string identifying the user.\n            + agent_id: Short UUID string identifying the agent.\n            + interaction_id: Short UUID string identifying the interaction.\n            + created_at: DateTime object of when the interaction was created.\n            + updated_at: DateTime object of when the interaction was last updated.\n            + messages (if `with_messages` = True): List of messages in the interaction.\n            + memories (if `with_memories` = True): List of memories gotten from all occurrences of this interaction.\n\n    Note:\n        A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str)\n        for param in (org_id, user_id, interaction_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `interaction_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(\n        f\"Retrieving interaction {interaction_id} for user {user_id} with messages={with_messages} and memories={with_memories}\"\n    )\n\n    async def get_interaction_tx(tx):\n\n        query = \"\"\"\n            MATCH (interaction: Interaction {\n                org_id: $org_id, \n                user_id: $user_id, \n                interaction_id: $interaction_id\n            })\n\n            // Initialize messages/memories upfront\n            WITH interaction, [] AS messages, [] AS memories\n        \"\"\"\n\n        if with_messages:\n            query += \"\"\"\n            OPTIONAL MATCH (interaction)-[:FIRST_MESSAGE|IS_NEXT*]-&gt;(m:MessageBlock)\n            WITH interaction, collect(m{.*}) as messages, memories\n            \"\"\"\n\n        if with_memories:\n            query += \"\"\"\n            OPTIONAL MATCH (interaction)&lt;-[:INTERACTION_SOURCE]-(mem:Memory)\n            OPTIONAL MATCH (mem)-[:MESSAGE_SOURCE]-&gt;(msg)\n\n            WITH interaction, messages, mem, collect(msg{.*}) AS msg_sources\n\n            OPTIONAL MATCH (user:User {org_id: mem.org_id, user_id: mem.user_id})              \n            OPTIONAL MATCH (agent:Agent {org_id: mem.org_id, agent_id: mem.agent_id})\n\n            WITH interaction, messages, collect(mem{\n                                                    .*, \n                                                    memory: apoc.text.replace(\n                                                        apoc.text.replace(mem.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                                                        '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                                                    ),\n                                                    message_sources: msg_sources\n                                                    }) as memories\n            \"\"\"\n\n        query += \"\"\"\n            RETURN interaction{\n                .org_id,\n                .user_id,\n                .agent_id,\n                .interaction_id,\n                .created_at,\n                .updated_at,\n                messages: messages,\n                memories: memories\n            } as interaction\n        \"\"\"\n\n        result = await tx.run(\n            query,\n            org_id=org_id,\n            user_id=user_id,\n            interaction_id=interaction_id,\n        )\n\n        record = await result.single()\n        return record[\"interaction\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        interaction_data = await session.execute_read(get_interaction_tx)\n\n        if interaction_data is None:\n            self.logger.info(\n                f\"Interaction {interaction_id} not found for user {user_id}\"\n            )\n            raise neo4j.exceptions.Neo4jError(\n                \"Interaction (`org_id`, `user_id`, `interaction_id`) does not exist.\"\n            )\n\n        return models.Interaction(\n            org_id=interaction_data[\"org_id\"],\n            user_id=interaction_data[\"user_id\"],\n            agent_id=interaction_data[\"agent_id\"],\n            interaction_id=interaction_data[\"interaction_id\"],\n            created_at=(interaction_data[\"created_at\"]).to_native(),\n            updated_at=(interaction_data[\"updated_at\"]).to_native(),\n            messages=[\n                models.MessageBlock(\n                    role=message.get(\"role\"),\n                    content=message.get(\"content\"),\n                    msg_position=message[\"msg_position\"],\n                )\n                for message in (interaction_data.get(\"messages\") or [])\n            ],\n            memories=[\n                models.Memory(\n                    org_id=memory[\"org_id\"],\n                    agent_id=memory[\"agent_id\"],\n                    user_id=memory[\"user_id\"],\n                    interaction_id=memory[\"interaction_id\"],\n                    memory_id=memory[\"memory_id\"],\n                    memory=memory[\"memory\"],\n                    obtained_at=(memory[\"obtained_at\"]).to_native(),\n                    message_sources=[\n                        models.MessageBlock(\n                            role=msg.get(\"role\"),\n                            content=msg.get(\"content\"),\n                            msg_position=msg[\"msg_position\"],\n                        )\n                        for msg in (memory.get(\"message_sources\") or [])\n                    ],\n                )\n                for memory in (interaction_data.get(\"memories\") or [])\n            ],\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.save_interaction_with_memories","title":"save_interaction_with_memories  <code>async</code>","text":"<pre><code>save_interaction_with_memories(\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]\n</code></pre> <p>Creates a new interaction record with associated memories.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>memories_and_interaction</code> <p>Contains both the interaction and the associated memories.</p> <p> TYPE: <code>MemoriesAndInteraction</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also stored there for data consistency.</p> RETURNS DESCRIPTION <code>Tuple[str, datetime]</code> <p>Tuple[str, datetime] containing:</p> <ul> <li>interaction_id: Short UUID string identifying the created interaction</li> <li>created_at: DateTime object of when the interaction was created.</li> </ul> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def save_interaction_with_memories(\n    self,\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]:\n    \"\"\"\n    Creates a new interaction record with associated memories.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent.\n        user_id (str): Short UUID string identifying the user.\n        memories_and_interaction (MemoriesAndInteraction): Contains both the interaction and the associated memories.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also stored there for data consistency.\n\n    Returns:\n        Tuple[str, datetime] containing:\n\n            + interaction_id: Short UUID string identifying the created interaction\n            + created_at: DateTime object of when the interaction was created.\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, user_id, agent_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `agent_id` must be strings and have a value.\"\n        )\n\n    interaction_id = shortuuid.uuid()\n    new_memory_ids = [\n        str(uuid.uuid4()) for _ in range(len(memories_and_interaction.memories))\n    ]\n    new_contrary_memory_ids = [\n        str(uuid.uuid4())\n        for _ in range(len(memories_and_interaction.contrary_memories))\n    ]\n\n    self.logger.info(\n        f\"Saving interaction {interaction_id} for user {user_id} with agent {agent_id}\"\n    )\n\n    async def save_tx(tx):\n\n        # Create interaction and connect to date of occurance.\n        await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})-[:INTERACTIONS_IN]-&gt;(ic)\n            CREATE (interaction:Interaction {\n                org_id: $org_id,\n                user_id: $user_id,\n                agent_id: $agent_id,\n                interaction_id: $interaction_id,\n                created_at: datetime($interaction_date),\n                updated_at: datetime($interaction_date)\n            })\n            CREATE (ic)-[:HAD_INTERACTION]-&gt;(interaction)\n\n            WITH interaction, u\n            MERGE (d:Date {\n                org_id: $org_id,\n                user_id: $user_id,\n                date: date(datetime($interaction_date))\n            })\n            CREATE (interaction)-[:HAS_OCCURRENCE_ON]-&gt;(d)\n\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            agent_id=agent_id,\n            interaction_id=interaction_id,\n            interaction_date=memories_and_interaction.interaction_date.isoformat(),\n        )\n\n        if not memories_and_interaction.interaction:\n            self.logger.info(\n                f\"No messages to save for interaction {interaction_id}\"\n            )\n            return (\n                interaction_id,\n                memories_and_interaction.interaction_date.isoformat(),\n            )\n\n        # Add the messages to the interaction.\n        self.logger.info(f\"Adding messages to interaction {interaction_id}\")\n        await self._add_messages_to_interaction_from_top(\n            tx,\n            org_id,\n            user_id,\n            interaction_id,\n            memories_and_interaction.interaction,\n        )\n\n        if new_memory_ids or new_contrary_memory_ids:\n            # Add the all memories (new &amp; new contrary) and connect to their interaction message source.\n            self.logger.info(\"Adding memories and linking to their message source\")\n            await self._add_memories_with_their_source_links(\n                tx,\n                org_id,\n                user_id,\n                agent_id,\n                interaction_id,\n                memories_and_interaction,\n                new_memory_ids,\n                new_contrary_memory_ids,\n            )\n\n        if new_contrary_memory_ids:\n            # Link the new contary memories as updates to the old memory they contradicted.\n            self.logger.info(\n                \"Linking contrary memories to existing memories they contradicted\"\n            )\n            await self._link_update_contrary_memories_to_existing_memories(\n                tx,\n                org_id,\n                user_id,\n                new_contrary_memory_ids,\n                memories_and_interaction,\n            )\n\n        if new_memory_ids or new_contrary_memory_ids:\n            if (\n                self.associated_vector_db\n            ):  # If the graph database is associated with a vector database\n                # Add memories to vector DB within this transcation function to ensure data consistency (They succeed or fail together).\n                await self.associated_vector_db.add_memories(\n                    org_id=org_id,\n                    user_id=user_id,\n                    agent_id=agent_id,\n                    memory_ids=(\n                        new_memory_ids + new_contrary_memory_ids\n                    ),  # All memory ids\n                    memories=[\n                        memory_obj.memory\n                        for memory_obj in (\n                            memories_and_interaction.memories\n                            + memories_and_interaction.contrary_memories\n                        )\n                    ],  # All memories\n                    obtained_at=memories_and_interaction.interaction_date.isoformat(),\n                )\n\n        return interaction_id, memories_and_interaction.interaction_date\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        result = await session.execute_write(save_tx)\n        self.logger.info(\n            f\"Successfully saved interaction {interaction_id} for user {user_id}\"\n        )\n        return result\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jInteraction.update_interaction_and_memories","title":"update_interaction_and_memories  <code>async</code>","text":"<pre><code>update_interaction_and_memories(\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    interaction_id: str,\n    updated_memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]\n</code></pre> <p>Update an existing interaction record and add new memories.</p> Compares updated interaction with existing one <ul> <li>If differences are found, truncates existing record from that point and replaces with updated version. Old memories from truncated message(s) remain but become standalone (no longer linked to truncated messages).</li> <li>If no differences, appends new messages from the update.</li> </ul> <p>New memories are always added, regardless of interaction changes.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization.</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Short UUID string identifying the agent in the updated interaction.</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user.</p> <p> TYPE: <code>str</code> </p> <code>interaction_id</code> <p>Short UUID string identifying the interaction to update.</p> <p> TYPE: <code>str</code> </p> <code>updated_memories_and_interaction</code> <p>Contains both the updated interaction and the associated new memories.</p> <p> TYPE: <code>MemoriesAndInteraction</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also stored there for data consistency.</p> RETURNS DESCRIPTION <code>Tuple[str, datetime]</code> <p>Tuple[str, datetime] containing:</p> <ul> <li>interaction_id: Short UUID string identifying the updated interaction</li> <li>updated_at: DateTime object of when the interaction was last updated.</li> </ul> Source code in <code>memora/graph_db/neo4j/interaction.py</code> <pre><code>@override\nasync def update_interaction_and_memories(\n    self,\n    org_id: str,\n    agent_id: str,\n    user_id: str,\n    interaction_id: str,\n    updated_memories_and_interaction: MemoriesAndInteraction,\n) -&gt; Tuple[str, datetime]:\n    \"\"\"\n    Update an existing interaction record and add new memories.\n\n    Compares updated interaction with existing one:\n        - If differences are found, truncates existing record from that point and\n        replaces with updated version. Old memories from truncated message(s)\n        remain but become standalone (no longer linked to truncated messages).\n        - If no differences, appends new messages from the update.\n\n    New memories are always added, regardless of interaction changes.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization.\n        agent_id (str): Short UUID string identifying the agent in the updated interaction.\n        user_id (str): Short UUID string identifying the user.\n        interaction_id (str): Short UUID string identifying the interaction to update.\n        updated_memories_and_interaction (MemoriesAndInteraction): Contains both the updated interaction and the associated new memories.\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also stored there for data consistency.\n\n    Returns:\n        Tuple[str, datetime] containing:\n\n            + interaction_id: Short UUID string identifying the updated interaction\n            + updated_at: DateTime object of when the interaction was last updated.\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, user_id, agent_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `agent_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(\n        f\"Updating interaction {interaction_id} for user {user_id} with agent {agent_id}\"\n    )\n\n    new_memory_ids = [\n        str(uuid.uuid4())\n        for _ in range(len(updated_memories_and_interaction.memories))\n    ]\n    new_contrary_memory_ids = [\n        str(uuid.uuid4())\n        for _ in range(len(updated_memories_and_interaction.contrary_memories))\n    ]\n\n    # First get the existing messages.\n    existing_messages: List[models.MessageBlock] = (\n        await self.get_interaction(\n            org_id, user_id, interaction_id, with_messages=True, with_memories=False\n        )\n    ).messages\n\n    updated_interaction_length = len(updated_memories_and_interaction.interaction)\n    existing_interaction_length = len(existing_messages)\n\n    async def update_tx(tx):\n\n        # Case 1: Empty updated interaction - delete all existing messages\n        if updated_interaction_length == 0:\n            self.logger.info(\n                f\"Truncating all messages from interaction {interaction_id} as updated interaction is empty\"\n            )\n            await self._truncate_interaction_message_below_point(\n                tx, org_id, user_id, interaction_id, truncation_point_inclusive=0\n            )\n\n        # Case 2: Empty existing interaction - add all new messages from the top\n        elif existing_interaction_length == 0:\n            self.logger.info(f\"Adding all messages to interaction {interaction_id}\")\n            await self._add_messages_to_interaction_from_top(\n                tx,\n                org_id,\n                user_id,\n                interaction_id,\n                updated_memories_and_interaction.interaction,\n            )\n\n        # Case 3: Both interactions have messages - compare and update\n        else:\n            # Find first point of difference\n            truncate_from = -1\n            for i in range(\n                min(existing_interaction_length, updated_interaction_length)\n            ):\n                if (\n                    existing_messages[i].role\n                    != updated_memories_and_interaction.interaction[i].get(\"role\")\n                ) or (\n                    existing_messages[i].content\n                    != updated_memories_and_interaction.interaction[i].get(\n                        \"content\"\n                    )\n                ):\n                    truncate_from = i\n                    break\n\n            # If no differences found in prefix messages, but updated interaction is shorter\n            if (\n                truncate_from == -1\n                and updated_interaction_length &lt; existing_interaction_length\n            ):\n                truncate_from = updated_interaction_length\n\n            # Handle different cases based on where the difference was found\n            if truncate_from == -1:\n                # Append the new messages at the bottom.\n                self.logger.info(\n                    f\"Appending new messages to interaction {interaction_id}\"\n                )\n                await self._append_messages_to_interaction(\n                    tx,\n                    org_id,\n                    user_id,\n                    interaction_id,\n                    updated_memories_and_interaction.interaction,\n                )\n\n            elif truncate_from == 0:\n                # Complete replacement needed\n                self.logger.info(\n                    f\"Storing latest interaction {interaction_id} messages\"\n                )\n                await self._truncate_interaction_message_below_point(\n                    tx,\n                    org_id,\n                    user_id,\n                    interaction_id,\n                    truncation_point_inclusive=0,\n                )\n                await self._add_messages_to_interaction_from_top(\n                    tx,\n                    org_id,\n                    user_id,\n                    interaction_id,\n                    updated_memories_and_interaction.interaction,\n                )\n\n            elif truncate_from &gt; 0:\n                # Partial replacement needed\n                self.logger.info(\n                    f\"Updating messages in interaction {interaction_id} from position {truncate_from}\"\n                )\n                await self._truncate_interaction_message_below_point(\n                    tx, org_id, user_id, interaction_id, truncate_from\n                )\n                await self._append_messages_to_interaction(\n                    tx,\n                    org_id,\n                    user_id,\n                    interaction_id,\n                    updated_memories_and_interaction.interaction,\n                )\n\n        if new_memory_ids or new_contrary_memory_ids:\n            self.logger.info(\"Adding memories and linking to their source messages\")\n            await self._add_memories_with_their_source_links(\n                tx,\n                org_id,\n                user_id,\n                agent_id,\n                interaction_id,\n                updated_memories_and_interaction,\n                new_memory_ids,\n                new_contrary_memory_ids,\n            )\n\n        if new_contrary_memory_ids:\n            self.logger.info(\n                \"Linking contrary memories to existing memories they contradicted\"\n            )\n            await self._link_update_contrary_memories_to_existing_memories(\n                tx,\n                org_id,\n                user_id,\n                new_contrary_memory_ids,\n                updated_memories_and_interaction,\n            )\n\n        # Update the interaction agent, updated_at datetime, and connect occurance to the particular date.\n        await tx.run(\n            \"\"\"\n            MATCH (i:Interaction {\n                org_id: $org_id,\n                user_id: $user_id,\n                interaction_id: $interaction_id\n            })\n            SET i.updated_at = datetime($updated_date), i.agent_id = $agent_id\n            MERGE (d:Date {\n                org_id: $org_id,\n                user_id: $user_id,\n                date: date(datetime($updated_date))\n            })\n            MERGE (i)-[:HAS_OCCURRENCE_ON]-&gt;(d)\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            agent_id=agent_id,\n            interaction_id=interaction_id,\n            updated_date=updated_memories_and_interaction.interaction_date.isoformat(),\n        )\n\n        if new_memory_ids or new_contrary_memory_ids:\n            if self.associated_vector_db:\n                # If the graph database is associated with a vector database\n                # Add memories to vector DB within this transcation function to ensure data consistency (They succeed or fail together).\n                await self.associated_vector_db.add_memories(\n                    org_id=org_id,\n                    user_id=user_id,\n                    agent_id=agent_id,\n                    memory_ids=(\n                        new_memory_ids + new_contrary_memory_ids\n                    ),  # All memory ids\n                    memories=[\n                        memory_obj.memory\n                        for memory_obj in (\n                            updated_memories_and_interaction.memories\n                            + updated_memories_and_interaction.contrary_memories\n                        )\n                    ],  # All memories\n                    obtained_at=updated_memories_and_interaction.interaction_date.isoformat(),\n                )\n\n        return (\n            interaction_id,\n            updated_memories_and_interaction.interaction_date,\n        )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        result = await session.execute_write(update_tx)\n        self.logger.info(f\"Successfully updated interaction {interaction_id}\")\n        return result\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory","title":"memora.graph_db.neo4j.Neo4jMemory","text":"<p>               Bases: <code>BaseGraphDB</code></p>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory-functions","title":"Functions","text":""},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.delete_all_user_memories","title":"delete_all_user_memories  <code>async</code>","text":"<pre><code>delete_all_user_memories(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Deletes all memories of a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memories are also deleted there for data consistency.</p> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def delete_all_user_memories(\n    self,\n    org_id: str,\n    user_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes all memories of a specific user.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n\n    Note:\n        If the graph database is associated with a vector database, the memories are also deleted there for data consistency.\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    self.logger.info(f\"Deleting all memories for user {user_id}\")\n\n    async def delete_all_memories_tx(tx):\n        await tx.run(\n            \"\"\"\n            MATCH (u:User {org_id: $org_id, user_id: $user_id})-[:HAS_MEMORIES]-&gt;(mc:MemoryCollection)\n            MATCH (mc)-[:INCLUDES]-&gt;(memory:Memory)\n            DETACH DELETE memory\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n        )\n\n        if (\n            self.associated_vector_db\n        ):  # If the graph database is associated with a vector database\n            # Delete all memories from vector DB.\n            await self.associated_vector_db.delete_all_user_memories(\n                org_id, user_id\n            )\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_all_memories_tx)\n        self.logger.info(f\"Successfully deleted all memories for user {user_id}\")\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.delete_user_memory","title":"delete_user_memory  <code>async</code>","text":"<pre><code>delete_user_memory(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; None\n</code></pre> <p>Deletes a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory to delete</p> <p> TYPE: <code>str</code> </p> Note <p>If the graph database is associated with a vector database, the memory is also deleted there for data consistency.</p> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def delete_user_memory(\n    self,\n    org_id: str,\n    user_id: str,\n    memory_id: str,\n) -&gt; None:\n    \"\"\"\n    Deletes a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory to delete\n\n    Note:\n        If the graph database is associated with a vector database, the memory is also deleted there for data consistency.\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, user_id, memory_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `memory_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Deleting memory {memory_id}\")\n\n    async def delete_memory_tx(tx):\n        await tx.run(\n            \"\"\"\n            MATCH (m:Memory {org_id: $org_id, user_id: $user_id, memory_id: $memory_id})\n            DETACH DELETE m\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            memory_id=memory_id,\n        )\n\n        if (\n            self.associated_vector_db and memory_id\n        ):  # If the graph database is associated with a vector database\n            # Delete memory from vector DB.\n            await self.associated_vector_db.delete_memory(memory_id)\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.WRITE_ACCESS\n    ) as session:\n        await session.execute_write(delete_memory_tx)\n        self.logger.info(f\"Successfully deleted memory {memory_id}\")\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.fetch_user_memories_resolved","title":"fetch_user_memories_resolved  <code>async</code>","text":"<pre><code>fetch_user_memories_resolved(\n    org_user_mem_ids: List[Dict[str, str]]\n) -&gt; List[models.Memory]\n</code></pre> <p>Fetches memories from the Neo4j GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.</p> This method performs several operations <ol> <li>Retrieves memories using (org_id, user_id, memory_ids)</li> <li>If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version</li> <li>Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels</li> </ol> PARAMETER DESCRIPTION <code>org_user_mem_ids</code> <p>List of Dicts containing org, user, and memory ids of the memories to fetch and process</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Example <pre><code>&gt;&gt;&gt; org_user_mem_ids = [{'memory_id': '443ac3a8-fe87-49a4-93d2-05d3eb58ddeb', 'org_id': 'gmDr4sUiWMNqbGAiV8ijbU', 'user_id': 'CcyKXxhi2skEcDpRzNZim7'}, ...]\n&gt;&gt;&gt; memories = graphInstance.fetch_memories_resolved(org_user_mem_ids)\n&gt;&gt;&gt; print([memoryObj.memory for memoryObj in memories])\n[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"]\n</code></pre> Note <ul> <li>Org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.</li> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def fetch_user_memories_resolved(\n    self, org_user_mem_ids: List[Dict[str, str]]\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Fetches memories from the Neo4j GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.\n\n    This method performs several operations:\n      1. Retrieves memories using (org_id, user_id, memory_ids)\n      2. If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version\n      3. Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels\n\n    Args:\n        org_user_mem_ids (List[Dict[str, str]]): List of Dicts containing org, user, and memory ids of the memories to fetch and process\n\n    Returns:\n        List[Memory] containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Example:\n        ```python\n        &gt;&gt;&gt; org_user_mem_ids = [{'memory_id': '443ac3a8-fe87-49a4-93d2-05d3eb58ddeb', 'org_id': 'gmDr4sUiWMNqbGAiV8ijbU', 'user_id': 'CcyKXxhi2skEcDpRzNZim7'}, ...]\n        &gt;&gt;&gt; memories = graphInstance.fetch_memories_resolved(org_user_mem_ids)\n        &gt;&gt;&gt; print([memoryObj.memory for memoryObj in memories])\n        [\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"]\n        ```\n\n    Note:\n        - Org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    results = await self.fetch_user_memories_resolved_batch([org_user_mem_ids])\n    return results[0]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.fetch_user_memories_resolved_batch","title":"fetch_user_memories_resolved_batch  <code>async</code>","text":"<pre><code>fetch_user_memories_resolved_batch(\n    batch_org_user_mem_ids: List[List[Dict[str, str]]]\n) -&gt; List[List[models.Memory]]\n</code></pre> <p>Fetches memories from the Neo4j GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.</p> This method performs several operations <ol> <li>Retrieves memories using (org_id, user_id, memory_ids)</li> <li>If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version</li> <li>Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels</li> </ol> PARAMETER DESCRIPTION <code>batch_org_user_mem_ids</code> <p>List of lists containing Dicts with org, user, and memory ids of the memories to fetch and process</p> <p> TYPE: <code>List[List[Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[List[Memory]]</code> <p>List[List[Memory]] with memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Example <pre><code>&gt;&gt;&gt; batch_org_user_mem_ids = [[{\"memory_id\": \"413ac3a8-fe87-49a4-93d2-05d3eb58ddeb\", \"org_id\": \"gmDr4sUiWMNqbGAiV8ijbU\", \"user_id\": \"CcyKXxhi2skEcDpRzNZim7\"}, ...], [{...}, ...]]\n&gt;&gt;&gt; batch_memories = graphInstance.fetch_memories_resolved_batch(batch_org_user_mem_ids)\n&gt;&gt;&gt; print([[memoryObj.memory for memoryObj in memories] for memories in batch_memories])\n[[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"], [\"John is about to propose to Sarah\"]]\n</code></pre> Note <ul> <li>Batch org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.</li> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def fetch_user_memories_resolved_batch(\n    self, batch_org_user_mem_ids: List[List[Dict[str, str]]]\n) -&gt; List[List[models.Memory]]:\n    \"\"\"\n    Fetches memories from the Neo4j GraphDB by their IDs, resolves any contrary updates, and replaces user/agent placeholders with actual names.\n\n    This method performs several operations:\n      1. Retrieves memories using (org_id, user_id, memory_ids)\n      2. If a memory has a CONTRARY_UPDATE relationship, uses the newer memory version\n      3. Replaces user_id &amp; agent_id placeholders (e.g 'user_abc123' or 'agent_xyz789') in memories with actual user names / agent labels\n\n    Args:\n        batch_org_user_mem_ids (List[List[Dict[str, str]]]): List of lists containing Dicts with org, user, and memory ids of the memories to fetch and process\n\n    Returns:\n        List[List[Memory]] with memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Example:\n        ```python\n        &gt;&gt;&gt; batch_org_user_mem_ids = [[{\"memory_id\": \"413ac3a8-fe87-49a4-93d2-05d3eb58ddeb\", \"org_id\": \"gmDr4sUiWMNqbGAiV8ijbU\", \"user_id\": \"CcyKXxhi2skEcDpRzNZim7\"}, ...], [{...}, ...]]\n        &gt;&gt;&gt; batch_memories = graphInstance.fetch_memories_resolved_batch(batch_org_user_mem_ids)\n        &gt;&gt;&gt; print([[memoryObj.memory for memoryObj in memories] for memories in batch_memories])\n        [[\"John asked for help with a wedding ring\", \"Sarah is allergic to peanuts\"], [\"John is about to propose to Sarah\"]]\n        ```\n\n    Note:\n        - Batch org, user, and memory IDs are typically retrieved from a vector database before being passed to this method.\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    async def fetch_resolved_batch_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            UNWIND $batch_ids AS ids_dict_list\n\n            CALL (ids_dict_list) {\n\n                UNWIND ids_dict_list AS ids_dict\n                MATCH (memory:Memory {org_id: ids_dict.org_id, user_id: ids_dict.user_id, memory_id: ids_dict.memory_id})\n\n                // Use the most up to date contrary update memory if it exists\n                OPTIONAL MATCH (memory)-[:CONTRARY_UPDATE*]-&gt;(contraryMemory:Memory) WHERE NOT (contraryMemory)-[:CONTRARY_UPDATE]-&gt;()\n                WITH coalesce(contraryMemory, memory) AS memoryToReturn\n\n                OPTIONAL MATCH (memoryToReturn)-[:MESSAGE_SOURCE]-&gt;(msgSource)\n                WITH memoryToReturn, collect(msgSource{.*}) as msgSources\n\n                MATCH (user:User {org_id: memoryToReturn.org_id, user_id: memoryToReturn.user_id})              \n                MATCH (agent:Agent {org_id: memoryToReturn.org_id, agent_id: memoryToReturn.agent_id})\n\n                // Case-insensitive 'user_' or 'agent_' followed by UUID and optional ('s) placeholders are replaced with actual names\n                RETURN collect(DISTINCT memoryToReturn{\n                                            .org_id,\n                                            .agent_id,\n                                            .user_id,\n                                            .interaction_id,\n                                            .memory_id, \n                                            .obtained_at,\n                                            memory: apoc.text.replace(\n                                                apoc.text.replace(memoryToReturn.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                                                '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                                            ),\n                                            message_sources: msgSources\n                                        }) as resolved_memories\n            }\n            RETURN resolved_memories\n\n        \"\"\",\n            batch_ids=batch_org_user_mem_ids,\n        )\n\n        records = await result.value(\"resolved_memories\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n\n        all_resolved_memories = await session.execute_read(fetch_resolved_batch_tx)\n\n        return [\n            [\n                models.Memory(\n                    org_id=resolved_memory[\"org_id\"],\n                    agent_id=resolved_memory[\"agent_id\"],\n                    user_id=resolved_memory[\"user_id\"],\n                    interaction_id=resolved_memory[\"interaction_id\"],\n                    memory_id=resolved_memory[\"memory_id\"],\n                    memory=resolved_memory[\"memory\"],\n                    obtained_at=(resolved_memory[\"obtained_at\"]).to_native(),\n                    message_sources=[\n                        models.MessageBlock(\n                            role=msg_source[\"role\"],\n                            content=msg_source[\"content\"],\n                            msg_position=msg_source[\"msg_position\"],\n                        )\n                        for msg_source in (resolved_memory[\"message_sources\"] or [])\n                    ],\n                )\n                for resolved_memory in resolved_memories\n            ]\n            for resolved_memories in all_resolved_memories\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.get_all_user_memories","title":"get_all_user_memories  <code>async</code>","text":"<pre><code>get_all_user_memories(\n    org_id: str,\n    user_id: str,\n    agent_id: Optional[str] = None,\n) -&gt; List[models.Memory]\n</code></pre> <p>Retrieves all memories associated with a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Optional short UUID string identifying the agent. If provided, only memories obtained from interactions with this agent are returned. Otherwise, all memories associated with the user are returned.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def get_all_user_memories(\n    self, org_id: str, user_id: str, agent_id: Optional[str] = None\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Retrieves all memories associated with a specific user.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        agent_id (Optional[str]): Optional short UUID string identifying the agent. If provided, only memories obtained from\n            interactions with this agent are returned.\n            Otherwise, all memories associated with the user are returned.\n\n    Returns:\n        List[Memory] containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    if not all(param and isinstance(param, str) for param in (org_id, user_id)):\n        raise ValueError(\"`org_id` and `user_id` must be strings and have a value.\")\n\n    if agent_id:\n        if not isinstance(agent_id, str):\n            raise ValueError(\"`agent_id` must be a string.\")\n\n    self.logger.info(f\" and agent {agent_id}\" if agent_id else \"\")\n\n    async def get_all_memories_tx(tx):\n        query = \"\"\"\n            MATCH (user:User {{org_id: $org_id, user_id: $user_id}})-[:HAS_MEMORIES]-&gt;(mc:MemoryCollection)\n            MATCH (mc)-[:INCLUDES]-&gt;(m:Memory)\n            {agent_filter}\n            WITH m, user\n\n            OPTIONAL MATCH (m)-[:MESSAGE_SOURCE]-&gt;(msgSource)\n            WITH m, user, collect(msgSource{{.*}}) as msgSources\n\n            MATCH (agent:Agent {{org_id: m.org_id, agent_id: m.agent_id}})\n            RETURN m{{\n                .org_id,\n                .agent_id,\n                .user_id,\n                .interaction_id,\n                .memory_id, \n                .obtained_at,\n                memory: apoc.text.replace(\n                    apoc.text.replace(m.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                    '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                ),\n                message_sources: msgSources\n            }} as memory\n        \"\"\"\n        agent_filter = \"WHERE m.agent_id = $agent_id\" if agent_id else \"\"\n        result = await tx.run(\n            query.format(agent_filter=agent_filter),\n            org_id=org_id,\n            user_id=user_id,\n            agent_id=agent_id,\n        )\n\n        records = await result.value(\"memory\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        memories = await session.execute_read(get_all_memories_tx)\n        return [\n            models.Memory(\n                org_id=memory[\"org_id\"],\n                agent_id=memory[\"agent_id\"],\n                user_id=memory[\"user_id\"],\n                interaction_id=memory[\"interaction_id\"],\n                memory_id=memory[\"memory_id\"],\n                memory=memory[\"memory\"],\n                obtained_at=(memory[\"obtained_at\"]).to_native(),\n                message_sources=[\n                    models.MessageBlock(\n                        role=msg_source[\"role\"],\n                        content=msg_source[\"content\"],\n                        msg_position=msg_source[\"msg_position\"],\n                    )\n                    for msg_source in (memory[\"message_sources\"] or [])\n                ],\n            )\n            for memory in memories\n        ]\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.get_user_memory","title":"get_user_memory  <code>async</code>","text":"<pre><code>get_user_memory(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; models.Memory\n</code></pre> <p>Retrieves a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Memory</code> <p>Memory containing memory details:</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>The memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def get_user_memory(\n    self, org_id: str, user_id: str, memory_id: str\n) -&gt; models.Memory:\n    \"\"\"\n    Retrieves a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory\n\n    Returns:\n        Memory containing memory details:\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - The memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, user_id, memory_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `memory_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Getting memory {memory_id} for user {user_id}\")\n\n    async def get_memory_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH (m:Memory {org_id: $org_id, user_id: $user_id, memory_id: $memory_id})\n\n            MATCH (m)-[:MESSAGE_SOURCE]-&gt;(msgSource)\n            WITH m, collect(msgSource{.*}) as msgSources\n\n            MATCH (user:User {org_id: m.org_id, user_id: m.user_id})              \n            MATCH (agent:Agent {org_id: m.org_id, agent_id: m.agent_id})\n            RETURN m{\n                    .org_id,\n                    .agent_id,\n                    .user_id,\n                    .interaction_id,\n                    .memory_id, \n                    .obtained_at,\n                    memory: apoc.text.replace(\n                        apoc.text.replace(m.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                        '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                    ),\n                    message_sources: msgSources\n                } as memory\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            memory_id=memory_id,\n        )\n        record = await result.single()\n        return record[\"memory\"] if record else None\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        memory = await session.execute_read(get_memory_tx)\n\n        if not memory:\n            self.logger.info(\n                f\"Failed to get memory {memory_id}: Memory does not exist\"\n            )\n            raise neo4j.exceptions.Neo4jError(\n                \"Memory (`org_id`, `user_id`, `memory_id`) does not exist.\"\n            )\n\n        return models.Memory(\n            org_id=memory[\"org_id\"],\n            agent_id=memory[\"agent_id\"],\n            user_id=memory[\"user_id\"],\n            interaction_id=memory[\"interaction_id\"],\n            memory_id=memory[\"memory_id\"],\n            memory=memory[\"memory\"],\n            obtained_at=(memory[\"obtained_at\"]).to_native(),\n            message_sources=[\n                models.MessageBlock(\n                    role=msg_source[\"role\"],\n                    content=msg_source[\"content\"],\n                    msg_position=msg_source[\"msg_position\"],\n                )\n                for msg_source in (memory[\"message_sources\"] or [])\n            ],\n        )\n</code></pre>"},{"location":"api/graph_db/neo4j/#memora.graph_db.neo4j.Neo4jMemory.get_user_memory_history","title":"get_user_memory_history  <code>async</code>","text":"<pre><code>get_user_memory_history(\n    org_id: str, user_id: str, memory_id: str\n) -&gt; List[models.Memory]\n</code></pre> <p>Retrieves the history of a specific memory.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Short UUID string identifying the organization</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Short UUID string identifying the user</p> <p> TYPE: <code>str</code> </p> <code>memory_id</code> <p>UUID string identifying the memory</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Memory]</code> <p>List[Memory] containing the history of memory details in descending order (starting with the current version, to the oldest version):</p> <ul> <li>org_id: Short UUID string identifying the organization</li> <li>agent_id: Short UUID string identifying the agent</li> <li>user_id: Short UUID string identifying the user</li> <li>interaction_id: Short UUID string identifying the interaction the memory was sourced from</li> <li>memory_id: Full UUID string identifying the memory</li> <li>memory: The resolved memory</li> <li>obtained_at: DateTime object of when the memory was obtained</li> <li>message_sources: List of messages in the interaction that triggered the memory</li> </ul> Note <ul> <li>A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See <code>graph.update_interaction_and_memories</code></li> </ul> Source code in <code>memora/graph_db/neo4j/memory.py</code> <pre><code>@override\nasync def get_user_memory_history(\n    self, org_id: str, user_id: str, memory_id: str\n) -&gt; List[models.Memory]:\n    \"\"\"\n    Retrieves the history of a specific memory.\n\n    Args:\n        org_id (str): Short UUID string identifying the organization\n        user_id (str): Short UUID string identifying the user\n        memory_id (str): UUID string identifying the memory\n\n    Returns:\n        List[Memory] containing the history of memory details in descending order (starting with the current version, to the oldest version):\n\n            + org_id: Short UUID string identifying the organization\n            + agent_id: Short UUID string identifying the agent\n            + user_id: Short UUID string identifying the user\n            + interaction_id: Short UUID string identifying the interaction the memory was sourced from\n            + memory_id: Full UUID string identifying the memory\n            + memory: The resolved memory\n            + obtained_at: DateTime object of when the memory was obtained\n            + message_sources: List of messages in the interaction that triggered the memory\n\n    Note:\n        - A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`\n    \"\"\"\n\n    if not all(\n        param and isinstance(param, str) for param in (org_id, user_id, memory_id)\n    ):\n        raise ValueError(\n            \"`org_id`, `user_id` and `memory_id` must be strings and have a value.\"\n        )\n\n    self.logger.info(f\"Getting memory history for memory {memory_id}\")\n\n    async def get_memory_history_tx(tx):\n        result = await tx.run(\n            \"\"\"\n            MATCH path=(m:Memory {org_id: $org_id, user_id: $user_id, memory_id: $memory_id})&lt;-[:CONTRARY_UPDATE*0..]-(olderMemory:Memory)\n            WHERE NOT (olderMemory)&lt;-[:CONTRARY_UPDATE]-()\n            WITH nodes(path) AS memory_history\n            UNWIND memory_history AS memory\n\n            OPTIONAL MATCH (memory)-[:MESSAGE_SOURCE]-&gt;(msgSource)\n            WITH memory, collect(msgSource{.*}) as msgSources\n\n            MATCH (user:User {org_id: memory.org_id, user_id: memory.user_id})              \n            MATCH (agent:Agent {org_id: memory.org_id, agent_id: memory.agent_id})\n            RETURN memory{\n                    .org_id,\n                    .agent_id,\n                    .user_id,\n                    .interaction_id,\n                    .memory_id, \n                    .obtained_at,\n                    memory: apoc.text.replace(\n                        apoc.text.replace(memory.memory, '(?i)user_[a-z0-9\\\\-]+(?:\\\\'s)?', user.user_name), \n                        '(?i)agent_[a-z0-9\\\\-]+(?:\\\\'s)?',  agent.agent_label\n                    ),\n                    message_sources: msgSources\n                } as memory\n        \"\"\",\n            org_id=org_id,\n            user_id=user_id,\n            memory_id=memory_id,\n        )\n        records = await result.value(\"memory\", [])\n        return records\n\n    async with self.driver.session(\n        database=self.database, default_access_mode=neo4j.READ_ACCESS\n    ) as session:\n        memory_history = await session.execute_read(get_memory_history_tx)\n        return [\n            models.Memory(\n                org_id=memory[\"org_id\"],\n                agent_id=memory[\"agent_id\"],\n                user_id=memory[\"user_id\"],\n                interaction_id=memory[\"interaction_id\"],\n                memory_id=memory[\"memory_id\"],\n                memory=memory[\"memory\"],\n                obtained_at=(memory[\"obtained_at\"]).to_native(),\n                message_sources=[\n                    models.MessageBlock(\n                        role=msg_source[\"role\"],\n                        content=msg_source[\"content\"],\n                        msg_position=msg_source[\"msg_position\"],\n                    )\n                    for msg_source in (memory[\"message_sources\"] or [])\n                ],\n            )\n            for memory in memory_history\n        ]\n</code></pre>"},{"location":"api/llm_backends/azure_openai/","title":"<code>Azure OpenAI</code> Backend LLM Implementation","text":"<p>This section details the Azure OpenAI implementation of the <code>BaseBackendLLM</code> interface.</p>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM","title":"memora.llm_backends.AzureOpenAIBackendLLM","text":"<pre><code>AzureOpenAIBackendLLM(\n    azure_openai_client: AsyncAzureOpenAI = None,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n)\n</code></pre> <p>               Bases: <code>BaseBackendLLM</code></p> PARAMETER DESCRIPTION <code>azure_openai_client</code> <p>A pre-initialized Async Azure OpenAI client</p> <p> TYPE: <code>AsyncAzureOpenAI</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The name of the Azure OpenAI model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'gpt-4o'</code> </p> <code>temperature</code> <p>The temperature to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>top_p</code> <p>The top_p value to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>max_tokens</code> <p>The maximum number of tokens to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> Example <pre><code>from openai import AsyncAzureOpenAI\nfrom memora.llm_backends import AzureOpenAIBackendLLM\n\nazure_openai_backend_llm = AzureOpenAIBackendLLM(\n    azure_openai_client=AsyncAzureOpenAI(\n        azure_endpoint=\"AZURE_OPENAI_ENDPOINT\",\n        api_key=\"AZURE_OPENAI_API_KEY\",\n        api_version=\"API_VERSION\", # e.g \"2024-08-01-preview\" or later\n        max_retries=3\n        )\n    )\n</code></pre> Source code in <code>memora/llm_backends/azure_openai_backend_llm.py</code> <pre><code>def __init__(\n    self,\n    azure_openai_client: AsyncAzureOpenAI = None,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n):\n    \"\"\"\n    Initialize the AzureOpenAIBackendLLM class with the Azure OpenAI client and specific parameters.\n\n    Args:\n        azure_openai_client (AsyncAzureOpenAI): A pre-initialized Async Azure OpenAI client\n        model (str): The name of the Azure OpenAI model to use\n        temperature (float): The temperature to use for sampling\n        top_p (float): The top_p value to use for sampling\n        max_tokens (int): The maximum number of tokens to generate\n\n    Example:\n        ```python\n        from openai import AsyncAzureOpenAI\n        from memora.llm_backends import AzureOpenAIBackendLLM\n\n        azure_openai_backend_llm = AzureOpenAIBackendLLM(\n            azure_openai_client=AsyncAzureOpenAI(\n                azure_endpoint=\"AZURE_OPENAI_ENDPOINT\",\n                api_key=\"AZURE_OPENAI_API_KEY\",\n                api_version=\"API_VERSION\", # e.g \"2024-08-01-preview\" or later\n                max_retries=3\n                )\n            )\n        ```\n    \"\"\"\n\n    self.azure_client = azure_openai_client\n    self.model = model\n    self.temperature = temperature\n    self.top_p = top_p\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM-attributes","title":"Attributes","text":""},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.azure_client","title":"azure_client  <code>instance-attribute</code>","text":"<pre><code>azure_client = azure_openai_client\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.get_model_kwargs","title":"get_model_kwargs  <code>property</code>","text":"<pre><code>get_model_kwargs: Dict[str, Any]\n</code></pre> <p>Returns dictionary of model configuration parameters</p>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p = top_p\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM-functions","title":"Functions","text":""},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)</p> PARAMETER DESCRIPTION <code>messages</code> <p>List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>output_schema_model</code> <p>Optional Pydantic base model for structured output (\ud83d\udccc Ensure the api version and selected model supportes this.)</p> <p> TYPE: <code>Type[BaseModel] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[str, BaseModel]</code> <p>Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified</p> Source code in <code>memora/llm_backends/azure_openai_backend_llm.py</code> <pre><code>@override\nasync def __call__(\n    self,\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n    Args:\n        messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n        output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output (\ud83d\udccc Ensure the api version and selected model supportes this.)\n\n    Returns:\n        Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n    \"\"\"\n\n    if output_schema_model:\n        response = await self.azure_client.beta.chat.completions.parse(\n            messages=messages,\n            **self.get_model_kwargs,\n            response_format=output_schema_model,\n        )\n        return response.choices[0].message.parsed\n    else:\n        response = await self.azure_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/llm_backends/azure_openai/#memora.llm_backends.AzureOpenAIBackendLLM.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the LLM connection.</p> Source code in <code>memora/llm_backends/azure_openai_backend_llm.py</code> <pre><code>@override\nasync def close(self) -&gt; None:\n    \"\"\"Closes the LLM connection.\"\"\"\n\n    await self.azure_client.close()\n    self.azure_client = None\n</code></pre>"},{"location":"api/llm_backends/base/","title":"<code>BaseLLMBackend</code> Interface Class","text":"<p>This interface defines the contract that all LLMs implementations to be used in the backend by <code>Memora</code> must follow.</p>"},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM","title":"memora.llm_backends.base.BaseBackendLLM","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLMs used in the backend by Memora.</p>"},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM-attributes","title":"Attributes","text":""},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM.get_model_kwargs","title":"get_model_kwargs  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>get_model_kwargs: Dict[str, Any]\n</code></pre> <p>Returns dictionary of model configuration parameters</p> Example <p>return {     \"model\": self.model, # model_name: gpt-4o     \"temperature\": self.temperature, # 1     \"top_p\": self.top_p, # 1     \"max_tokens\": self.max_tokens, # 1024     \"stream\": False, }</p>"},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM-functions","title":"Functions","text":""},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM.__call__","title":"__call__  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>__call__(\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)</p> PARAMETER DESCRIPTION <code>messages</code> <p>List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>output_schema_model</code> <p>Optional Pydantic base model for structured output (\ud83d\udccc Ensure your model provider supports this for the chosen model)</p> <p> TYPE: <code>Type[BaseModel] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[str, BaseModel]</code> <p>Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified</p> Source code in <code>memora/llm_backends/base.py</code> <pre><code>@abstractmethod\nasync def __call__(\n    self,\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n    Args:\n        messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n        output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output (\ud83d\udccc Ensure your model provider supports this for the chosen model)\n\n    Returns:\n        Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llm_backends/base/#memora.llm_backends.base.BaseBackendLLM.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the LLM connection.</p> Source code in <code>memora/llm_backends/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Closes the LLM connection.\"\"\"\n    pass\n</code></pre>"},{"location":"api/llm_backends/groq/","title":"<code>Groq</code> Backend LLM Implementation","text":"<p>This section details the Groq implementation of the <code>BaseBackendLLM</code> interface.</p>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM","title":"memora.llm_backends.GroqBackendLLM","text":"<pre><code>GroqBackendLLM(\n    api_key: str,\n    model: str = \"llama-3.3-70b-specdec\",\n    temperature: float = 1,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n)\n</code></pre> <p>               Bases: <code>BaseBackendLLM</code></p> PARAMETER DESCRIPTION <code>api_key</code> <p>The API key to use for authentication</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>The name of the Groq model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'llama-3.3-70b-specdec'</code> </p> <code>temperature</code> <p>The temperature to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>top_p</code> <p>The top_p value to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>max_tokens</code> <p>The maximum number of tokens to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>max_retries</code> <p>The maximum number of retries for API requests</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Example <pre><code>from memora.llm_backends import GroqBackendLLM\n\ngroq_backend_llm = GroqBackendLLM(\n    api_key=\"GROQ_API_KEY\",\n    model=\"llama-3.3-70b-specdec\"\n)\n</code></pre> Source code in <code>memora/llm_backends/groq_backend_llm.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = \"llama-3.3-70b-specdec\",\n    temperature: float = 1,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize the GroqBackendLLM class with specific parameters.\n\n    Args:\n        api_key (str): The API key to use for authentication\n        model (str): The name of the Groq model to use\n        temperature (float): The temperature to use for sampling\n        top_p (float): The top_p value to use for sampling\n        max_tokens (int): The maximum number of tokens to generate\n        max_retries (int): The maximum number of retries for API requests\n\n    Example:\n        ```python\n        from memora.llm_backends import GroqBackendLLM\n\n        groq_backend_llm = GroqBackendLLM(\n            api_key=\"GROQ_API_KEY\",\n            model=\"llama-3.3-70b-specdec\"\n        )\n        ```\n    \"\"\"\n\n    self.groq_client = AsyncGroq(api_key=api_key, max_retries=max_retries)\n\n    self.model = model\n    self.temperature = temperature\n    self.top_p = top_p\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM-attributes","title":"Attributes","text":""},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.get_model_kwargs","title":"get_model_kwargs  <code>property</code>","text":"<pre><code>get_model_kwargs: Dict[str, Any]\n</code></pre> <p>Returns dictionary of model configuration parameters</p>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.groq_client","title":"groq_client  <code>instance-attribute</code>","text":"<pre><code>groq_client = AsyncGroq(\n    api_key=api_key, max_retries=max_retries\n)\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p = top_p\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM-functions","title":"Functions","text":""},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)</p> PARAMETER DESCRIPTION <code>messages</code> <p>List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>output_schema_model</code> <p>Optional Pydantic base model for structured output (\ud83d\udccc Ensure the choosen model supports this)</p> <p> TYPE: <code>Type[BaseModel] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[str, BaseModel]</code> <p>Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified</p> Source code in <code>memora/llm_backends/groq_backend_llm.py</code> <pre><code>@override\nasync def __call__(\n    self,\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n    Args:\n        messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n        output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output (\ud83d\udccc Ensure the choosen model supports this)\n\n    Returns:\n        Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n    \"\"\"\n\n    if output_schema_model:\n        response = await self.groq_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n            response_format={\"type\": \"json_object\"},\n        )\n        content = response.choices[0].message.content\n        return output_schema_model.model_validate_json(content)\n\n    else:\n        response = await self.groq_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/llm_backends/groq/#memora.llm_backends.GroqBackendLLM.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the LLM connection.</p> Source code in <code>memora/llm_backends/groq_backend_llm.py</code> <pre><code>@override\nasync def close(self) -&gt; None:\n    \"\"\"Closes the LLM connection.\"\"\"\n\n    await self.groq_client.close()\n    self.groq_client = None\n</code></pre>"},{"location":"api/llm_backends/openai/","title":"<code>OpenAI</code> Backend LLM Implementation","text":"<p>This section details the OpenAI implementation of the <code>BaseBackendLLM</code> interface.</p>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM","title":"memora.llm_backends.OpenAIBackendLLM","text":"<pre><code>OpenAIBackendLLM(\n    api_key: str,\n    organization: str | None = None,\n    project: str | None = None,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n)\n</code></pre> <p>               Bases: <code>BaseBackendLLM</code></p> PARAMETER DESCRIPTION <code>api_key</code> <p>The API key to use for authentication</p> <p> TYPE: <code>str</code> </p> <code>organization</code> <p>Your OpenAI organization ID</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>project</code> <p>Your OpenAI project ID</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The name of the OpenAI model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'gpt-4o'</code> </p> <code>temperature</code> <p>The temperature to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>top_p</code> <p>The top_p value to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>max_tokens</code> <p>The maximum number of tokens to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>max_retries</code> <p>The maximum number of retries to make if a request fails</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Example <pre><code>from memora.llm_backends import OpenAIBackendLLM\n\nopenai_backend_llm = OpenAIBackendLLM(\n    api_key=\"OPENAI_API_KEY\",\n    model=\"gpt-4o\"\n)\n</code></pre> Source code in <code>memora/llm_backends/openai_backend_llm.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    organization: str | None = None,\n    project: str | None = None,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize the OpenAIBackendLLM class with specific parameters.\n\n    Args:\n        api_key (str): The API key to use for authentication\n        organization (str | None): Your OpenAI organization ID\n        project (str | None): Your OpenAI project ID\n        model (str): The name of the OpenAI model to use\n        temperature (float): The temperature to use for sampling\n        top_p (float): The top_p value to use for sampling\n        max_tokens (int): The maximum number of tokens to generate\n        max_retries (int): The maximum number of retries to make if a request fails\n\n    Example:\n        ```python\n        from memora.llm_backends import OpenAIBackendLLM\n\n        openai_backend_llm = OpenAIBackendLLM(\n            api_key=\"OPENAI_API_KEY\",\n            model=\"gpt-4o\"\n        )\n        ```\n    \"\"\"\n\n    self.openai_client = AsyncOpenAI(\n        api_key=api_key,\n        organization=organization,\n        project=project,\n        max_retries=max_retries,\n    )\n\n    self.model = model\n    self.temperature = temperature\n    self.top_p = top_p\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM-attributes","title":"Attributes","text":""},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.get_model_kwargs","title":"get_model_kwargs  <code>property</code>","text":"<pre><code>get_model_kwargs: Dict[str, Any]\n</code></pre> <p>Returns dictionary of model configuration parameters</p>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.openai_client","title":"openai_client  <code>instance-attribute</code>","text":"<pre><code>openai_client = AsyncOpenAI(\n    api_key=api_key,\n    organization=organization,\n    project=project,\n    max_retries=max_retries,\n)\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p = top_p\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM-functions","title":"Functions","text":""},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)</p> PARAMETER DESCRIPTION <code>messages</code> <p>List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>output_schema_model</code> <p>Optional Pydantic base model for structured output.</p> <p> TYPE: <code>Type[BaseModel] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[str, BaseModel]</code> <p>Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified</p> Source code in <code>memora/llm_backends/openai_backend_llm.py</code> <pre><code>@override\nasync def __call__(\n    self,\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n    Args:\n        messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n        output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output.\n\n    Returns:\n        Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n    \"\"\"\n\n    if output_schema_model:\n        response = await self.openai_client.beta.chat.completions.parse(\n            messages=messages,\n            **self.get_model_kwargs,\n            response_format=output_schema_model,\n        )\n        return response.choices[0].message.parsed\n    else:\n        response = await self.openai_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/llm_backends/openai/#memora.llm_backends.OpenAIBackendLLM.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the LLM connection.</p> Source code in <code>memora/llm_backends/openai_backend_llm.py</code> <pre><code>@override\nasync def close(self) -&gt; None:\n    \"\"\"Closes the LLM connection.\"\"\"\n\n    await self.openai_client.close()\n    self.openai_client = None\n</code></pre>"},{"location":"api/llm_backends/together/","title":"<code>Together</code> Backend LLM Implementation","text":"<p>This section details the Together implementation of the <code>BaseBackendLLM</code> interface.</p>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM","title":"memora.llm_backends.TogetherBackendLLM","text":"<pre><code>TogetherBackendLLM(\n    api_key: str,\n    model: str = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n)\n</code></pre> <p>               Bases: <code>BaseBackendLLM</code></p> PARAMETER DESCRIPTION <code>api_key</code> <p>The API key to use for authentication</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>The name of the Together model to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'meta-llama/Llama-3.3-70B-Instruct-Turbo'</code> </p> <code>temperature</code> <p>The temperature to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>top_p</code> <p>The top_p value to use for sampling</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>max_tokens</code> <p>The maximum number of tokens to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>max_retries</code> <p>The maximum number of retries to make if a request fails</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Example <pre><code>from memora.llm_backends import TogetherBackendLLM\n\ntogether_backend_llm = TogetherBackendLLM(\n    api_key=\"TOGETHER_API_KEY\",\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n)\n</code></pre> Source code in <code>memora/llm_backends/together_backend_llm.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n    temperature: float = 0.7,\n    top_p: float = 1,\n    max_tokens: int = 1024,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize the TogetherBackendLLM class with specific parameters.\n\n    Args:\n        api_key (str): The API key to use for authentication\n        model (str): The name of the Together model to use\n        temperature (float): The temperature to use for sampling\n        top_p (float): The top_p value to use for sampling\n        max_tokens (int): The maximum number of tokens to generate\n        max_retries (int): The maximum number of retries to make if a request fails\n\n    Example:\n        ```python\n        from memora.llm_backends import TogetherBackendLLM\n\n        together_backend_llm = TogetherBackendLLM(\n            api_key=\"TOGETHER_API_KEY\",\n            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n        )\n        ```\n    \"\"\"\n\n    self.together_client = AsyncTogether(api_key=api_key, max_retries=max_retries)\n\n    self.model = model\n    self.temperature = temperature\n    self.top_p = top_p\n    self.max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM-attributes","title":"Attributes","text":""},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.get_model_kwargs","title":"get_model_kwargs  <code>property</code>","text":"<pre><code>get_model_kwargs: Dict[str, Any]\n</code></pre> <p>Returns dictionary of model configuration parameters</p>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.max_tokens","title":"max_tokens  <code>instance-attribute</code>","text":"<pre><code>max_tokens = max_tokens\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.together_client","title":"together_client  <code>instance-attribute</code>","text":"<pre><code>together_client = AsyncTogether(\n    api_key=api_key, max_retries=max_retries\n)\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.top_p","title":"top_p  <code>instance-attribute</code>","text":"<pre><code>top_p = top_p\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM-functions","title":"Functions","text":""},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]\n</code></pre> <p>Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)</p> PARAMETER DESCRIPTION <code>messages</code> <p>List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]</p> <p> TYPE: <code>List[Dict[str, str]]</code> </p> <code>output_schema_model</code> <p>Optional Pydantic base model for structured output (\ud83d\udccc Ensure the choosen model supports this)</p> <p> TYPE: <code>Type[BaseModel] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[str, BaseModel]</code> <p>Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified</p> Source code in <code>memora/llm_backends/together_backend_llm.py</code> <pre><code>@override\nasync def __call__(\n    self,\n    messages: List[Dict[str, str]],\n    output_schema_model: Type[BaseModel] | None = None,\n) -&gt; Union[str, BaseModel]:\n    \"\"\"\n    Process messages and generate response (\ud83d\udccc Streaming is not supported, as full response is required at once)\n\n    Args:\n        messages (List[Dict[str, str]]): List of message dicts with role and content e.g [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n        output_schema_model (Type[BaseModel] | None): Optional Pydantic base model for structured output (\ud83d\udccc Ensure the choosen model supports this)\n\n    Returns:\n        Union[str, BaseModel]: Generated text response as a string, or an instance of the output schema model if specified\n    \"\"\"\n\n    if output_schema_model:\n        response = await self.together_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": output_schema_model.model_json_schema(),\n            },\n        )\n        content = response.choices[0].message.content\n        return output_schema_model.model_validate_json(content)\n\n    else:\n        response = await self.together_client.chat.completions.create(\n            messages=messages,\n            **self.get_model_kwargs,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/llm_backends/together/#memora.llm_backends.TogetherBackendLLM.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the LLM connection.</p> Source code in <code>memora/llm_backends/together_backend_llm.py</code> <pre><code>@override\nasync def close(self) -&gt; None:\n    \"\"\"Closes the LLM connection.\"\"\"\n\n    self.together_client = None\n</code></pre>"},{"location":"api/schema/extraction/","title":"Memory Extraction Schema","text":"<p>Contains Pydantic data models that are used for structured memory extraction from the model (LLM).</p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ExtractedMemory","title":"memora.schema.extraction_schema.ExtractedMemory","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ExtractedMemory-attributes","title":"Attributes","text":""},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ExtractedMemory.memory","title":"memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memory: str = Field(\n    description=\"The memory, max 25 words, self-contained, remember use of #user_#id# or #agent_#id#.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ExtractedMemory.msg_source_ids","title":"msg_source_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msg_source_ids: list[int] = Field(\n    description=\"List with id or ids indicating which message blocks this memory was extracted from.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryExtractionResponse","title":"memora.schema.extraction_schema.MemoryExtractionResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryExtractionResponse-attributes","title":"Attributes","text":""},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryExtractionResponse.memories_first_pass","title":"memories_first_pass  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memories_first_pass: Optional[list[ExtractedMemory]] = (\n    Field(\n        default_factory=list,\n        description=\"First pass of useful info written for memory (remember use of #user_#id# or #agent_#id#) with their source message ids.\",\n    )\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryExtractionResponse.memories_second_pass","title":"memories_second_pass  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memories_second_pass: Optional[list[ExtractedMemory]] = (\n    Field(\n        default_factory=list,\n        description=\"Second pass containing info missed in first pass with their source message ids, if any.\",\n    )\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryExtractionResponse.memories_third_pass","title":"memories_third_pass  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memories_third_pass: Optional[list[ExtractedMemory]] = (\n    Field(\n        default_factory=list,\n        description=\"Third pass containing info missed in first and second passes with their source message ids, if any.\",\n    )\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.NewGleanedMemory","title":"memora.schema.extraction_schema.NewGleanedMemory","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.NewGleanedMemory-attributes","title":"Attributes","text":""},{"location":"api/schema/extraction/#memora.schema.extraction_schema.NewGleanedMemory.memory","title":"memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memory: str = Field(\n    description=\"A new gleaned memory, max 25 words, self-contained.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.NewGleanedMemory.source_candidate_pos_id","title":"source_candidate_pos_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_candidate_pos_id: int = Field(\n    description=\"The POS_ID of the candidate memory from which this new memory was gleaned.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ContraryMemory","title":"memora.schema.extraction_schema.ContraryMemory","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ContraryMemory-attributes","title":"Attributes","text":""},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ContraryMemory.contradicted_memory_id","title":"contradicted_memory_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>contradicted_memory_id: str = Field(\n    description=\"The ID of the existing memory that was contradicted.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ContraryMemory.memory","title":"memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memory: str = Field(\n    description=\"The candidate memory that directly contradicting an existing memory, max 25 words, self-contained.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.ContraryMemory.source_candidate_pos_id","title":"source_candidate_pos_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_candidate_pos_id: int = Field(\n    description=\"The POS_ID of the candidate memory from which this contrary memory was sourced.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryComparisonResponse","title":"memora.schema.extraction_schema.MemoryComparisonResponse","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryComparisonResponse-attributes","title":"Attributes","text":""},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryComparisonResponse.contrary_memories","title":"contrary_memories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>contrary_memories: list[ContraryMemory] = Field(\n    description=\"List of candidate memories that contradict existing ones, if any.\"\n)\n</code></pre>"},{"location":"api/schema/extraction/#memora.schema.extraction_schema.MemoryComparisonResponse.new_memories","title":"new_memories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>new_memories: list[NewGleanedMemory] = Field(\n    description=\"List of newly gleaned memories.\"\n)\n</code></pre>"},{"location":"api/schema/models/","title":"Models Used Throughout","text":"<p>Contains Pydantic data models that are used throughout the application for storing and retrieving from data sources.</p>"},{"location":"api/schema/models/#memora.schema.models.Organization","title":"memora.schema.models.Organization","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.Organization-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.Organization.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: datetime = Field(\n    description=\"DateTime object of when the organization was created.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Organization.org_id","title":"org_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_id: str = Field(\n    description=\"Short UUID string identifying the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Organization.org_name","title":"org_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_name: str = Field(\n    description=\"Name of the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Agent","title":"memora.schema.models.Agent","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.Agent-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.Agent.agent_id","title":"agent_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_id: str = Field(\n    description=\"Short UUID string identifying the agent.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Agent.agent_label","title":"agent_label  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_label: str = Field(\n    description=\"Label/name for the agent.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Agent.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: datetime = Field(\n    description=\"DateTime object of when the agent was created.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Agent.org_id","title":"org_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_id: str = Field(\n    description=\"Short UUID string identifying the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Agent.user_id","title":"user_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_id: Optional[str] = Field(\n    default=None,\n    description=\"Short UUID string identifying the user.\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.User","title":"memora.schema.models.User","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.User-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.User.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: datetime = Field(\n    description=\"DateTime object of when the user was created.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.User.org_id","title":"org_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_id: str = Field(\n    description=\"Short UUID string identifying the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.User.user_id","title":"user_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_id: str = Field(\n    description=\"Short UUID string identifying the user.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.User.user_name","title":"user_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_name: str = Field(description='Name of the user.')\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction","title":"memora.schema.models.Interaction","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.Interaction-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.Interaction.agent_id","title":"agent_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_id: str = Field(\n    description=\"Short UUID string identifying the agent.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: datetime = Field(\n    description=\"DateTime object of when the interaction was created.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.interaction_id","title":"interaction_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interaction_id: str = Field(\n    description=\"Short UUID string identifying the interaction.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.memories","title":"memories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memories: Optional[List[Memory]] = Field(\n    default=None,\n    description=\"List of memories gotten across all occurrences of this interaction.\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.messages","title":"messages  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>messages: Optional[List[MessageBlock]] = Field(\n    default=None,\n    description=\"List of messages in the interaction.\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.org_id","title":"org_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_id: str = Field(\n    description=\"Short UUID string identifying the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.updated_at","title":"updated_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>updated_at: datetime = Field(\n    description=\"DateTime object of when the interaction was last updated.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Interaction.user_id","title":"user_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_id: str = Field(\n    description=\"Short UUID string identifying the user.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.MessageBlock","title":"memora.schema.models.MessageBlock","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.MessageBlock-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.MessageBlock.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[str] = Field(\n    default=None,\n    description=\"The actual content of the message.\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.MessageBlock.msg_position","title":"msg_position  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msg_position: int = Field(\n    description=\"Position of this message in the interaction.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.MessageBlock.role","title":"role  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>role: Optional[str] = Field(\n    default=None, description=\"e.g., user, assistant, tool.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory","title":"memora.schema.models.Memory","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/models/#memora.schema.models.Memory-attributes","title":"Attributes","text":""},{"location":"api/schema/models/#memora.schema.models.Memory.agent_id","title":"agent_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agent_id: str = Field(\n    description=\"Short UUID string identifying the agent.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.interaction_id","title":"interaction_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interaction_id: Optional[str] = Field(\n    default=None,\n    description=\"Short UUID string identifying the interaction where this memory was sourced from.\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.memory","title":"memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memory: str = Field(description='The memory.')\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.memory_id","title":"memory_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memory_id: str = Field(\n    description=\"Full UUID string identifying the memory.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.message_sources","title":"message_sources  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>message_sources: Optional[List[MessageBlock]] = Field(\n    default=None,\n    description=\"List of messages in the interaction that triggered the memory. (Note: A memory won't have a message source, if its interaction was updated with a conflicting conversation thread that lead to truncation of the former thread. See `graph.update_interaction_and_memories`)\",\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.obtained_at","title":"obtained_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>obtained_at: datetime = Field(\n    description=\"DateTime object of when the memory was obtained.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.org_id","title":"org_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>org_id: str = Field(\n    description=\"Short UUID string identifying the organization.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.user_id","title":"user_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_id: str = Field(\n    description=\"Short UUID string identifying the user.\"\n)\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory-functions","title":"Functions","text":""},{"location":"api/schema/models/#memora.schema.models.Memory.id_memory_and_timestamp_dict","title":"id_memory_and_timestamp_dict","text":"<pre><code>id_memory_and_timestamp_dict()\n</code></pre> Source code in <code>memora/schema/models.py</code> <pre><code>def id_memory_and_timestamp_dict(self):\n    return {\n        \"memory_id\": self.memory_id,\n        \"memory\": self.memory,\n        \"obtained_at\": str(self.obtained_at),\n    }\n</code></pre>"},{"location":"api/schema/models/#memora.schema.models.Memory.memory_and_timestamp_dict","title":"memory_and_timestamp_dict","text":"<pre><code>memory_and_timestamp_dict()\n</code></pre> Source code in <code>memora/schema/models.py</code> <pre><code>def memory_and_timestamp_dict(self):\n    return {\"memory\": self.memory, \"obtained_at\": str(self.obtained_at)}\n</code></pre>"},{"location":"api/schema/storage/","title":"Save Memory Schema","text":"<p>Contains Pydantic data models that are used to ease memory storage in stores (e.g. vector and graph databases).</p>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoryToStore","title":"memora.schema.storage_schema.MemoryToStore","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoryToStore-attributes","title":"Attributes","text":""},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoryToStore.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory: str\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoryToStore.source_msg_block_pos","title":"source_msg_block_pos  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_msg_block_pos: list[int] = Field(\n    description=\"The position of the message block that resulted in this memory.\"\n)\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.ContraryMemoryToStore","title":"memora.schema.storage_schema.ContraryMemoryToStore","text":"<p>               Bases: <code>MemoryToStore</code></p>"},{"location":"api/schema/storage/#memora.schema.storage_schema.ContraryMemoryToStore-attributes","title":"Attributes","text":""},{"location":"api/schema/storage/#memora.schema.storage_schema.ContraryMemoryToStore.existing_contrary_memory_id","title":"existing_contrary_memory_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>existing_contrary_memory_id: str = Field(\n    description=\"The memory_id of the existing memory that was contradicted.\"\n)\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction","title":"memora.schema.storage_schema.MemoriesAndInteraction","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains both the interaction, its date and the associated memories to store in memory stores.</p>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction-attributes","title":"Attributes","text":""},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction.contrary_memories","title":"contrary_memories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>contrary_memories: list[ContraryMemoryToStore] = Field(\n    default=[],\n    description=\"The memory extracted from the interaction with the above but also the memory id of the existing memory they contradicted.\",\n)\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction.interaction","title":"interaction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interaction: list[dict[str, str]] = Field(\n    default=[],\n    description=\"The messages in the interaction [{'role': 'user', 'content': 'hello'}, ...]\",\n)\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction.interaction_date","title":"interaction_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interaction_date: datetime = Field(\n    default=now(),\n    description=\"The date and time the interaction occurred.\",\n)\n</code></pre>"},{"location":"api/schema/storage/#memora.schema.storage_schema.MemoriesAndInteraction.memories","title":"memories  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>memories: list[MemoryToStore] = Field(\n    default=[],\n    description=\"The memories extracted from the interaction with their source messages position.\",\n)\n</code></pre>"},{"location":"api/vector_db/base/","title":"<code>BaseVectorDB</code> Interface Class","text":"<p>This interface defines the contract that all vector database implementations must follow.</p>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB","title":"memora.vector_db.base.BaseVectorDB","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining a common interface for different Vector DB implementations.</p> <p>This class provides a standardized interface for vector database operations including adding, searching, and deleting memories.</p>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB-functions","title":"Functions","text":""},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.add_memories","title":"add_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>add_memories(\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    memory_ids: List[uuid.UUID],\n    memories: List[str],\n    obtained_at: str,\n) -&gt; None\n</code></pre> <p>Add memories to collection with their org_id, user_id, agent_id, and obtained_at datetime as metadata.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Agent ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>memory_ids</code> <p>List of UUIDs for each memory</p> <p> TYPE: <code>List[UUID]</code> </p> <code>memories</code> <p>List of memory strings to add</p> <p> TYPE: <code>List[str]</code> </p> <code>obtained_at</code> <p>ISO format datetime string when the memories were obtained</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def add_memories(\n    self,\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    memory_ids: List[uuid.UUID],\n    memories: List[str],\n    obtained_at: str,\n) -&gt; None:\n    \"\"\"\n    Add memories to collection with their org_id, user_id, agent_id, and obtained_at datetime as metadata.\n\n    Args:\n        org_id (str): Organization ID for the memories\n        user_id (str): User ID for the memories\n        agent_id (str): Agent ID for the memories\n        memory_ids (List[uuid.UUID]): List of UUIDs for each memory\n        memories (List[str]): List of memory strings to add\n        obtained_at (str): ISO format datetime string when the memories were obtained\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.close","title":"close  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the database connection.</p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def close(self) -&gt; None:\n    \"\"\"Closes the database connection.\"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.delete_all_organization_memories","title":"delete_all_organization_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_all_organization_memories(org_id: str) -&gt; None\n</code></pre> <p>Delete all memories associated with an organization.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>ID of the organization whose memories should be deleted</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_all_organization_memories(self, org_id: str) -&gt; None:\n    \"\"\"\n    Delete all memories associated with an organization.\n\n    Args:\n        org_id (str): ID of the organization whose memories should be deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.delete_all_user_memories","title":"delete_all_user_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_all_user_memories(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Delete all memories associated with a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID the user belongs to</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>ID of the user whose memories should be deleted</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_all_user_memories(self, org_id: str, user_id: str) -&gt; None:\n    \"\"\"\n    Delete all memories associated with a specific user.\n\n    Args:\n        org_id (str): Organization ID the user belongs to\n        user_id (str): ID of the user whose memories should be deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.delete_memories","title":"delete_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_memories(memory_ids: List[str]) -&gt; None\n</code></pre> <p>Delete multiple memories by their IDs.</p> PARAMETER DESCRIPTION <code>memory_ids</code> <p>List of memory IDs to delete</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_memories(self, memory_ids: List[str]) -&gt; None:\n    \"\"\"\n    Delete multiple memories by their IDs.\n\n    Args:\n        memory_ids (List[str]): List of memory IDs to delete\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.delete_memory","title":"delete_memory  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>delete_memory(memory_id: str) -&gt; None\n</code></pre> <p>Delete a memory by its ID with optional org/user filtering.</p> PARAMETER DESCRIPTION <code>memory_id</code> <p>ID of the memory to delete</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def delete_memory(self, memory_id: str) -&gt; None:\n    \"\"\"\n    Delete a memory by its ID with optional org/user filtering.\n\n    Args:\n        memory_id (str): ID of the memory to delete\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.search_memories","title":"search_memories  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>search_memories(\n    queries: List[str],\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[List[Tuple[models.Memory, float]]]\n</code></pre> <p>Batch memory search with optional user/agent filtering.</p> PARAMETER DESCRIPTION <code>queries</code> <p>List of search query strings</p> <p> TYPE: <code>List[str]</code> </p> <code>memory_search_scope</code> <p>Memory search scope (organization or user)</p> <p> TYPE: <code>MemorySearchScope</code> </p> <code>org_id</code> <p>Organization ID for filtering</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional user ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>agent_id</code> <p>Optional agent ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[List[Tuple[Memory, float]]]</code> <p>List[List[Tuple[models.Memory, float]]] of search results for each query, with a tuple containing: Memory:</p> <pre><code>+ org_id: str\n+ agent_id: str\n+ user_id: str\n+ memory_id: str\n+ memory: str\n+ obtained_at: datetime\n</code></pre> <p>float: Score of the memory</p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def search_memories(\n    self,\n    queries: List[str],\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[List[Tuple[models.Memory, float]]]:\n    \"\"\"\n    Batch memory search with optional user/agent filtering.\n\n    Args:\n        queries (List[str]): List of search query strings\n        memory_search_scope (MemorySearchScope): Memory search scope (organization or user)\n        org_id (str): Organization ID for filtering\n        user_id (Optional[str]): Optional user ID for filtering\n        agent_id (Optional[str]): Optional agent ID for filtering\n\n    Returns:\n        List[List[Tuple[models.Memory, float]]] of search results for each query, with a tuple containing:\n            Memory:\n\n                + org_id: str\n                + agent_id: str\n                + user_id: str\n                + memory_id: str\n                + memory: str\n                + obtained_at: datetime\n\n            float: Score of the memory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.search_memory","title":"search_memory  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>search_memory(\n    query: str,\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[Tuple[models.Memory, float]]\n</code></pre> <p>Memory search with optional user/agent filtering.</p> PARAMETER DESCRIPTION <code>query</code> <p>Search query string</p> <p> TYPE: <code>str</code> </p> <code>memory_search_scope</code> <p>Memory search scope (organization or user)</p> <p> TYPE: <code>MemorySearchScope</code> </p> <code>org_id</code> <p>Organization ID for filtering</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional user ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>agent_id</code> <p>Optional agent ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Tuple[Memory, float]]</code> <p>List[Tuple[Memory, float]] containing tuple of search results and score: Memory:</p> <pre><code>+ org_id: str\n+ agent_id: str\n+ user_id: str\n+ memory_id: str\n+ memory: str\n+ obtained_at: datetime\n</code></pre> <p>float: Score of the memory</p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def search_memory(\n    self,\n    query: str,\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[Tuple[models.Memory, float]]:\n    \"\"\"\n    Memory search with optional user/agent filtering.\n\n    Args:\n        query (str): Search query string\n        memory_search_scope (MemorySearchScope): Memory search scope (organization or user)\n        org_id (str): Organization ID for filtering\n        user_id (Optional[str]): Optional user ID for filtering\n        agent_id (Optional[str]): Optional agent ID for filtering\n\n    Returns:\n        List[Tuple[Memory, float]] containing tuple of search results and score:\n            Memory:\n\n                + org_id: str\n                + agent_id: str\n                + user_id: str\n                + memory_id: str\n                + memory: str\n                + obtained_at: datetime\n\n            float: Score of the memory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/base/#memora.vector_db.base.BaseVectorDB.setup","title":"setup  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>setup(*args, **kwargs) -&gt; None\n</code></pre> <p>Setup the vector database by initializing collections, indices, etc.</p> Source code in <code>memora/vector_db/base.py</code> <pre><code>@abstractmethod\nasync def setup(self, *args, **kwargs) -&gt; None:\n    \"\"\"Setup the vector database by initializing collections, indices, etc.\"\"\"\n    pass\n</code></pre>"},{"location":"api/vector_db/memory_search_scope/","title":"<code>MemorySearchScope</code> Enum Class","text":"<p>This enum class defines the possible scopes for vector memory search, which can be either \"organization\" or \"user\".</p>"},{"location":"api/vector_db/memory_search_scope/#memora.vector_db.base.MemorySearchScope","title":"memora.vector_db.base.MemorySearchScope","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/vector_db/memory_search_scope/#memora.vector_db.base.MemorySearchScope-attributes","title":"Attributes","text":""},{"location":"api/vector_db/memory_search_scope/#memora.vector_db.base.MemorySearchScope.ORGANIZATION","title":"ORGANIZATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ORGANIZATION = 'organization'\n</code></pre>"},{"location":"api/vector_db/memory_search_scope/#memora.vector_db.base.MemorySearchScope.USER","title":"USER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER = 'user'\n</code></pre>"},{"location":"api/vector_db/qdrant/","title":"<code>Qdrant</code> VectorDB Implementation","text":"<p>This section details the Qdrant implementation of the <code>BaseVectorDB</code> interface.</p>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB","title":"memora.vector_db.qdrant.QdrantDB","text":"<pre><code>QdrantDB(\n    async_client: AsyncQdrantClient = None,\n    collection_name: str = \"memory_collection_v0_2\",\n    embed_models_cache_dir: str = \"./cache\",\n    enable_logging: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseVectorDB</code></p> PARAMETER DESCRIPTION <code>async_client</code> <p>A pre-initialized Async Qdrant client</p> <p> TYPE: <code>AsyncQdrantClient</code> DEFAULT: <code>None</code> </p> <code>collection_name</code> <p>Name of the Qdrant collection</p> <p> TYPE: <code>str</code> DEFAULT: <code>'memory_collection_v0_2'</code> </p> <code>embed_models_cache_dir</code> <p>Directory to cache the embedding models</p> <p> TYPE: <code>str</code> DEFAULT: <code>'./cache'</code> </p> <code>enable_logging</code> <p>Whether to enable console logging</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>from qdrant_client import AsyncQdrantClient\nfrom memora.vector_db.qdrant import QdrantDB\n\nqdrant_db = QdrantDB(\n                async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\")\n            )\n</code></pre> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>def __init__(\n    self,\n    async_client: AsyncQdrantClient = None,\n    collection_name: str = \"memory_collection_v0_2\",\n    embed_models_cache_dir: str = \"./cache\",\n    enable_logging: bool = False,\n):\n    \"\"\"\n    Initialize the QdrantDB class.\n\n    Args:\n        async_client (AsyncQdrantClient): A pre-initialized Async Qdrant client\n        collection_name (str): Name of the Qdrant collection\n        embed_models_cache_dir (str): Directory to cache the embedding models\n        enable_logging (bool): Whether to enable console logging\n\n    Example:\n        ```python\n        from qdrant_client import AsyncQdrantClient\n        from memora.vector_db.qdrant import QdrantDB\n\n        qdrant_db = QdrantDB(\n                        async_client=AsyncQdrantClient(url=\"QDRANT_URL\", api_key=\"QDRANT_API_KEY\")\n                    )\n        ```\n    \"\"\"\n\n    # Set Qdrant Client.\n    self.async_client: AsyncQdrantClient = async_client\n\n    # Set both dense and sparse embedding models to use for hybrid search.\n    self.vector_embedding_model: str = (\n        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n    )\n    self.sparse_vector_embedding_model: str = \"prithivida/Splade_PP_en_v1\"\n\n    self.async_client.set_model(\n        self.vector_embedding_model, cache_dir=embed_models_cache_dir\n    )\n    self.async_client.set_sparse_model(\n        self.sparse_vector_embedding_model, cache_dir=embed_models_cache_dir\n    )\n\n    # Set the collection name.\n    self.collection_name = collection_name\n\n    # Configure logging\n    self.logger = logging.getLogger(__name__)\n    if enable_logging:\n        logging.basicConfig(level=logging.INFO)\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB-attributes","title":"Attributes","text":""},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.async_client","title":"async_client  <code>instance-attribute</code>","text":"<pre><code>async_client: AsyncQdrantClient = async_client\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.collection_name","title":"collection_name  <code>instance-attribute</code>","text":"<pre><code>collection_name = collection_name\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.sparse_vector_embedding_model","title":"sparse_vector_embedding_model  <code>instance-attribute</code>","text":"<pre><code>sparse_vector_embedding_model: str = (\n    \"prithivida/Splade_PP_en_v1\"\n)\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.vector_embedding_model","title":"vector_embedding_model  <code>instance-attribute</code>","text":"<pre><code>vector_embedding_model: str = (\n    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n)\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB-functions","title":"Functions","text":""},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.add_memories","title":"add_memories  <code>async</code>","text":"<pre><code>add_memories(\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    memory_ids: List[uuid.UUID],\n    memories: List[str],\n    obtained_at: str,\n) -&gt; None\n</code></pre> <p>Add memories to collection with their org_id, user_id, agent_id, and obtained_at datetime as metadata.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>agent_id</code> <p>Agent ID for the memories</p> <p> TYPE: <code>str</code> </p> <code>memory_ids</code> <p>List of UUIDs for each memory</p> <p> TYPE: <code>List[UUID]</code> </p> <code>memories</code> <p>List of memory strings to add</p> <p> TYPE: <code>List[str]</code> </p> <code>obtained_at</code> <p>ISO format datetime string when the memories were obtained</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def add_memories(\n    self,\n    org_id: str,\n    user_id: str,\n    agent_id: str,\n    memory_ids: List[uuid.UUID],\n    memories: List[str],\n    obtained_at: str,\n) -&gt; None:\n    \"\"\"\n    Add memories to collection with their org_id, user_id, agent_id, and obtained_at datetime as metadata.\n\n    Args:\n        org_id (str): Organization ID for the memories\n        user_id (str): User ID for the memories\n        agent_id (str): Agent ID for the memories\n        memory_ids (List[uuid.UUID]): List of UUIDs for each memory\n        memories (List[str]): List of memory strings to add\n        obtained_at (str): ISO format datetime string when the memories were obtained\n    \"\"\"\n\n    if not memories:\n        raise ValueError(\"At least one memory and its memory id is required\")\n\n    if len(memories) != len(memory_ids):\n        raise ValueError(\"Length of memories and memory_ids must match\")\n\n    metadata = [\n        {\n            \"org_id\": org_id,\n            \"org_user_id\": f\"{org_id}:{user_id}\",\n            \"user_id\": user_id,\n            \"agent_id\": agent_id,\n            \"obtained_at\": obtained_at,\n        }\n        for _ in memories\n    ]\n\n    await self.async_client.add(\n        collection_name=self.collection_name,\n        documents=memories,\n        metadata=metadata,\n        ids=[str(memory_id) for memory_id in memory_ids],\n        # parallel=_  # Use all CPU cores\n    )\n    self.logger.info(\n        f\"Added {len(memories)} memories to collection: {self.collection_name}\"\n    )\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Closes the qdrant database connection.</p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def close(self) -&gt; None:\n    \"\"\"Closes the qdrant database connection.\"\"\"\n\n    await self.async_client.close()\n    self.logger.info(\"QdrantDB connection closed\")\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.delete_all_organization_memories","title":"delete_all_organization_memories  <code>async</code>","text":"<pre><code>delete_all_organization_memories(org_id: str) -&gt; None\n</code></pre> <p>Delete all memories associated with an organization.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>ID of the organization whose memories should be deleted</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def delete_all_organization_memories(self, org_id: str) -&gt; None:\n    \"\"\"\n    Delete all memories associated with an organization.\n\n    Args:\n        org_id (str): ID of the organization whose memories should be deleted\n    \"\"\"\n\n    filter_conditions = [\n        models.FieldCondition(key=\"org_id\", match=models.MatchValue(value=org_id))\n    ]\n\n    await self.async_client.delete(\n        collection_name=self.collection_name,\n        points_selector=models.Filter(must=filter_conditions),\n    )\n    self.logger.info(f\"Deleted all memories for organization {org_id}\")\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.delete_all_user_memories","title":"delete_all_user_memories  <code>async</code>","text":"<pre><code>delete_all_user_memories(org_id: str, user_id: str) -&gt; None\n</code></pre> <p>Delete all memories associated with a specific user.</p> PARAMETER DESCRIPTION <code>org_id</code> <p>Organization ID the user belongs to</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>ID of the user whose memories should be deleted</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def delete_all_user_memories(self, org_id: str, user_id: str) -&gt; None:\n    \"\"\"\n    Delete all memories associated with a specific user.\n\n    Args:\n        org_id (str): Organization ID the user belongs to\n        user_id (str): ID of the user whose memories should be deleted\n    \"\"\"\n\n    await self.async_client.delete(\n        collection_name=self.collection_name,\n        points_selector=models.Filter(\n            must=models.FieldCondition(\n                key=\"org_user_id\",\n                match=models.MatchValue(value=f\"{org_id}:{user_id}\"),\n            )\n        ),\n    )\n    self.logger.info(\n        f\"Deleted all memories for user {user_id} in organization {org_id}\"\n    )\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.delete_memories","title":"delete_memories  <code>async</code>","text":"<pre><code>delete_memories(memory_ids: List[str]) -&gt; None\n</code></pre> <p>Delete multiple memories by their IDs.</p> PARAMETER DESCRIPTION <code>memory_ids</code> <p>List of memory IDs to delete</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def delete_memories(self, memory_ids: List[str]) -&gt; None:\n    \"\"\"\n    Delete multiple memories by their IDs.\n\n    Args:\n        memory_ids (List[str]): List of memory IDs to delete\n    \"\"\"\n\n    if memory_ids:\n        await self.async_client.delete(\n            collection_name=self.collection_name,\n            points_selector=models.PointIdsList(points=memory_ids),\n        )\n        self.logger.info(f\"Deleted memories with IDs: {memory_ids}\")\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.delete_memory","title":"delete_memory  <code>async</code>","text":"<pre><code>delete_memory(memory_id: str) -&gt; None\n</code></pre> <p>Delete a memory by its ID with optional org/user filtering.</p> PARAMETER DESCRIPTION <code>memory_id</code> <p>ID of the memory to delete</p> <p> TYPE: <code>str</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def delete_memory(self, memory_id: str) -&gt; None:\n    \"\"\"\n    Delete a memory by its ID with optional org/user filtering.\n\n    Args:\n        memory_id (str): ID of the memory to delete\n    \"\"\"\n    if memory_id:\n        await self.async_client.delete(\n            collection_name=self.collection_name,\n            points_selector=models.PointIdsList(points=[memory_id]),\n        )\n        self.logger.info(f\"Deleted memory with ID: {memory_id}\")\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.migrate_to_v0_2","title":"migrate_to_v0_2  <code>async</code>","text":"<pre><code>migrate_to_v0_2(\n    former_collection_name: str = \"memory_collection\",\n    new_collection_name: str = \"memory_collection_v0_2\",\n    delete_former_collection_after: bool = True,\n    batch_size: int = 50,\n    parallel: Optional[int] = None,\n)\n</code></pre> <p>Perform the migration from the old Qdrant collection to a new Qdrant collection using the current dense vector embedding model (<code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code> with <code>384</code> dimensions).</p> Note <p>Make sure to halt any operations like adding or deleting on the vector database during migration.</p> PARAMETER DESCRIPTION <code>former_collection_name</code> <p>Name of the former collection in the vector database, if you didn't explicitly specify this in earlier version it will be <code>memory_collection</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'memory_collection'</code> </p> <code>new_collection_name</code> <p>Name for the new Qdrant collection. Defaults to <code>memory_collection_v0_2</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'memory_collection_v0_2'</code> </p> <code>delete_former_collection_after</code> <p>If True, delete the former collection after migration. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>batch_size</code> <p>Number of records to fetch and migrate in each batch. Defaults to 50.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <code>parallel</code> <p>How many parallel workers to use for embedding. Defaults to None. If number is specified, Qdrant will use a data-parallel process. Note: Parallel processing is recommended only for large collections (tens of thousands of memories) and large batch sizes (above 100). For smaller collections or batch sizes, parallel processing may be slower.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>async def migrate_to_v0_2(\n    self,\n    former_collection_name: str = \"memory_collection\",\n    new_collection_name: str = \"memory_collection_v0_2\",\n    delete_former_collection_after: bool = True,\n    batch_size: int = 50,\n    parallel: Optional[int] = None,\n):\n    \"\"\"\n    Perform the migration from the old Qdrant collection to a new Qdrant collection using the current dense vector embedding model (`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` with `384` dimensions).\n\n    Note:\n        Make sure to halt any operations like adding or deleting on the vector database during migration.\n\n    Args:\n        former_collection_name (str): Name of the former collection in the vector database, if you didn't explicitly specify this in earlier version it will be `memory_collection`.\n        new_collection_name (str): Name for the new Qdrant collection. Defaults to `memory_collection_v0_2`.\n        delete_former_collection_after (bool): If True, delete the former collection after migration. Defaults to True.\n        batch_size (int): Number of records to fetch and migrate in each batch. Defaults to 50.\n        parallel (Optional[int]): How many parallel workers to use for embedding. Defaults to None. If number is specified, Qdrant will use a data-parallel process. Note: Parallel processing is recommended only for large collections (tens of thousands of memories) and large batch sizes (above 100). For smaller collections or batch sizes, parallel processing may be slower.\n    \"\"\"\n\n    self.logger.info(\n        f\"Starting migration from {former_collection_name} to {new_collection_name}\"\n    )\n    await self._create_collection_if_not_exists(new_collection_name)\n    number_of_memories_to_migrate = (\n        await self.async_client.get_collection(former_collection_name)\n    ).points_count\n\n    if number_of_memories_to_migrate == 0:\n        await self.async_client.delete_collection(former_collection_name)\n        self.logger.info(\"No memories to migrate, just deleted former collection.\")\n        return\n\n    offset_id = None\n    num_migrated = 0\n\n    while True:\n        records, offset_id = await self.async_client.scroll(\n            collection_name=former_collection_name,\n            limit=batch_size,\n            offset=offset_id,\n        )\n        memories, metadata, memory_ids = zip(\n            *[\n                (record.payload.pop(\"document\", \"\"), record.payload, record.id)\n                for record in records\n            ]\n        )\n        await self.async_client.add(\n            collection_name=new_collection_name,\n            documents=memories,\n            metadata=metadata,\n            ids=memory_ids,\n            parallel=parallel,\n        )\n        num_migrated += len(records)\n        self.logger.info(\n            f\"Migrated: {num_migrated}/{number_of_memories_to_migrate}\"\n        )\n\n        if offset_id is None:\n            self.logger.info(\"Migration completed \ud83d\ude0a\")\n\n            if delete_former_collection_after:\n\n                await self.async_client.delete_collection(former_collection_name)\n                self.logger.info(\n                    f\"Deleted former collection: {former_collection_name}\"\n                )\n\n            break  # Migration Done\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.search_memories","title":"search_memories  <code>async</code>","text":"<pre><code>search_memories(\n    queries: List[str],\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[List[Tuple[schema_models.Memory, float]]]\n</code></pre> <p>Batch memory search with optional user/agent filtering.</p> PARAMETER DESCRIPTION <code>queries</code> <p>List of search query strings</p> <p> TYPE: <code>List[str]</code> </p> <code>memory_search_scope</code> <p>Memory search scope (organization or user)</p> <p> TYPE: <code>MemorySearchScope</code> </p> <code>org_id</code> <p>Organization ID for filtering</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional user ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>agent_id</code> <p>Optional agent ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[List[Tuple[Memory, float]]]</code> <p>List[List[Tuple[Memory, float]]] of search results for each query, with a tuple containing: Memory:</p> <pre><code>+ org_id: str\n+ agent_id: str\n+ user_id: str\n+ memory_id: str\n+ memory: str\n+ obtained_at: datetime\n</code></pre> <p>float: Score of the memory</p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def search_memories(\n    self,\n    queries: List[str],\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[List[Tuple[schema_models.Memory, float]]]:\n    \"\"\"\n    Batch memory search with optional user/agent filtering.\n\n    Args:\n        queries (List[str]): List of search query strings\n        memory_search_scope (MemorySearchScope): Memory search scope (organization or user)\n        org_id (str): Organization ID for filtering\n        user_id (Optional[str]): Optional user ID for filtering\n        agent_id (Optional[str]): Optional agent ID for filtering\n\n    Returns:\n        List[List[Tuple[Memory, float]]] of search results for each query, with a tuple containing:\n            Memory:\n\n                + org_id: str\n                + agent_id: str\n                + user_id: str\n                + memory_id: str\n                + memory: str\n                + obtained_at: datetime\n\n            float: Score of the memory\n    \"\"\"\n\n    if not queries:\n        raise ValueError(\"At least one query is required\")\n\n    # Build filter conditions\n    filter_conditions = []\n\n    if (\n        memory_search_scope == MemorySearchScope.ORGANIZATION\n    ):  # Search memories across the organization.\n        filter_conditions.append(\n            models.FieldCondition(\n                key=\"org_id\", match=models.MatchValue(value=org_id)\n            )\n        )\n    elif (\n        memory_search_scope == MemorySearchScope.USER\n    ):  # Search memories for a specific user in an organization.\n\n        if user_id is None:\n            raise ValueError(\n                \"user_id is required in addition to org_id for user-specific search\"\n            )\n\n        filter_conditions.append(\n            models.FieldCondition(\n                key=\"org_user_id\",\n                match=models.MatchValue(value=f\"{org_id}:{user_id}\"),\n            )\n        )\n\n    if (\n        agent_id\n    ):  # If agent id is provided, filter by agent also regardless of memory search scope.\n        filter_conditions.append(\n            models.FieldCondition(\n                key=\"agent_id\", match=models.MatchValue(value=agent_id)\n            )\n        )\n\n    # Embed queries\n    dense_embeddings = self._dense_embed_queries(queries)\n    sparse_embeddings = self._sparse_embed_queries(queries)\n\n    search_results = await self.async_client.query_batch_points(\n        collection_name=self.collection_name,\n        requests=[\n            models.QueryRequest(\n                prefetch=[\n                    models.Prefetch(\n                        query=models.SparseVector(\n                            indices=sparse.indices, values=sparse.values\n                        ),\n                        using=self.async_client.get_sparse_vector_field_name(),\n                        limit=12,\n                    ),\n                    models.Prefetch(\n                        query=dense,\n                        using=self.async_client.get_vector_field_name(),\n                        score_threshold=0.4,\n                        limit=12,\n                    ),\n                ],\n                filter=(\n                    models.Filter(must=filter_conditions)\n                    if filter_conditions\n                    else None\n                ),\n                with_payload=True,\n                query=models.FusionQuery(fusion=models.Fusion.RRF),\n                params=models.SearchParams(\n                    quantization=models.QuantizationSearchParams(rescore=False)\n                ),\n            )\n            for sparse, dense in zip(sparse_embeddings, dense_embeddings)\n        ],\n    )\n\n    search_results = [\n        [\n            (\n                schema_models.Memory(\n                    org_id=point.payload[\"org_id\"],\n                    agent_id=point.payload[\"agent_id\"],\n                    user_id=point.payload[\"user_id\"],\n                    memory_id=point.id,\n                    memory=point.payload[\"document\"],\n                    obtained_at=datetime.fromisoformat(\n                        point.payload[\"obtained_at\"]\n                    ),\n                ),\n                point.score,\n            )\n            for point in query.points\n            if point.score &gt; 0.35  # Filter out low relevance memory.\n        ]\n        for query in search_results\n    ]\n    return search_results\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.search_memory","title":"search_memory  <code>async</code>","text":"<pre><code>search_memory(\n    query: str,\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[Tuple[schema_models.Memory, float]]\n</code></pre> <p>Memory search with optional user/agent filtering.</p> PARAMETER DESCRIPTION <code>query</code> <p>Search query string</p> <p> TYPE: <code>str</code> </p> <code>memory_search_scope</code> <p>Memory search scope (organization or user)</p> <p> TYPE: <code>MemorySearchScope</code> </p> <code>org_id</code> <p>Organization ID for filtering</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>Optional user ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>agent_id</code> <p>Optional agent ID for filtering</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Tuple[Memory, float]]</code> <p>List[Tuple[Memory, float]] containing tuple of search results and score: Memory:</p> <pre><code>+ org_id: str\n+ agent_id: str\n+ user_id: str\n+ memory_id: str\n+ memory: str\n+ obtained_at: datetime\n</code></pre> <p>float: Score of the memory</p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def search_memory(\n    self,\n    query: str,\n    memory_search_scope: MemorySearchScope,\n    org_id: str,\n    user_id: Optional[str] = None,\n    agent_id: Optional[str] = None,\n) -&gt; List[Tuple[schema_models.Memory, float]]:\n    \"\"\"\n    Memory search with optional user/agent filtering.\n\n    Args:\n        query (str): Search query string\n        memory_search_scope (MemorySearchScope): Memory search scope (organization or user)\n        org_id (str): Organization ID for filtering\n        user_id (Optional[str]): Optional user ID for filtering\n        agent_id (Optional[str]): Optional agent ID for filtering\n\n    Returns:\n        List[Tuple[Memory, float]] containing tuple of search results and score:\n            Memory:\n\n                + org_id: str\n                + agent_id: str\n                + user_id: str\n                + memory_id: str\n                + memory: str\n                + obtained_at: datetime\n\n            float: Score of the memory\n    \"\"\"\n\n    if not query:\n        raise ValueError(\"A query is required\")\n\n    results = await self.search_memories(\n        queries=[query],\n        memory_search_scope=memory_search_scope,\n        org_id=org_id,\n        user_id=user_id,\n        agent_id=agent_id,\n    )\n    return results[0] if results else []\n</code></pre>"},{"location":"api/vector_db/qdrant/#memora.vector_db.qdrant.QdrantDB.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(*args, **kwargs) -&gt; None\n</code></pre> <p>Setup the QdrantDB by creating the collection and payload indices.</p> Source code in <code>memora/vector_db/qdrant.py</code> <pre><code>@override\nasync def setup(self, *args, **kwargs) -&gt; None:\n    \"\"\"Setup the QdrantDB by creating the collection and payload indices.\"\"\"\n\n    await self._create_collection_if_not_exists()\n    await self._create_payload_indices()\n    self.logger.info(\"QdrantDB setup completed\")\n</code></pre>"}]}